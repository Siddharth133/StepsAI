{
    "title": "Natural Language Processing with Python",
    "parts": [
        {
            "title": "Chapter 1: Language Processing and Python",
            "page": "1",
            "sections": [
                {
                    "title": "1.1 Computing with Language: Texts and Words",
                    "page": "2",
                    "subsections": [],
                    "content": " Were all very familiar with text since we read and write it every day Here we will treat text as raw data for the programs we write programs that manipulate and analyze it in a variety of interesting ways But before we can do this we have to get started with the Python interpreter 1 Getting Started with Python One of the friendly things about Python is that it allows you to type directly into the interactive interpreterthe program that will be running your Python programs You can access the Python interpreter using a simple graphical interface called the In teractive DeveLopment Environment IDLE On a Mac you can find this under Ap plicationsMacPython and on Windows under All ProgramsPython Under Unix you can run Python from the shell by typing idle if this is not installed try typing python The interpreter will print a blurb about your Python version simply check that you are running Python 24 or 25 here it is 251 Python 251 r25154863 Apr 15 2008 225726 GCC 401 Apple Inc build 5465 on darwin Type help copyright credits or license for more information  If you are unable to run the Python interpreter you probably dont have Python installed correctly Please visit httppythonorg for detailed in structions The  prompt indicates that the Python interpreter is now waiting for input When copying examples from this book dont type the  yourself Now lets begin by using Python as a calculator  1  5  2  3 8  Once the interpreter has finished calculating the answer and displaying it the prompt reappears This means the Python interpreter is waiting for another instruction Your Turn Enter a few more expressions of your own You can use asterisk  for multiplication and slash  for division and parentheses for bracketing expressions Note that division doesnt always behave as you might expectit does integer division with rounding of fractions downwards when you type 13 and floatingpoint or decimal divi sion when you type 1030 In order to get the expected behavior of division standard in Python 30 you need to type from future import division The preceding examples demonstrate how you can work interactively with the Python interpreter experimenting with various expressions in the language to see what they do Now lets try a nonsensical expression to see how the interpreter handles it  1  File stdin line 1 1   SyntaxError invalid syntax  This produced a syntax error In Python it doesnt make sense to end an instruction with a plus sign The Python interpreter indicates the line where the problem occurred line 1 of stdin which stands for standard input Now that we can use the Python interpreter were ready to start working with language data Getting Started with NLTK Before going further you should install NLTK downloadable for free from httpwww nltkorg Follow the instructions there to download the version required for your platform Once youve installed NLTK start up the Python interpreter as before and install the data required for the book by typing the following two commands at the Python prompt then selecting the book collection as shown in Figure 11  import nltk  nltkdownload Figure 11 Downloading the NLTK Book Collection Browse the available packages using nltkdownload The Collections tab on the downloader shows how the packages are grouped into sets and you should select the line labeled book to obtain all data required for the examples and exercises in this book It consists of about 30 compressed files requiring about 100Mb disk space The full collection of data ie all in the downloader is about five times this size at the time of writing and continues to expand Once the data is downloaded to your machine you can load some of it using the Python interpreter The first step is to type a special command at the Python prompt which",
                    "word_count": 638
                },
                {
                    "title": "1.2 A Closer Look at Python: Texts as Lists of Words",
                    "page": "10",
                    "subsections": [],
                    "content": " Youve seen some important elements of the Python programming language Lets take a few moments to review them systematically Lists What is a text At one level it is a sequence of symbols on a page such as this one At another level it is a sequence of chapters made up of a sequence of sections where each section is a sequence of paragraphs and so on However for our purposes we will think of a text as nothing more than a sequence of words and punctuation Heres how we represent text in Python in this case the opening sentence of Moby Dick  sent1  Call me Ishmael   After the prompt weve given a name we made up sent1 followed by the equals sign and then some quoted words separated with commas and surrounded with brackets This bracketed material is known as a list in Python it is how we store a text We can inspect it by typing the name  We can ask for its length  We can even apply our own lexicaldiversity function to it   sent1 Call me Ishmael   lensent1 4  lexicaldiversitysent1 10  Some more lists have been defined for you one for the opening sentence of each of our texts sent2  sent9 We inspect two of them here you can see the rest for yourself using the Python interpreter if you get an error saying that sent2 is not defined you need to first type from nltkbook import   sent2 The family of Dashwood had long been settled in Sussex   sent3 In the beginning God created the heaven and the earth   Your Turn Make up a few sentences of your own by typing a name equals sign and a list of words like this ex1  Monty Python and the Holy Grail Repeat some of the other Python op erations we saw earlier in Section 11 eg sortedex1 lensetex1 ex1countthe A pleasant surprise is that we can use Pythons addition operator on lists Adding two lists  creates a new list with everything from the first list followed by everything from the second list  Monty Python  and the Holy Grail Monty Python and the Holy Grail This special use of the addition operation is called concatenation it combines the lists together into a single list We can concatenate sen tences to build up a text We dont have to literally type the lists either we can use short names that refer to pre defined lists  sent4  sent1 Fellow  Citizens of the Senate and of the House of Representatives  Call me Ishmael   What if we want to add a single item to a list This is known as appending When we append to a list the list itself is updated as a result of the operation  sent1appendSome  sent1 Call me Ishmael  Some ",
                    "word_count": 460
                },
                {
                    "title": "1.3 Computing with Language: Simple Statistics",
                    "page": "16",
                    "subsections": [],
                    "content": " Lets return to our exploration of the ways we can bring our computational resources to bear on large quantities of text We began this discussion in Section 11 and saw how to search for words in context how to compile the vocabulary of a text how to generate random text in the same style and so on In this section we pick up the question of what makes a text distinct and use automatic methods to find characteristic words and expressions of a text As in Section 11 you can try new features of the Python language by copying them into the interpreter and youll learn about these features systematically in the following section Before continuing further you might like to check your understanding of the last sec tion by predicting the output of the following code You can use the interpreter to check whether you got it right If youre not sure how to do this task it would be a good idea to review the previous section before continuing further  saying  After all is said and done            more is said than done  tokens  setsaying  tokens  sortedtokens  tokens2 what output do you expect here  Frequency Distributions How can we automatically identify the words of a text that are most informative about the topic and genre of the text Imagine how you might go about finding the 50 most frequent words of a book One method would be to keep a tally for each vocabulary item like that shown in Figure 13 The tally would need thousands of rows and it would be an exceedingly laborious processso laborious that we would rather assign the task to a machine Figure 13 Counting words appearing in a text a frequency distribution The table in Figure 13 is known as a frequency distribution  and it tells us the frequency of each vocabulary item in the text In general it could count any kind of observable event It is a distribution since it tells us how the total number of word tokens in the text are distributed across the vocabulary items Since we often need frequency distributions in language processing NLTK provides builtin support for them Lets use a FreqDist to find the 50 most frequent words of Moby Dick Try to work out what is going on here then read the explanation that follows  fdist1  FreqDisttext1  fdist1 FreqDist with 260819 outcomes  vocabulary1  fdist1keys  vocabulary150  the  of and a to  in that   his it I s is he with was as  all for this  at by but not  him from be on so whale one you had have there But or were now which  me like  fdist1whale 906  When we first invoke FreqDist we pass the name of the text as an argument  We can inspect the total number of words outcomes that have been counted up  260819 in the case of Moby Dick The expression keys gives us a list of all the distinct types in the text  and we can look at the first 50 of these by slicing the list ",
                    "word_count": 509
                },
                {
                    "title": "1.4 Back to Python: Making Decisions and Taking Control",
                    "page": "22",
                    "subsections": [],
                    "content": " So far our little programs have had some interesting qualities the ability to work with language and the potential to save human effort through automation A key feature of programming is the ability of machines to make decisions on our behalf executing instructions when certain conditions are met or repeatedly looping through text data until some condition is satisfied This feature is known as control and is the focus of this section Conditionals Python supports a wide range of operators such as  and  for testing the relationship between values The full set of these relational operators are shown in Table 13 Table 13 Numerical comparison operators Operator Relationship  Less than  Less than or equal to  Equal to note this is two signs not one Operator Relationship  Not equal to  Greater than  Greater than or equal to We can use these to select different words from a sentence of news text Here are some examplesnotice only the operator is changed from one line to the next They all use sent7 the first sentence from text7 Wall Street Journal As before if you get an error saying that sent7 is undefined you need to first type from nltkbook import   sent7 Pierre Vinken  61 years old  will join the board as a nonexecutive director Nov 29   w for w in sent7 if lenw  4  61 old  the as a 29   w for w in sent7 if lenw  4  61 old  will join the as a Nov 29   w for w in sent7 if lenw  4 will join Nov  w for w in sent7 if lenw  4 Pierre Vinken  61 years old  the board as a nonexecutive director 29   There is a common pattern to all of these examples w for w in text if condition where condition is a Python test that yields either true or false In the cases shown in the previous code example the condition is always a numerical comparison How ever we can also test various properties of words using the functions listed in Table 14 Table 14 Some word comparison operators Function Meaning sstartswitht Test if s starts with t sendswitht Test if s ends with t t in s Test if t is contained inside s sislower Test if all cased characters in s are lowercase sisupper Test if all cased characters in s are uppercase sisalpha Test if all characters in s are alphabetic sisalnum Test if all characters in s are alphanumeric sisdigit Test if all characters in s are digits sistitle Test if s is titlecased all words in s have initial capitals Here are some examples of these operators being used to select words from our texts words ending with ableness words containing gnt words having an initial capital and words consisting entirely of digits",
                    "word_count": 462
                },
                {
                    "title": "1.5 Automatic Natural Language Understanding",
                    "page": "26",
                    "subsections": [],
                    "content": " We have been exploring language bottomup with the help of texts and the Python programming language However were also interested in exploiting our knowledge of language and computation by building useful language technologies Well take the opportunity now to step back from the nittygritty of code in order to paint a bigger picture of natural language processing At a purely practical level we all need help to navigate the universe of information locked up in text on the Web Search engines have been crucial to the growth and popularity of the Web but have some shortcomings It takes skill knowledge and some luck to extract answers to such questions as What tourist sites can I visit between Philadelphia and Pittsburgh on a limited budget What do experts say about digital SLR cameras What predictions about the steel market were made by credible commentators in the past week Getting a computer to answer them automatically involves a range of language processing tasks including information extraction inference and summari zation and would need to be carried out on a scale and with a level of robustness that is still beyond our current capabilities On a more philosophical level a longstanding challenge within artificial intelligence has been to build intelligent machines and a major part of intelligent behavior is un derstanding language For many years this goal has been seen as too difficult However as NLP technologies become more mature and robust methods for analyzing unre stricted text become more widespread the prospect of natural language understanding has reemerged as a plausible goal",
                    "word_count": 260
                },
                {
                    "title": "1.6 Summary",
                    "page": "32",
                    "subsections": [],
                    "content": "  Texts are represented in Python using lists Monty Python We can use in dexing slicing and the len function on lists  A word token is a particular appearance of a given word in a text a word type is the unique form of the word as a particular sequence of letters We count word tokens using lentext and word types using lensettext  We obtain the vocabulary of a text t using sortedsett  We operate on each item of a text using fx for x in text  To derive the vocabulary collapsing case distinctions and ignoring punctuation we can write setwlower for w in text if wisalpha  We process each word in a text using a for statement such as for w in t or for word in text This must be followed by the colon character and an indented block of code to be executed each time through the loop  We test a condition using an if statement if lenword  5 This must be fol lowed by the colon character and an indented block of code to be executed only if the condition is true  A frequency distribution is a collection of items along with their frequency counts eg the words of a text and their frequency of appearance",
                    "word_count": 209
                },
                {
                    "title": "1.7 Further Reading",
                    "page": "34",
                    "subsections": [],
                    "content": " This chapter has introduced new concepts in programming natural language process ing and linguistics all mixed in together Many of them are consolidated in the fol lowing chapters However you may also want to consult the online materials provided with this chapter at httpwwwnltkorg including links to additional background materials and links to online NLP systems You may also like to read up on some linguistics and NLPrelated concepts in Wikipedia eg collocations the Turing Test the typetoken distinction You should acquaint yourself with the Python documentation available at httpdocs pythonorg including the many tutorials and comprehensive reference materials linked there A Beginners Guide to Python is available at httpwikipythonorgmoin BeginnersGuide Miscellaneous questions about Python might be answered in the FAQ at httpwwwpythonorgdocfaqgeneral As you delve into NLTK you might want to subscribe to the mailing list where new releases of the toolkit are announced There is also an NLTKUsers mailing list where users help each other as they learn how to use Python and NLTK for language analysis work Details of these lists are available at httpwwwnltkorg For more information on the topics covered in Section 15 and on NLP more generally you might like to consult one of the following excellent books  Indurkhya Nitin and Fred Damerau eds 2010 Handbook of Natural Language Processing second edition Chapman  HallCRC  Jurafsky Daniel and James Martin 2008 Speech and Language Processing second edition Prentice Hall  Mitkov Ruslan ed 2002 The Oxford Handbook of Computational Linguistics Oxford University Press second edition expected in 2010 The Association for Computational Linguistics is the international organization that represents the field of NLP The ACL website hosts many useful resources including information about international and regional conferences and workshops the ACL Wiki with links to hundreds of useful resources and the ACL Anthology which contains most of the NLP research literature from the past 50 years fully indexed and freely downloadable Some excellent introductory linguistics textbooks are Finegan 2007 OGrady et al 2004 OSU 2007 You might like to consult LanguageLog a popular linguistics blog with occasional posts that use the techniques described in this book",
                    "word_count": 350
                },
                {
                    "title": "1.8 Exercises",
                    "page": "34",
                    "subsections": [],
                    "content": " 1  Try using the Python interpreter as a calculator and typing expressions like 12  4  1 2  Given an alphabet of 26 letters there are 26 to the power 10 or 26  10 10 letter strings we can form That works out to 141167095653376L the L at the end just indicates that this is Pythons longnumber format How many hundredletter strings are possible 3  The Python multiplication operation can be applied to lists What happens when you type Monty Python  20 or 3  sent1 4  Review Section 11 on computing with language How many words are there in text2 How many distinct words are there 5  Compare the lexical diversity scores for humor and romance fiction in Ta ble 11 Which genre is more lexically diverse 6  Produce a dispersion plot of the four main protagonists in Sense and Sensibility Elinor Marianne Edward and Willoughby What can you observe about the different roles played by the males and females in this novel Can you identify the couples 7  Find the collocations in text5 8  Consider the following Python expression lensettext4 State the purpose of this expression Describe the two steps involved in performing this computation 9  Review Section 12 on lists and strings a Define a string and assign it to a variable eg mystring  My String but put something more interesting in the string Print the contents of this variable in two ways first by simply typing the variable name and pressing Enter then by using the print statement b Try adding the string to itself using mystring  mystring or multiplying it by a number eg mystring  3 Notice that the strings are joined together without any spaces How could you fix this 10  Define a variable mysent to be a list of words using the syntax mysent  My sent but with your own words or a favorite saying a Use  joinmysent to convert this into a string b Use split to split the string back into the list form you had to start with 11  Define several variables containing lists of words eg phrase1 phrase2 and so on Join them together in various combinations using the plus operator to form",
                    "word_count": 362
                }
            ]
        },
        {
            "title": "Chapter 2: Accessing Text Corpora and Lexical Resources",
            "page": "39",
            "sections": [
                {
                    "title": "2.1 Accessing Text Corpora",
                    "page": "40",
                    "subsections": [],
                    "content": " As just mentioned a text corpus is a large body of text Many corpora are designed to contain a careful balance of material in one or more genres We examined some small text collections in Chapter 1 such as the speeches known as the US Presidential Inau gural Addresses This particular corpus actually contains dozens of individual texts one per addressbut for convenience we glued them endtoend and treated them as a single text Chapter 1 also used various predefined texts that we accessed by typing from book import  However since we want to be able to work with other texts this section examines a variety of text corpora Well see how to select individual texts and how to work with them 39 Gutenberg Corpus NLTK includes a small selection of texts from the Project Gutenberg electronic text archive which contains some 25000 free electronic books hosted at httpwwwgu tenbergorg We begin by getting the Python interpreter to load the NLTK package then ask to see nltkcorpusgutenbergfileids the file identifiers in this corpus  import nltk  nltkcorpusgutenbergfileids austenemmatxt austenpersuasiontxt austensensetxt biblekjvtxt blakepoemstxt bryantstoriestxt burgessbusterbrowntxt carrollalicetxt chestertonballtxt chestertonbrowntxt chestertonthursdaytxt edgeworthparentstxt melvillemobydicktxt miltonparadisetxt shakespearecaesartxt shakespearehamlettxt shakespearemacbethtxt whitmanleavestxt Lets pick out the first of these textsEmma by Jane Austenand give it a short name emma then find out how many words it contains  emma  nltkcorpusgutenbergwordsaustenemmatxt  lenemma 192427 In Section 11 we showed how you could carry out concordancing of a text such as text1 with the command text1concordance However this assumes that you are using one of the nine texts obtained as a result of doing from nltkbook import  Now that you have started examining data from nltkcorpus as in the previous example you have to employ the following pair of statements to perform concordancing and other tasks from Section 11  emma  nltkTextnltkcorpusgutenbergwordsaustenemmatxt  emmaconcordancesurprize When we defined emma we invoked the words function of the gutenberg object in NLTKs corpus package But since it is cumbersome to type such long names all the time Python provides another version of the import statement as follows  from nltkcorpus import gutenberg  gutenbergfileids austenemmatxt austenpersuasiontxt austensensetxt   emma  gutenbergwordsaustenemmatxt Lets write a short program to display other information about each text by looping over all the values of fileid corresponding to the gutenberg file identifiers listed earlier and then computing statistics for each text For a compact output display we will make sure that the numbers are all integers using int  for fileid in gutenbergfileids      numchars  lengutenbergrawfileid      numwords  lengutenbergwordsfileid      numsents  lengutenbergsentsfileid      numvocab  lensetwlower for w in gutenbergwordsfileid      print intnumcharsnumwords intnumwordsnumsents intnumwordsnumvocab fileid  4 21 26 austenemmatxt 4 23 16 austenpersuasiontxt 4 24 22 austensensetxt 4 33 79 biblekjvtxt 4 18 5 blakepoemstxt 4 17 14 bryantstoriestxt 4 17 12 burgessbusterbrowntxt 4 16 12 carrollalicetxt 4 17 11 chestertonballtxt 4 19 11 chestertonbrowntxt 4 16 10 chestertonthursdaytxt 4 18 24 edgeworthparentstxt 4 24 15 melvillemobydicktxt 4 52 10 miltonparadisetxt 4 12 8 shakespearecaesartxt 4 13 7 shakespearehamlettxt 4 13 6 shakespearemacbethtxt 4 35 12 whitmanleavestxt This program displays three statistics for each text average word length average sen tence length and the number of times each vocabulary item appears in the text on average our lexical diversity score Observe that average word length appears to be a general property of English since it has a recurrent value of 4 In fact the average word length is really 3 not 4 since the numchars variable counts space characters By con trast average sentence length and lexical diversity appear to be characteristics of par ticular authors The previous example also showed how we can access the raw text of the book  not split up into tokens The raw function gives us the contents of the file without any linguistic processing So for example lengutenbergrawblakepoemstxt tells us how many letters occur in the text including the spaces between words The sents function divides the text up into its sentences where each sentence is a list of words  macbethsentences  gutenbergsentsshakespearemacbethtxt  macbethsentences  The Tragedie of Macbeth by William Shakespeare 1603  Actus Primus    macbethsentences1037 Double  double  toile and trouble  Fire burne  and Cauldron bubble  longestlen  maxlens for s in macbethsentences  s for s in macbethsentences if lens  longestlen Doubtfull it stood  As two spent Swimmers  that doe cling together  And choake their Art  The mercilesse Macdonwald  ",
                    "word_count": 710
                },
                {
                    "title": "2.2 Conditional Frequency Distributions",
                    "page": "52",
                    "subsections": [],
                    "content": " We introduced frequency distributions in Section 13 We saw that given some list mylist of words or other items FreqDistmylist would compute the number of occurrences of each item in the list Here we will generalize this idea When the texts of a corpus are divided into several categories by genre topic author etc we can maintain separate frequency distributions for each category This will allow us to study systematic differences between the categories In the previous section we achieved this using NLTKs ConditionalFreqDist data type A conditional fre quency distribution is a collection of frequency distributions each one for a different condition The condition will often be the category of the text Figure 24 depicts a fragment of a conditional frequency distribution having just two conditions one for news text and one for romance text Figure 24 Counting words appearing in a text collection a conditional frequency distribution Conditions and Events A frequency distribution counts observable events such as the appearance of words in a text A conditional frequency distribution needs to pair each event with a condition So instead of processing a sequence of words  we have to process a sequence of pairs   text  The Fulton County Grand Jury said   pairs  news The news Fulton news County  Each pair has the form condition event If we were processing the entire Brown Corpus by genre there would be 15 conditions one per genre and 1161192 events one per word Counting Words by Genre In Section 21 we saw a conditional frequency distribution where the condition was the section of the Brown Corpus and for each condition we counted words Whereas FreqDist takes a simple list as input ConditionalFreqDist takes a list of pairs  from nltkcorpus import brown  cfd  nltkConditionalFreqDist            genre word            for genre in browncategories            for word in brownwordscategoriesgenre Lets break this down and look at just two genres news and romance For each genre  we loop over every word in the genre  producing pairs consisting of the genre and the word   genreword  genre word                for genre in news romance                for word in brownwordscategoriesgenre  lengenreword 170576 So as we can see in the following code pairs at the beginning of the list genreword will be of the form news word  whereas those at the end will be of the form roman ce word   genreword4 news The news Fulton news County news Grand  genreword4 romance afraid romance not romance  romance  We can now use this list of pairs to create a ConditionalFreqDist and save it in a variable cfd As usual we can type the name of the variable to inspect it  and verify it has two conditions   cfd  nltkConditionalFreqDistgenreword  cfd ConditionalFreqDist with 2 conditions  cfdconditions news romance Lets access the two conditions and satisfy ourselves that each is just a frequency distribution  cfdnews FreqDist with 100554 outcomes  cfdromance FreqDist with 70022 outcomes  listcfdromance   the and to a of   was I in he had  her that it his she with you for at He on him said   be as  have but not would She The   cfdromancecould 193 Plotting and Tabulating Distributions Apart from combining two or more frequency distributions and being easy to initialize a ConditionalFreqDist provides some useful methods for tabulation and plotting",
                    "word_count": 534
                },
                {
                    "title": "2.3 More Python: Reusing Code",
                    "page": "56",
                    "subsections": [],
                    "content": " By this time youve probably typed and retyped a lot of code in the Python interactive interpreter If you mess up when retyping a complex example you have to enter it again Using the arrow keys to access and modify previous commands is helpful but only goes so far In this section we see two important ways to reuse code text editors and Python functions Creating Programs with a Text Editor The Python interactive interpreter performs your instructions as soon as you type them Often it is better to compose a multiline program using a text editor then ask Python to run the whole program at once Using IDLE you can do this by going to the File menu and opening a new window Try this now and enter the following oneline program print Monty Python Save this program in a file called montypy then go to the Run menu and select the command Run Module Well learn what modules are shortly The result in the main IDLE window should look like this   RESTART   Monty Python  You can also type from monty import  and it will do the same thing From now on you have a choice of using the interactive interpreter or a text editor to create your programs It is often convenient to test your ideas using the interpreter revising a line of code until it does what you expect Once youre ready you can paste the code minus any  or  prompts into the text editor continue to expand it and finally save the program in a file so that you dont have to type it in again later Give the file a short but descriptive name using all lowercase letters and separating words with underscore and using the py filename extension eg montypythonpy Important Our inline code examples include the  and  prompts as if we are interacting directly with the interpreter As they get more complicated you should instead type them into the editor without the prompts and run them from the editor as shown earlier When we pro vide longer programs in this book we will leave out the prompts to remind you to type them into a file rather than using the interpreter You can see this already in Example 21 Note that the example still includes a couple of lines with the Python prompt this is the interactive part of the task where you inspect some data and invoke a function Remember that all code samples like Example 21 are downloadable from httpwwwnltkorg Functions Suppose that you work on analyzing text that involves different forms of the same word and that part of your program needs to work out the plural form of a given singular noun Suppose it needs to do this work in two places once when it is processing some texts and again when it is processing user input Rather than repeating the same code several times over it is more efficient and reliable to localize this work inside a function A function is just a named block of code that performs some welldefined task as we saw in Section 11 A function is usually defined to take some inputs using special variables known as parameters and it may produce a result also known as a return value We define a function using the keyword def followed by the function name and any input parameters followed by the body of the function Heres the function we saw in Section 11 including the import statement that makes division behave as expected",
                    "word_count": 589
                },
                {
                    "title": "2.4 Lexical Resources",
                    "page": "58",
                    "subsections": [],
                    "content": " A lexicon or lexical resource is a collection of words andor phrases along with asso ciated information such as partofspeech and sense definitions Lexical resources are secondary to texts and are usually created and enriched with the help of texts For example if we have defined a text mytext then vocab  sortedsetmytext builds the vocabulary of mytext whereas wordfreq  FreqDistmytext counts the fre quency of each word in the text Both vocab and wordfreq are simple lexical resources Similarly a concordance like the one we saw in Section 11 gives us information about word usage that might help in the preparation of a dictionary Standard terminology for lexicons is illustrated in Figure 25 A lexical entry consists of a headword also known as a lemma along with additional information such as the partofspeech and",
                    "word_count": 134
                },
                {
                    "title": "2.5 WordNet",
                    "page": "66",
                    "subsections": [],
                    "content": " WordNet is a semantically oriented dictionary of English similar to a traditional the saurus but with a richer structure NLTK includes the English WordNet with 155287 words and 117659 synonym sets Well begin by looking at synonyms and how they are accessed in WordNet Senses and Synonyms Consider the sentence in 1a If we replace the word motorcar in 1a with automo bile to get 1b the meaning of the sentence stays pretty much the same 1 a Benz is credited with the invention of the motorcar b Benz is credited with the invention of the automobile Since everything else in the sentence has remained unchanged we can conclude that the words motorcar and automobile have the same meaning ie they are synonyms We can explore these words with the help of WordNet  from nltkcorpus import wordnet as wn  wnsynsetsmotorcar Synsetcarn01 Thus motorcar has just one possible meaning and it is identified as carn01 the first noun sense of car The entity carn01 is called a synset or synonym set a collection of synonymous words or lemmas",
                    "word_count": 177
                },
                {
                    "title": "2.6 Summary",
                    "page": "72",
                    "subsections": [],
                    "content": "  A text corpus is a large structured collection of texts NLTK comes with many corpora eg the Brown Corpus nltkcorpusbrown  Some text corpora are categorized eg by genre or topic sometimes the categories of a corpus overlap each other  A conditional frequency distribution is a collection of frequency distributions each one for a different condition They can be used for counting word frequencies given a context or a genre  Python programs more than a few lines long should be entered using a text editor saved to a file with a py extension and accessed using an import statement  Python functions permit you to associate a name with a particular block of code and reuse that code as often as necessary  Some functions known as methods are associated with an object and we give the object name followed by a period followed by the method name like this xfuncty eg wordisalpha  To find out about some variable v type helpv in the Python interactive interpreter to read the help entry for this kind of object  WordNet is a semantically oriented dictionary of English consisting of synonym setsor synsetsand organized into a network  Some functions are not available by default but must be accessed using Pythons import statement",
                    "word_count": 206
                },
                {
                    "title": "2.7 Further Reading",
                    "page": "72",
                    "subsections": [],
                    "content": " Extra materials for this chapter are posted at httpwwwnltkorg including links to freely available resources on the Web The corpus methods are summarized in the Corpus HOWTO at httpwwwnltkorghowto and documented extensively in the online API documentation Significant sources of published corpora are the Linguistic Data Consortium LDC and the European Language Resources Agency ELRA Hundreds of annotated text and speech corpora are available in dozens of languages Noncommercial licenses permit the data to be used in teaching and research For some corpora commercial licenses are also available but for a higher fee",
                    "word_count": 93
                },
                {
                    "title": "2.8 Exercises",
                    "page": "74",
                    "subsections": [],
                    "content": " 1  Create a variable phrase containing a list of words Experiment with the opera tions described in this chapter including addition multiplication indexing slic ing and sorting 2  Use the corpus module to explore austenpersuasiontxt How many word tokens does this book have How many word types 3  Use the Brown Corpus reader nltkcorpusbrownwords or the Web Text Cor pus reader nltkcorpuswebtextwords to access some sample text in two differ ent genres 4  Read in the texts of the State of the Union addresses using the stateunion corpus reader Count occurrences of men women and people in each document What has happened to the usage of these words over time 5  Investigate the holonymmeronym relations for some nouns Remember that there are three kinds of holonymmeronym relation so you need to use membermer onyms partmeronyms substancemeronyms memberholonyms partholonyms and substanceholonyms 6  In the discussion of comparative wordlists we created an object called trans late which you could look up using words in both German and Italian in order to get corresponding words in English What problem might arise with this ap proach Can you suggest a way to avoid this problem 7  According to Strunk and Whites Elements of Style the word however used at the start of a sentence means in whatever way or to whatever extent and not nevertheless They give this example of correct usage However you advise him he will probably do as he thinks best httpwwwbartlebycom141strunk3html Use the concordance tool to study actual usage of this word in the various texts we have been considering See also the LanguageLog posting Fossilized prejudices about however at httpitrecisupennedumyllanguagelogarchives001913 html 8  Define a conditional frequency distribution over the Names Corpus that allows you to see which initial letters are more frequent for males versus females see Figure 27 9  Pick a pair of texts and study the differences between them in terms of vocabu lary vocabulary richness genre etc Can you find pairs of words that have quite different meanings across the two texts such as monstrous in Moby Dick and in Sense and Sensibility 10  Read the BBC News article UKs Vicky Pollards left behind at httpnews bbccouk1hieducation6173441stm The article gives the following statistic about teen language the top 20 words used including yeah no but and like account for around a third of all words How many word types account for a third of all word tokens for a variety of text sources What do you conclude about this statistic Read more about this on LanguageLog at httpitrecisupennedumyl languagelogarchives003993html 11  Investigate the table of modal distributions and look for other patterns Try to explain them in terms of your own impressionistic understanding of the different genres Can you find other closed classes of words that exhibit significant differ ences across different genres 12  The CMU Pronouncing Dictionary contains multiple pronunciations for certain words How many distinct words does it contain What fraction of words in this dictionary have more than one possible pronunciation 13  What percentage of noun synsets have no hyponyms You can get all noun syn sets using wnallsynsetsn 14  Define a function superglosss that takes a synset s as its argument and returns a string consisting of the concatenation of the definition of s and the definitions of all the hypernyms and hyponyms of s 15  Write a program to find all words that occur at least three times in the Brown Corpus 16  Write a program to generate a table of lexical diversity scores ie tokentype ratios as we saw in Table 11 Include the full set of Brown Corpus genres",
                    "word_count": 599
                }
            ]
        },
        {
            "title": "Chapter 3: Processing Raw Text",
            "page": "79",
            "sections": [
                {
                    "title": "3.1 Accessing Text from the Web and from Disk",
                    "page": "80",
                    "subsections": [],
                    "content": " Electronic Books A small sample of texts from Project Gutenberg appears in the NLTK corpus collection However you may be interested in analyzing other texts from Project Gutenberg You can browse the catalog of 25000 free online books at httpwwwgutenbergorgcata log and obtain a URL to an ASCII text file Although 90 of the texts in Project Gutenberg are in English it includes material in over 50 other languages including Catalan Chinese Dutch Finnish French German Italian Portuguese and Spanish with more than 100 texts each Text number 2554 is an English translation of Crime and Punishment and we can access it as follows  from urllib import urlopen  url  httpwwwgutenbergorgfiles25542554txt  raw  urlopenurlread  typeraw type str  lenraw 1176831  raw75 The Project Gutenberg EBook of Crime and Punishment by Fyodor Dostoevskyrn The read process will take a few seconds as it downloads this large book If youre using an Internet proxy that is not correctly detected by Python you may need to specify the proxy manually as follows  proxies  http httpwwwsomeproxycom3128  raw  urlopenurl proxiesproxiesread The variable raw contains a string with 1176831 characters We can see that it is a string using typeraw This is the raw content of the book including many details we are not interested in such as whitespace line breaks and blank lines Notice the r and n in the opening line of the file which is how Python displays the special carriage return and linefeed characters the file must have been created on a Windows ma chine For our language processing we want to break up the string into words and punctuation as we saw in Chapter 1 This step is called tokenization and it produces our familiar structure a list of words and punctuation  tokens  nltkwordtokenizeraw  typetokens type list  lentokens 255809  tokens10 The Project Gutenberg EBook of Crime and Punishment  by Notice that NLTK was needed for tokenization but not for any of the earlier tasks of opening a URL and reading it into a string If we now take the further step of creating an NLTK text from this list we can carry out all of the other linguistic processing we saw in Chapter 1 along with the regular list operations such as slicing  text  nltkTexttokens  typetext type nltktextText  text10201060 CHAPTER I On an exceptionally hot evening early in July a young man came out of the garret in which he lodged in S  Place and walked slowly  as though in hesitation  towards K  bridge   textcollocations Katerina Ivanovna Pulcheria Alexandrovna Avdotya Romanovna Pyotr Petrovitch Project Gutenberg Marfa Petrovna Rodion Romanovitch Sofya Semyonovna Nikodim Fomitch did not Hay Market Andrey Semyonovitch old woman Literary Archive Dmitri Prokofitch great deal United States Praskovya Pavlovna Porfiry Petrovitch ear rings Notice that Project Gutenberg appears as a collocation This is because each text down loaded from Project Gutenberg contains a header with the name of the text the author the names of people who scanned and corrected the text a license and so on Some times this information appears in a footer at the end of the file We cannot reliably detect where the content begins and ends and so have to resort to manual inspection of the file to discover unique strings that mark the beginning and the end before trimming raw to be just the content and nothing else  rawfindPART I 5303  rawrfindEnd of Project Gutenbergs Crime 1157681  raw  raw53031157681  rawfindPART I 0 The find and rfind reverse find methods help us get the right index values to use for slicing the string  We overwrite raw with this slice so now it begins with PART I and goes up to but not including the phrase that marks the end of the content This was our first brush with the reality of the Web texts found on the Web may contain unwanted material and there may not be an automatic way to remove it But with a small amount of extra work we can extract the material we need Dealing with HTML Much of the text on the Web is in the form of HTML documents You can use a web browser to save a page as text to a local file then access this as described in the later section on files However if youre going to do this often its easiest to get Python to do the work directly The first step is the same as before using urlopen For fun well",
                    "word_count": 736
                },
                {
                    "title": "3.2 Strings: Text Processing at the Lowest Level",
                    "page": "86",
                    "subsections": [],
                    "content": " Its time to study a fundamental data type that weve been studiously avoiding so far In earlier chapters we focused on a text as a list of words We didnt look too closely at words and how they are handled in the programming language By using NLTKs corpus interface we were able to ignore the files that these texts had come from The contents of a word and of a file are represented by programming languages as a fun damental data type known as a string In this section we explore strings in detail and show the connection between strings words texts and files Basic Operations with Strings Strings are specified using single quotes or double quotes  as shown in the fol lowing code example If a string contains a single quote we must backslashescape the quote  so Python knows a literal quote character is intended or else put the string in double quotes  Otherwise the quote inside the string  will be interpreted as a close quote and the Python interpreter will report a syntax error  monty  Monty Python  monty Monty Python  circus  Monty Pythons Flying Circus  circus Monty Pythons Flying Circus  circus  Monty Pythons Flying Circus  circus Monty Pythons Flying Circus  circus  Monty Pythons Flying Circus File stdin line 1 circus  Monty Pythons Flying Circus  SyntaxError invalid syntax",
                    "word_count": 219
                },
                {
                    "title": "3.3 Text Processing with Unicode",
                    "page": "92",
                    "subsections": [],
                    "content": " Our programs will often need to deal with different languages and different character sets The concept of plain text is a fiction If you live in the Englishspeaking world you probably use ASCII possibly without realizing it If you live in Europe you might use one of the extended Latin character sets containing such characters as  for Danish and Norwegian  for Hungarian  for Spanish and Breton and  for Czech and Slovak In this section we will give an overview of how to use Unicode for processing texts that use nonASCII character sets",
                    "word_count": 92
                },
                {
                    "title": "3.4 Regular Expressions for Detecting Word Patterns",
                    "page": "96",
                    "subsections": [],
                    "content": " Many linguistic processing tasks involve pattern matching For example we can find words ending with ed using endswithed We saw a variety of such word tests in Table 14 Regular expressions give us a more powerful and flexible method for de scribing the character patterns we are interested in There are many other published introductions to regular expressions organized around the syntax of regular expressions and applied to searching text files Instead of doing this again we focus on the use of regular expressions at different stages of linguistic processing As usual well adopt a problembased approach and present new features only as they are needed to solve practical problems In our discussion we will mark regular expressions using chevrons like this patt",
                    "word_count": 123
                },
                {
                    "title": "3.5 Useful Applications of Regular Expressions",
                    "page": "102",
                    "subsections": [],
                    "content": " The previous examples all involved searching for words w that match some regular expression regexp using researchregexp w Apart from checking whether a regular expression matches a word we can use regular expressions to extract material from words or to modify words in specific ways Extracting Word Pieces The refindall find all method finds all nonoverlapping matches of the given regular expression Lets find all the vowels in a word then count them  word  supercalifragilisticexpialidocious  refindallraeiou word u e a i a i i i e i a i o i o u  lenrefindallraeiou word 16 Lets look for all sequences of two or more vowels in some text and determine their relative frequency  wsj  sortedsetnltkcorpustreebankwords  fd  nltkFreqDistvs for word in wsj                        for vs in refindallraeiou2 word  fditems io 549 ea 476 ie 331 ou 329 ai 261 ia 253 ee 217 oo 174 ua 109 au 106 ue 105 ui 95 ei 86 oi 65 oa 59 eo 39 iou 27 eu 18  Your Turn In the W3C Date Time Format dates are represented like this 20091231 Replace the  in the following Python code with a regular expression in order to convert the string 20091231 to a list of integers 2009 12 31 intn for n in refindall 20091231 Doing More with Word Pieces Once we can use refindall to extract material from words there are interesting things to do with the pieces such as glue them back together or plot them It is sometimes noted that English text is highly redundant and it is still easy to read when wordinternal vowels are left out For example declaration becomes dclrtn and inalienable becomes inlnble retaining any initial or final vowel sequences The regular expression in our next example matches initial vowel sequences final vowel sequences and all consonants everything else is ignored This threeway disjunction is processed lefttoright and if one of the three parts matches the word any later parts of the regular expression are ignored We use refindall to extract all the matching pieces and join to join them together see Section 39 for more about the join operation  regexp  rAEIOUaeiouAEIOUaeiouAEIOUaeiou  def compressword      pieces  refindallregexp word      return joinpieces   englishudhr  nltkcorpusudhrwordsEnglishLatin1  print nltktokenwrapcompressw for w in englishudhr75 Unvrsl Dclrtn of Hmn Rghts Prmble Whrs rcgntn of the inhrnt dgnty and of the eql and inlnble rghts of all mmbrs of the hmn fmly is the fndtn of frdm  jstce and pce in the wrld  Whrs dsrgrd and cntmpt fr hmn rghts hve rsltd in brbrs acts whch hve outrgd the cnscnce of mnknd  and the advnt of a wrld in whch hmn bngs shll enjy frdm of spch and Next lets combine regular expressions with conditional frequency distributions Here we will extract all consonantvowel sequences from the words of Rotokas such as ka and si Since each of these is a pair it can be used to initialize a conditional frequency distribution We then tabulate the frequency of each pair  rotokaswords  nltkcorpustoolboxwordsrotokasdic  cvs  cv for w in rotokaswords for cv in refindallrptksvraeiou w  cfd  nltkConditionalFreqDistcvs  cfdtabulate a    e    i    o    u k  418  148   94  420  173 p   83   31  105   34   51 r  187   63   84   89   79 s    0    0  100    2    1 t   47    8    0  148   37 v   93   27  105   48   49 Examining the rows for s and t we see they are in partial complementary distribution which is evidence that they are not distinct phonemes in the language Thus we could conceivably drop s from the Rotokas alphabet and simply have a pronunciation rule that the letter t is pronounced s when followed by i Note that the single entry having su namely kasuari cassowary is borrowed from English If we want to be able to inspect the words behind the numbers in that table it would be helpful to have an index allowing us to quickly find the list of words that contains a given consonantvowel pair For example cvindexsu should give us all words containing su Heres how we can do this  cvwordpairs  cv w for w in rotokaswords                           for cv in refindallrptksvraeiou w  cvindex  nltkIndexcvwordpairs  cvindexsu kasuari  cvindexpo kaapo kaapopato kaipori kaiporipie kaiporivira kapo kapoa kapokao kapokapo kapokapo kapokapoa kapokapoa kapokapora  This program processes each word w in turn and for each one finds every substring that matches the regular expression ptksvraeiou In the case of the word ka suari it finds ka su and ri Therefore the cvwordpairs list will contain ka ka",
                    "word_count": 745
                },
                {
                    "title": "3.6 Normalizing Text",
                    "page": "106",
                    "subsections": [],
                    "content": " In earlier program examples we have often converted text to lowercase before doing anything with its words eg setwlower for w in text By using lower we have normalized the text to lowercase so that the distinction between The and the is ignored Often we want to go further than this and strip off any affixes a task known as stem ming A further step is to make sure that the resulting form is a known word in a dictionary a task known as lemmatization We discuss each of these in turn First we need to define the data we will use in this section  raw  DENNIS Listen strange women lying in ponds distributing swords  is no basis for a system of government  Supreme executive power derives from  a mandate from the masses not from some farcical aquatic ceremony  tokens  nltkwordtokenizeraw Stemmers NLTK includes several offtheshelf stemmers and if you ever need a stemmer you should use one of these in preference to crafting your own using regular expressions since NLTKs stemmers handle a wide range of irregular cases The Porter and Lan caster stemmers follow their own rules for stripping affixes Observe that the Porter stemmer correctly handles the word lying mapping it to lie whereas the Lancaster stemmer does not  porter  nltkPorterStemmer  lancaster  nltkLancasterStemmer  porterstemt for t in tokens DENNI  Listen  strang women lie in pond distribut sword is no basi for a system of govern  Suprem execut power deriv from a mandat from the mass  not from some farcic aquat ceremoni   lancasterstemt for t in tokens den  list  strange wom lying in pond distribut sword is no bas for a system of govern  suprem execut pow der from a mand from the mass  not from som farc aqu ceremony  Stemming is not a welldefined process and we typically pick the stemmer that best suits the application we have in mind The Porter Stemmer is a good choice if you are indexing some texts and want to support search using alternative forms of words il lustrated in Example 31 which uses objectoriented programming techniques that are outside the scope of this book string formatting techniques to be covered in Sec tion 39 and the enumerate function to be explained in Section 42 Example 31 Indexing a text using a stemmer class IndexedTextobject def initself stemmer text selftext  text selfstemmer  stemmer",
                    "word_count": 392
                },
                {
                    "title": "3.7 Regular Expressions for Tokenizing Text",
                    "page": "108",
                    "subsections": [],
                    "content": " Tokenization is the task of cutting a string into identifiable linguistic units that consti tute a piece of language data Although it is a fundamental task we have been able to delay it until now because many corpora are already tokenized and because NLTK includes some tokenizers Now that you are familiar with regular expressions you can learn how to use them to tokenize text and to have much more control over the process Simple Approaches to Tokenization The very simplest method for tokenizing text is to split on whitespace Consider the following text from Alices Adventures in Wonderland  raw  When IM a Duchess she said to herself not in a very hopeful tone  though I wont have any pepper in my kitchen AT ALL Soup does very  well withoutMaybe its always pepper that makes people hottempered We could split this raw text on whitespace using rawsplit To do the same using a regular expression it is not enough to match any space characters in the string  since this results in tokens that contain a n newline character instead we need to match any number of spaces tabs or newlines   resplitr  raw When IM a Duchess she said to herself not in a very hopeful tonenthough I wont have any pepper in my kitchen AT ALL Soup does verynwell withoutMaybe its always pepper that makes people hottempered  resplitr tn raw When IM a Duchess she said to herself not in a very hopeful tone though I wont have any pepper in my kitchen AT ALL Soup does very well withoutMaybe its always pepper that makes people hottempered The regular expression  tn matches one or more spaces tabs t or newlines n Other whitespace characters such as carriage return and form feed should really be included too Instead we will use a builtin re abbreviation s which means any whitespace character The second statement in the preceding example can be rewritten as resplitrs raw Important Remember to prefix regular expressions with the letter r meaning raw which instructs the Python interpreter to treat the string literally rather than processing any backslashed characters it contains Splitting on whitespace gives us tokens like not and herself An alternative is to use the fact that Python provides us with a character class w for word characters equivalent to azAZ09 It also defines the complement of this class W ie all",
                    "word_count": 396
                },
                {
                    "title": "3.8 Segmentation",
                    "page": "112",
                    "subsections": [],
                    "content": " This section discusses more advanced concepts which you may prefer to skip on the first time through this chapter Tokenization is an instance of a more general problem of segmentation In this section we will look at two other instances of this problem which use radically different tech niques to the ones we have seen so far in this chapter Sentence Segmentation Manipulating texts at the level of individual words often presupposes the ability to divide a text into individual sentences As we have seen some corpora already provide access at the sentence level In the following example we compute the average number of words per sentence in the Brown Corpus  lennltkcorpusbrownwords  lennltkcorpusbrownsents 20250994070456922 In other cases the text is available only as a stream of characters Before tokenizing the text into words we need to segment it into sentences NLTK facilitates this by including the Punkt sentence segmenter Kiss  Strunk 2006 Here is an example of its use in segmenting the text of a novel Note that if the segmenters internal data has been updated by the time you read this you will see different output  senttokenizernltkdataloadtokenizerspunktenglishpickle  text  nltkcorpusgutenbergrawchestertonthursdaytxt  sents  senttokenizertokenizetext  pprintpprintsents171181 Nonsense  said Gregory who was very rational when anyone elsenattempted paradox Why do all the clerks and navvies in thenrailway trains look so sad and tired I willntell you It is because they know that the train is going right Itnis because they know that whatever place they have taken a ticketnfor that  It is because after they havenpassed Sloane Square they know that the next stat Oh their wild rapture ohntheir eyes like stars and their souls again in Eden if the nextnstation w nnIt is you who are unpoetical replied the poet Syme Notice that this example is really a single sentence reporting the speech of Mr Lucian Gregory However the quoted speech contains several sentences and these have been split into individual strings This is reasonable behavior for most applications Sentence segmentation is difficult because a period is used to mark abbreviations and some periods simultaneously mark an abbreviation and terminate a sentence as often happens with acronyms like USA For another approach to sentence segmentation see Section 62 Word Segmentation For some writing systems tokenizing text is made more difficult by the fact that there is no visual representation of word boundaries For example in Chinese the three character string  ai4 love verb guo3 country ren2 person could be tokenized as    countryloving person or as    love countryperson A similar problem arises in the processing of spoken language where the hearer must segment a continuous speech stream into individual words A particularly challenging version of this problem arises when we dont know the words in advance This is the problem faced by a language learner such as a child hearing utterances from a parent Consider the following artificial example where word boundaries have been removed 1 a doyouseethekitty b seethedoggy c doyoulikethekitty d likethedoggy Our first challenge is simply to represent the problem we need to find a way to separate text content from the segmentation We can do this by annotating each character with a boolean value to indicate whether or not a wordbreak appears after the character an idea that will be used heavily for chunking in Chapter 7 Lets assume that the learner is given the utterance breaks since these often correspond to extended pauses Here is a possible representation including the initial and target segmentations  text  doyouseethekittyseethedoggydoyoulikethekittylikethedoggy  seg1  0000000000000001000000000010000000000000000100000000000  seg2  0100100100100001001001000010100100010010000100010010000 Observe that the segmentation strings consist of zeros and ones They are one character shorter than the source text since a text of length n can be broken up in only n1 places The segment function in Example 32 demonstrates that we can get back to the orig inal segmented text from its representation",
                    "word_count": 635
                },
                {
                    "title": "3.9 Formatting: From Lists to Strings",
                    "page": "116",
                    "subsections": [],
                    "content": " Often we write a program to report a single data item such as a particular element in a corpus that meets some complicated criterion or a single summary statistic such as a wordcount or the performance of a tagger More often we write a program to produce a structured result for example a tabulation of numbers or linguistic forms or a re formatting of the original data When the results to be presented are linguistic textual output is usually the most natural choice However when the results are numerical it may be preferable to produce graphical output In this section you will learn about a variety of ways to present program output From Lists to Strings The simplest kind of structured object we use for text processing is lists of words When we want to output these to a display or a file we must convert these lists into strings To do this in Python we use the join method and specify the string to be used as the glue  silly  We called him Tortoise because he taught us    joinsilly We called him Tortoise because he taught us   joinsilly WecalledhimTortoisebecausehetaughtus  joinsilly WecalledhimTortoisebecausehetaughtus So  joinsilly means take all the items in silly and concatenate them as one big string using   as a spacer between the items Ie join is a method of the string that you want to use as the glue Many people find this notation for join counter intuitive The join method only works on a list of stringswhat we have been calling a texta complex type that enjoys some privileges in Python Strings and Formats We have seen that there are two ways to display the contents of an object  word  cat  sentence  hello  world  print word cat  print sentence hello world  word cat  sentence hellonworld The print command yields Pythons attempt to produce the most humanreadable form of an object The second methodnaming the variable at a promptshows us a string that can be used to recreate this object It is important to keep in mind that both of these are just strings displayed for the benefit of you the user They do not give us any clue as to the actual internal representation of the object There are many other useful ways to display an object as a string of characters This may be for the benefit of a human reader or because we want to export our data to a particular file format for use in an external program Formatted output typically contains a combination of variables and prespecified strings For example given a frequency distribution fdist we could do  fdist  nltkFreqDistdog cat dog cat dog snake dog cat  for word in fdist      print word  fdistword  dog  4  cat  3  snake  1  Apart from the problem of unwanted whitespace print statements that contain alter nating variables and constants can be difficult to read and maintain A better solution is to use string formatting expressions  for word in fdist     print sd  word fdistword dog4 cat3 snake1 To understand what is going on here lets test out the string formatting expression on its own By now this will be your usual method of exploring new syntax  sd  cat 3 cat3  sd  cat Traceback most recent call last File stdin line 1 in module TypeError not enough arguments for format string",
                    "word_count": 553
                },
                {
                    "title": "3.10 Summary",
                    "page": "120",
                    "subsections": [],
                    "content": "  In this book we view a text as a list of words A raw text is a potentially long string containing words and whitespace formatting and is how we typically store and visualize a text  A string is specified in Python using single or double quotes Monty Python Monty Python  The characters of a string are accessed using indexes counting from zero Monty Python0 gives the value M The length of a string is found using len  Substrings are accessed using slice notation Monty Python15 gives the value onty If the start index is omitted the substring begins at the start of the string if the end index is omitted the slice continues to the end of the string  Strings can be split into lists Monty Pythonsplit gives Monty Python Lists can be joined into strings joinMonty Python gives Monty Python  We can read text from a file f using text  openfread We can read text from a URL u using text  urlopenuread We can iterate over the lines of a text file using for line in openf  Texts found on the Web may contain unwanted material such as headers footers and markup that need to be removed before we do any linguistic processing  Tokenization is the segmentation of a text into basic unitsor tokenssuch as words and punctuation Tokenization based on whitespace is inadequate for many applications because it bundles punctuation together with words NLTK provides an offtheshelf tokenizer nltkwordtokenize  Lemmatization is a process that maps the various forms of a word such as ap peared appears to the canonical or citation form of the word also known as the lexeme or lemma eg appear  Regular expressions are a powerful and flexible method of specifying patterns Once we have imported the re module we can use refindall to find all sub strings in a string that match a pattern",
                    "word_count": 310
                },
                {
                    "title": "3.11 Further Reading",
                    "page": "122",
                    "subsections": [],
                    "content": " Extra materials for this chapter are posted at httpwwwnltkorg including links to freely available resources on the Web Remember to consult the Python reference ma terials at httpdocspythonorg For example this documentation covers universal newline support explaining how to work with the different newline conventions used by various operating systems For more examples of processing words with NLTK see the tokenization stemming and corpus HOWTOs at httpwwwnltkorghowto Chapters 2 and 3 of Jurafsky  Martin 2008 contain more advanced material on regular expressions and morphology For more extensive discussion of text processing with Python see Mertz 2003 For information about normalizing nonstandard words see Sproat et al 2001 There are many references for regular expressions both practical and theoretical For an introductory tutorial to using regular expressions in Python see Kuchlings Regular Expression HOWTO httpwwwamkcapythonhowtoregex For a comprehensive and detailed manual in using regular expressions covering their syntax in most major programming languages including Python see Friedl 2002 Other presentations in clude Section 21 of Jurafsky  Martin 2008 and Chapter 3 of Mertz 2003 There are many online resources for Unicode Useful discussions of Pythons facilities for handling Unicode are  PEP100 httpwwwpythonorgdevpepspep0100  Jason Orendorff Unicode for Programmers httpwwwjorendorffcomarticlesuni code  A M Kuchling Unicode HOWTO httpwwwamkcapythonhowtounicode  Frederik Lundh Python Unicode Objects httpeffbotorgzoneunicodeobjects htm  Joel Spolsky The Absolute Minimum Every Software Developer Absolutely Posi tively Must Know About Unicode and Character Sets No Excuses httpwwwjoe lonsoftwarecomarticlesUnicodehtml The problem of tokenizing Chinese text is a major focus of SIGHAN the ACL Special Interest Group on Chinese Language Processing httpsighanorg Our method for segmenting English text follows Brent  Cartwright 1995 this work falls in the area of language acquisition Niyogi 2006 Collocations are a special case of multiword expressions A multiword expression is a small phrase whose meaning and other properties cannot be predicted from its words alone eg partofspeech Baldwin  Kim 2010 Simulated annealing is a heuristic for finding a good approximation to the optimum value of a function in a large discrete search space based on an analogy with annealing in metallurgy The technique is described in many Artificial Intelligence texts The approach to discovering hyponyms in text using search patterns like x and other ys is described by Hearst 1992",
                    "word_count": 368
                },
                {
                    "title": "3.12 Exercises",
                    "page": "122",
                    "subsections": [],
                    "content": " 1  Define a string s  colorless Write a Python statement that changes this to colourless using only the slice and concatenation operations 2  We can use the slice notation to remove morphological endings on words For example dogs1 removes the last character of dogs leaving dog Use slice notation to remove the affixes from these words weve inserted a hyphen to indi cate the affix boundary but omit this from your strings dishes running nation ality undo preheat 3  We saw how we can generate an IndexError by indexing beyond the end of a string Is it possible to construct an index that goes too far to the left before the start of the string 4  We can specify a step size for the slice The following returns every second character within the slice monty6112 It also works in the reverse direction monty1052 Try these for yourself and then experiment with different step values 5  What happens if you ask the interpreter to evaluate monty1 Explain why this is a reasonable result 6  Describe the class of strings matched by the following regular expressions a azAZ b AZaz c paeiou2t d dd e aeiouaeiouaeiou f wws Test your answers using nltkreshow",
                    "word_count": 201
                }
            ]
        },
        {
            "title": "Chapter 4: Writing Structured Programs",
            "page": "129",
            "sections": [
                {
                    "title": "4.1 Back to the Basics",
                    "page": "130",
                    "subsections": [],
                    "content": " Assignment Assignment would seem to be the most elementary programming concept not deserv ing a separate discussion However there are some surprising subtleties here Consider the following code fragment  foo  Monty  bar  foo  foo  Python  bar Monty This behaves exactly as expected When we write bar  foo in the code  the value of foo the string Monty is assigned to bar That is bar is a copy of foo so when we overwrite foo with a new string Python on line  the value of bar is not affected However assignment statements do not always involve making copies in this way Assignment always copies the value of an expression but a value is not always what you might expect it to be In particular the value of a structured object such as a list is actually just a reference to the object In the following example assigns the refer ence of foo to the new variable bar Now when we modify something inside foo on line  we can see that the contents of bar have also been changed  foo  Monty Python  bar  foo  foo1  Bodkin  bar Monty Bodkin The line bar  foo  does not copy the contents of the variable only its object refer ence To understand what is going on here we need to know how lists are stored in the computers memory In Figure 41 we see that a list foo is a reference to an object stored at location 3133 which is itself a series of pointers to other locations holding strings When we assign bar  foo it is just the object reference 3133 that gets copied This behavior extends to other aspects of the language such as parameter passing Section 44 Lets experiment some more by creating a variable empty holding the empty list then using it three times on the next line  empty    nested  empty empty empty  nested     nested1appendPython  nested Python Python Python Observe that changing one of the items inside our nested list of lists changed them all This is because each of the three elements is actually just a reference to one and the same list in memory Your Turn Use multiplication to create a list of lists nested    3 Now modify one of the elements of the list and observe that all the elements are changed Use Pythons id function to find out the nu merical identifier for any object and verify that idnested0 idnested1 and idnested2 are all the same Now notice that when we assign a new value to one of the elements of the list it does not propagate to the others  nested    3  nested1appendPython  nested1  Monty  nested Python Monty Python We began with a list containing three references to a single empty list object Then we modified that object by appending Python to it resulting in a list containing three references to a single list object Python Next we overwrote one of those references with a reference to a new object Monty This last step modified one of the three object references inside the nested list However the Python object wasnt changed Figure 41 List assignment and computer memory Two list objects foo and bar reference the same location in the computers memory updating foo will also modify bar and vice versa",
                    "word_count": 543
                },
                {
                    "title": "4.2 Sequences",
                    "page": "132",
                    "subsections": [],
                    "content": " So far we have seen two kinds of sequence object strings and lists Another kind of sequence is called a tuple Tuples are formed with the comma operator  and typically enclosed using parentheses Weve actually seen them in the previous chapters and sometimes referred to them as pairs since there were always two members However tuples can have any number of members Like lists and strings tuples can be indexed and sliced  and have a length   t  walk fem 3  t walk fem 3",
                    "word_count": 84
                },
                {
                    "title": "4.3 Questions of Style",
                    "page": "138",
                    "subsections": [],
                    "content": " Programming is as much an art as a science The undisputed bible of programming a 2500 page multivolume work by Donald Knuth is called The Art of Computer Pro gramming Many books have been written on Literate Programming recognizing that humans not just computers must read and understand programs Here we pick up on some issues of programming style that have important ramifications for the readability of your code including code layout procedural versus declarative style and the use of loop variables Python Coding Style When writing programs you make many subtle choices about names spacing com ments and so on When you look at code written by other people needless differences in style make it harder to interpret the code Therefore the designers of the Python language have published a style guide for Python code available at httpwwwpython orgdevpepspep0008 The underlying value presented in the style guide is consis tency for the purpose of maximizing the readability of code We briefly review some of its key recommendations here and refer readers to the full guide for detailed dis cussion with examples Code layout should use four spaces per indentation level You should make sure that when you write Python code in a file you avoid tabs for indentation since these can be misinterpreted by different text editors and the indentation can be messed up Lines should be less than 80 characters long if necessary you can break a line inside paren theses brackets or braces because Python is able to detect that the line continues over to the next line as in the following examples  cvwordpairs  cv w for w in rotokaswords                           for cv in refindallptksvraeiou w  cfd  nltkConditionalFreqDist            genre word            for genre in browncategories            for word in brownwordscategoriesgenre   hawords  aaahhhh ah ahah ahahah ahh ahhahahaha              ahhh ahhhh ahhhhhh ahhhhhhhhhhhhhh ha              haaa hah haha hahaaa hahah hahaha If you need to break a line outside parentheses brackets or braces you can often add extra parentheses and you can always add a backslash at the end of the line that is broken  if lensyllables  4 and lensyllables2  3 and     syllables22 in aeiou and syllables23  syllables13      processsyllables  if lensyllables  4 and lensyllables2  3 and      syllables22 in aeiou and syllables23  syllables13      processsyllables Typing spaces instead of tabs soon becomes a chore Many program ming editors have builtin support for Python and can automatically indent code and highlight any syntax errors including indentation er rors For a list of Pythonaware editors please see httpwikipython orgmoinPythonEditors Procedural Versus Declarative Style We have just seen how the same task can be performed in different ways with impli cations for efficiency Another factor influencing program development is programming style Consider the following program to compute the average length of words in the Brown Corpus  tokens  nltkcorpusbrownwordscategoriesnews  count  0  total  0  for token in tokens      count  1      total  lentoken  print total  count 42765382469 In this program we use the variable count to keep track of the number of tokens seen and total to store the combined length of all words This is a lowlevel style not far removed from machine code the primitive operations performed by the computers CPU The two variables are just like a CPUs registers accumulating values at many intermediate stages values that are meaningless until the end We say that this program is written in a procedural style dictating the machine operations step by step Now consider the following program that computes the same thing",
                    "word_count": 572
                },
                {
                    "title": "4.4 Functions: The Foundation of Structured Programming",
                    "page": "142",
                    "subsections": [],
                    "content": " Functions provide an effective way to package and reuse program code as already explained in Section 23 For example suppose we find that we often want to read text from an HTML file This involves several steps opening the file reading it in normal izing whitespace and stripping HTML markup We can collect these steps into a func tion and give it a name such as gettext as shown in Example 41 Example 41 Read text from a file import re def gettextfile Read text from a file normalizing whitespace and stripping HTML markup text  openfileread text  resubs   text text  resubr   text return text Now any time we want to get cleanedup text from an HTML file we can just call gettext with the name of the file as its only argument It will return a string and we can assign this to a variable eg contents  gettexttesthtml Each time we want to use this series of steps we only have to call the function Using functions has the benefit of saving space in our program More importantly our choice of name for the function helps make the program readable In the case of the preceding example whenever our program needs to read cleanedup text from a file we dont have to clutter the program with four lines of code we simply need to call gettext This naming helps to provide some semantic interpretationit helps a reader of our program to see what the program means Notice that this example function definition contains a string The first string inside a function definition is called a docstring Not only does it document the purpose of the function to someone reading the code it is accessible to a programmer who has loaded the code from a file  helpgettext Help on function gettext gettextfile Read text from a file normalizing whitespace and stripping HTML markup We have seen that functions help to make our work reusable and readable They also help make it reliable When we reuse code that has already been developed and tested we can be more confident that it handles a variety of cases correctly We also remove the risk of forgetting some important step or introducing a bug The program that calls our function also has increased reliability The author of that program is dealing with a shorter program and its components behave transparently To summarize as its name suggests a function captures functionality It is a segment of code that can be given a meaningful name and which performs a welldefined task Functions allow us to abstract away from the details to see a bigger picture and to program more effectively The rest of this section takes a closer look at functions exploring the mechanics and discussing ways to make your programs easier to read Function Inputs and Outputs We pass information to functions using a functions parameters the parenthesized list of variables and constants following the functions name in the function definition Heres a complete example  def repeatmsg num      return  joinmsg  num  monty  Monty Python  repeatmonty 3 Monty Python Monty Python Monty Python We first define the function to take two parameters msg and num  Then we call the function and pass it two arguments monty and 3  these arguments fill the place holders provided by the parameters and provide values for the occurrences of msg and num in the function body It is not necessary to have any parameters as we see in the following example  def monty      return Monty Python  monty Monty Python",
                    "word_count": 588
                },
                {
                    "title": "4.5 Doing More with Functions",
                    "page": "148",
                    "subsections": [],
                    "content": " This section discusses more advanced features which you may prefer to skip on the first time through this chapter Functions As Arguments So far the arguments we have passed into functions have been simple objects such as strings or structured objects such as lists Python also lets us pass a function as an argument to another function Now we can abstract out the operation and apply a different operation on the same data As the following examples show we can pass the builtin function len or a userdefined function lastletter as arguments to an other function  sent  Take care of the sense  and the          sounds will take care of themselves   def extractpropertyprop      return propword for word in sent ",
                    "word_count": 118
                },
                {
                    "title": "4.6 Program Development",
                    "page": "154",
                    "subsections": [],
                    "content": " Programming is a skill that is acquired over several years of experience with a variety of programming languages and tasks Key highlevel abilities are algorithm design and its manifestation in structured programming Key lowlevel abilities include familiarity with the syntactic constructs of the language and knowledge of a variety of diagnostic methods for troubleshooting a program which does not exhibit the expected behavior This section describes the internal structure of a program module and how to organize a multimodule program Then it describes various kinds of error that arise during program development what you can do to fix them and better still to avoid them in the first place Structure of a Python Module The purpose of a program module is to bring logically related definitions and functions together in order to facilitate reuse and abstraction Python modules are nothing more than individual py files For example if you were working with a particular corpus format the functions to read and write the format could be kept together Constants used by both formats such as field separators or a EXTN  inf filename extension could be shared If the format was updated you would know that only one file needed to be changed Similarly a module could contain code for creating and manipulating a particular data structure such as syntax trees or code for performing a particular processing task such as plotting corpus statistics When you start writing Python modules it helps to have some examples to emulate You can locate the code for any NLTK module on your system using the file variable  nltkmetricsdistancefile usrlibpython25sitepackagesnltkmetricsdistancepyc This returns the location of the compiled pyc file for the module and youll probably see a different location on your machine The file that you will need to open is the corresponding py source file and this will be in the same directory as the pyc file Alternatively you can view the latest version of this module on the Web at httpcode googlecompnltksourcebrowsetrunknltknltkmetricsdistancepy Like every other NLTK module distancepy begins with a group of comment lines giving a oneline title of the module and identifying the authors Since the code is distributed it also includes the URL where the code is available a copyright statement and license information Next is the modulelevel docstring a triplequoted multiline string con taining information about the module that will be printed when someone types helpnltkmetricsdistance  Natural Language Toolkit Distance Metrics   Copyright C 20012009 NLTK Project  Author Edward Loper edlopergradientcisupennedu          Steven Bird sbcsseunimelbeduau          Tom Lippincott tomcscolumbiaedu  URL httpwwwnltkorg  For license information see LICENSETXT   Distance Metrics Compute the distance between two items usually strings As metrics they must satisfy the following three requirements 1 da a  0 2 da b  0 3 da c  da b  db c  After this comes all the import statements required for the module then any global variables followed by a series of function definitions that make up most of the module Other modules define classes the main building blocks of objectoriented program ming which falls outside the scope of this book Most NLTK modules also include a demo function which can be used to see examples of the module in use Some module variables and functions are only used within the module These should have names beginning with an underscore eg helper since this will hide the name If another module imports this one using the idiom from module import  these names will not be imported You can optionally list the externally accessible names of a module using a special builtin variable like this all  editdis tance jaccarddistance Multimodule Programs Some programs bring together a diverse range of tasks such as loading data from a corpus performing some analysis tasks on the data then visualizing it We may already",
                    "word_count": 625
                },
                {
                    "title": "4.7 Algorithm Design",
                    "page": "160",
                    "subsections": [],
                    "content": " This section discusses more advanced concepts which you may prefer to skip on the first time through this chapter A major part of algorithmic problem solving is selecting or adapting an appropriate algorithm for the problem at hand Sometimes there are several alternatives and choos ing the best one depends on knowledge about how each alternative performs as the size of the data grows Whole books are written on this topic and we only have space to introduce some key concepts and elaborate on the approaches that are most prevalent in natural language processing The bestknown strategy is known as divideandconquer We attack a problem of size n by dividing it into two problems of size n2 solve these problems and combine their results into a solution of the original problem For example suppose that we had a pile of cards with a single word written on each card We could sort this pile by splitting it in half and giving it to two other people to sort they could do the same in turn Then when two sorted piles come back it is an easy task to merge them into a single sorted pile See Figure 43 for an illustration of this process Another example is the process of looking up a word in a dictionary We open the book somewhere around the middle and compare our word with the current page If its earlier in the dictionary we repeat the process on the first half if its later we use the second half This search method is called binary search since it splits the problem in half at every step In another approach to algorithm design we attack a problem by transforming it into an instance of a problem we already know how to solve For example in order to detect duplicate entries in a list we can presort the list then scan through it once to check whether any adjacent pairs of elements are identical Recursion The earlier examples of sorting and searching have a striking property to solve a prob lem of size n we have to break it in half and then work on one or more problems of size n2 A common way to implement such methods uses recursion We define a function f which simplifies the problem and calls itself to solve one or more easier instances of the same problem It then combines the results into a solution for the original problem For example suppose we have a set of n words and want to calculate how many dif ferent ways they can be combined to make a sequence of words If we have only one word n1 there is just one way to make it into a sequence If we have a set of two words there are two ways to put them into a sequence For three words there are six possibilities In general for n words there are n  n1    2  1 ways ie the factorial of n We can code this up as follows  def factorial1n      result  1      for i in rangen          result  i1      return result However there is also a recursive algorithm for solving this problem based on the following observation Suppose we have a way to construct all orderings for n1 distinct words Then for each such ordering there are n places where we can insert a new word at the start the end or any of the n2 boundaries between the words Thus we simply multiply the number of solutions found for n1 by the value of n We also need the base case to say that if we have a single word theres just one ordering We can code this up as follows  def factorial2n      if n  1          return 1      else          return n  factorial2n1 Figure 43 Sorting by divideandconquer To sort an array we split it in half and sort each half recursively we merge each sorted half back into a whole list again recursively this algorithm is known as Merge Sort",
                    "word_count": 668
                },
                {
                    "title": "4.8 A Sample of Python Libraries",
                    "page": "166",
                    "subsections": [],
                    "content": " Python has hundreds of thirdparty libraries specialized software packages that extend the functionality of Python NLTK is one such library To realize the full power of Python programming you should become familiar with several other libraries Most of these will need to be manually installed on your computer",
                    "word_count": 48
                },
                {
                    "title": "4.9 Summary",
                    "page": "172",
                    "subsections": [],
                    "content": "  Pythons assignment and parameter passing use object references eg if a is a list and we assign b  a then any operation on a will modify b and vice versa  The is operation tests whether two objects are identical internal objects whereas  tests whether two objects are equivalent This distinction parallels the type token distinction  Strings lists and tuples are different kinds of sequence object supporting common operations such as indexing slicing len sorted and membership testing using in  We can write text to a file by opening the file for writing ofile  openoutputtxt w then adding content to the file ofilewriteMonty Python and finally closing the file ofileclose  A declarative programming style usually produces more compact readable code manually incremented loop variables are usually unnecessary When a sequence must be enumerated use enumerate  Functions are an essential programming abstraction key concepts to understand are parameter passing variable scope and docstrings  A function serves as a namespace names defined inside a function are not visible outside that function unless those names are declared to be global  Modules permit logically related material to be localized in a file A module serves as a namespace names defined in a modulesuch as variables and functions are not visible to other modules unless those names are imported  Dynamic programming is an algorithm design technique used widely in NLP that stores the results of previous computations in order to avoid unnecessary recomputation",
                    "word_count": 237
                },
                {
                    "title": "4.10 Further Reading",
                    "page": "172",
                    "subsections": [],
                    "content": " This chapter has touched on many topics in programming some specific to Python and some quite general Weve just scratched the surface and you may want to read more about these topics starting with the further materials for this chapter available at httpwwwnltkorg The Python website provides extensive documentation It is important to understand the builtin functions and standard types described at httpdocspythonorglibrary functionshtml and httpdocspythonorglibrarystdtypeshtml We have learned about generators and their importance for efficiency for information about iterators a closely related topic see httpdocspythonorglibraryitertoolshtml Consult your favorite Py thon book for more information on such topics An excellent resource for using Python for multimedia processing including working with sound files is Guzdial 2005 When using the online Python documentation be aware that your installed version might be different from the version of the documentation you are reading You can easily check what version you have with import sys sysversion Versionspecific documentation is available at httpwwwpythonorgdocversions Algorithm design is a rich field within computer science Some good starting points are Harel 2004 Levitin 2004 and Knuth 2006 Useful guidance on the practice of software development is provided in Hunt  Thomas 2000 and McConnell 2004",
                    "word_count": 195
                },
                {
                    "title": "4.11 Exercises",
                    "page": "172",
                    "subsections": [],
                    "content": " 1  Find out more about sequence objects using Pythons help facility In the inter preter type helpstr helplist and helptuple This will give you a full list of the functions supported by each type Some functions have special names flanked with underscores as the help documentation shows each such function corre sponds to something more familiar For example xgetitemy is just a long winded way of saying xy 2  Identify three operations that can be performed on both tuples and lists Identify three list operations that cannot be performed on tuples Name a context where using a list instead of a tuple generates a Python error",
                    "word_count": 106
                }
            ]
        },
        {
            "title": "Chapter 5: Categorizing and Tagging Words",
            "page": "179",
            "sections": [
                {
                    "title": "5.1 Using a Tagger",
                    "page": "180",
                    "subsections": [],
                    "content": " A partofspeech tagger or POS tagger processes a sequence of words and attaches a part of speech tag to each word dont forget to import nltk  text  nltkwordtokenizeAnd now for something completely different  nltkpostagtext And CC now RB for IN something NN completely RB different JJ Here we see that and is CC a coordinating conjunction now and completely are RB or adverbs for is IN a preposition something is NN a noun and different is JJ an adjective 179 NLTK provides documentation for each tag which can be queried using the tag eg nltkhelpupenntagsetRB or a regular expression eg nltkhelpupennbrowntagsetNN Some corpora have RE ADME files with tagset documentation see nltknamereadme sub stituting in the name of the corpus Lets look at another example this time including some homonyms  text  nltkwordtokenizeThey refuse to permit us to obtain the refuse permit  nltkpostagtext They PRP refuse VBP to TO permit VB us PRP to TO obtain VB the DT refuse NN permit NN Notice that refuse and permit both appear as a present tense verb VBP and a noun NN Eg refUSE is a verb meaning deny while REFuse is a noun meaning trash ie they are not homophones Thus we need to know which word is being used in order to pronounce the text correctly For this reason texttospeech systems usually perform POS tagging Your Turn Many words like ski and race can be used as nouns or verbs with no difference in pronunciation Can you think of others Hint think of a commonplace object and try to put the word to before it to see if it can also be a verb or think of an action and try to put the before it to see if it can also be a noun Now make up a sentence with both uses of this word and run the POS tagger on this sentence Lexical categories like noun and partofspeech tags like NN seem to have their uses but the details will be obscure to many readers You might wonder what justification there is for introducing this extra level of information Many of these categories arise from superficial analysis of the distribution of words in text Consider the following analysis involving woman a noun bought a verb over a preposition and the a determiner The textsimilar method takes a word w finds all contexts w1w w2 then finds all words w that appear in the same context ie w1ww2  text  nltkTextwordlower for word in nltkcorpusbrownwords  textsimilarwoman Building wordcontext index man time day year car moment world family house country child boy state job way war girl place room word  textsimilarbought made said put done seen had found left given heard brought got been was set told took in felt that  textsimilarover in on to of and for with from at by that into as up out down through is all about  textsimilarthe a his this their its her an that our any all one these my in your no some other and Observe that searching for woman finds nouns searching for bought mostly finds verbs searching for over generally finds prepositions searching for the finds several deter miners A tagger can correctly identify the tags on these words in the context of a sentence eg The woman bought over 150000 worth of clothes A tagger can also model our knowledge of unknown words for example we can guess that scrobbling is probably a verb with the root scrobble and likely to occur in contexts like he was scrobbling",
                    "word_count": 587
                },
                {
                    "title": "5.2 Tagged Corpora",
                    "page": "180",
                    "subsections": [],
                    "content": " Representing Tagged Tokens By convention in NLTK a tagged token is represented using a tuple consisting of the token and the tag We can create one of these special tuples from the standard string representation of a tagged token using the function str2tuple  taggedtoken  nltktagstr2tupleflyNN  taggedtoken fly NN  taggedtoken0 fly  taggedtoken1 NN We can construct a list of tagged tokens directly from a string The first step is to tokenize the string to access the individual wordtag strings and then to convert each of these into a tuple using str2tuple  sent    TheAT grandJJ juryNN commentedVBD onIN aAT numberNN ofIN  otherAP topicsNNS  AMONGIN themPPO theAT AtlantaNP andCC  FultonNPtl CountyNNtl purchasingVBG departmentsNNS whichWDT itPPS  saidVBD  AREBER wellQL operatedVBN andCC followVB generallyRB  acceptedVBN practicesNNS whichWDT inureVB toIN theAT bestJJT  interestNN ofIN bothABX governmentsNNS      nltktagstr2tuplet for t in sentsplit The AT grand JJ jury NN commented VBD on IN a AT number NN    Reading Tagged Corpora Several of the corpora included with NLTK have been tagged for their partofspeech Heres an example of what you might see if you opened a file from the Brown Corpus with a text editor Theat Fultonnptl Countynntl Grandjjtl Jurynntl saidvbd Fridaynr anat inves tigationnn ofin Atlantasnp recentjj primarynn electionnn producedvbd  noat evidencenn  thatcs anydti irregularitiesnns tookvbd placenn ",
                    "word_count": 209
                },
                {
                    "title": "5.3 Mapping Words to Properties Using Python Dictionaries",
                    "page": "188",
                    "subsections": [],
                    "content": " As we have seen a tagged word of the form word tag is an association between a word and a partofspeech tag Once we start doing partofspeech tagging we will be creating programs that assign a tag to a word the tag which is most likely in a given context We can think of this process as mapping from words to tags The most natural way to store mappings in Python uses the socalled dictionary data type also known as an associative array or hash array in other programming languages In this sec tion we look at dictionaries and see how they can represent a variety of language in formation including partsofspeech Indexing Lists Versus Dictionaries A text as we have seen is treated in Python as a list of words An important property of lists is that we can look up a particular item by giving its index eg text1100 Notice how we specify a number and get back a word We can think of a list as a simple kind of table as shown in Figure 52 Figure 52 List lookup We access the contents of a Python list with the help of an integer index",
                    "word_count": 198
                },
                {
                    "title": "5.4 Automatic Tagging",
                    "page": "198",
                    "subsections": [],
                    "content": " In the rest of this chapter we will explore various ways to automatically add partof speech tags to text We will see that the tag of a word depends on the word and its context within a sentence For this reason we will be working with data at the level of tagged sentences rather than words Well begin by loading the data we will be using  from nltkcorpus import brown  browntaggedsents  browntaggedsentscategoriesnews  brownsents  brownsentscategoriesnews The Default Tagger The simplest possible tagger assigns the same tag to each token This may seem to be a rather banal step but it establishes an important baseline for tagger performance In order to get the best result we tag each word with the most likely tag Lets find out which tag is most likely now using the unsimplified tagset  tags  tag for word tag in browntaggedwordscategoriesnews  nltkFreqDisttagsmax NN Now we can create a tagger that tags everything as NN  raw  I do not like green eggs and ham I do not like them Sam I am  tokens  nltkwordtokenizeraw  defaulttagger  nltkDefaultTaggerNN  defaulttaggertagtokens I NN do NN not NN like NN green NN eggs NN and NN ham NN  NN I NN do NN not NN like NN them NN Sam NN I NN am NN  NN Unsurprisingly this method performs rather poorly On a typical corpus it will tag only about an eighth of the tokens correctly as we see here  defaulttaggerevaluatebrowntaggedsents 013089484257215028 Default taggers assign their tag to every single word even words that have never been encountered before As it happens once we have processed several thousand words of English text most new words will be nouns As we will see this means that default taggers can help to improve the robustness of a language processing system We will return to them shortly The Regular Expression Tagger The regular expression tagger assigns tags to tokens on the basis of matching patterns For instance we might guess that any word ending in ed is the past participle of a verb and any word ending with s is a possessive noun We can express these as a list of regular expressions  patterns        ring VBG                gerunds      red VBD                 simple past      res VBZ                 3rd singular present      rould MD                modals      rs NN                possessive nouns      rs NNS                  plural nouns      r0909 CD   cardinal numbers      r NN                      nouns default   Note that these are processed in order and the first one that matches is applied Now we can set up a tagger and use it to tag a sentence After this step it is correct about a fifth of the time  regexptagger  nltkRegexpTaggerpatterns  regexptaggertagbrownsents3  NN Only NN a NN relative NN handful NN of NN such NN reports NNS was NNS received VBD  NN  NN the NN jury NN said NN  NN  NN considering VBG the NN widespread NN   regexptaggerevaluatebrowntaggedsents 020326391789486245 The final regular expression  is a catchall that tags everything as a noun This is equivalent to the default tagger only much less efficient Instead of respecifying this as part of the regular expression tagger is there a way to combine this tagger with the default tagger We will see how to do this shortly",
                    "word_count": 524
                },
                {
                    "title": "5.5 N-Gram Tagging",
                    "page": "206",
                    "subsections": [],
                    "content": " Unigram Tagging Unigram taggers are based on a simple statistical algorithm for each token assign the tag that is most likely for that particular token For example it will assign the tag JJ to any occurrence of the word frequent since frequent is used as an adjective eg a fre quent word more often than it is used as a verb eg I frequent this cafe A unigram tagger behaves just like a lookup tagger Section 54 except there is a more convenient Figure 54 Lookup tagger technique for setting it up called training In the following code sample we train a unigram tagger use it to tag a sentence and then evaluate  from nltkcorpus import brown  browntaggedsents  browntaggedsentscategoriesnews  brownsents  brownsentscategoriesnews  unigramtagger  nltkUnigramTaggerbrowntaggedsents  unigramtaggertagbrownsents2007 Various JJ of IN the AT apartments NNS are BER of IN the AT terrace NN type NN   being BEG on IN the AT ground NN floor NN so QL that CS entrance NN is BEZ direct JJ    unigramtaggerevaluatebrowntaggedsents 09349006503968017 We train a UnigramTagger by specifying tagged sentence data as a parameter when we initialize the tagger The training process involves inspecting the tag of each word and storing the most likely tag for any word in a dictionary that is stored inside the tagger Separating the Training and Testing Data Now that we are training a tagger on some data we must be careful not to test it on the same data as we did in the previous example A tagger that simply memorized its training data and made no attempt to construct a general model would get a perfect score but would be useless for tagging new text Instead we should split the data training on 90 and testing on the remaining 10  size  intlenbrowntaggedsents  09  size 4160  trainsents  browntaggedsentssize  testsents  browntaggedsentssize  unigramtagger  nltkUnigramTaggertrainsents  unigramtaggerevaluatetestsents 081202033290142528 Although the score is worse we now have a better picture of the usefulness of this tagger ie its performance on previously unseen text General NGram Tagging When we perform a language processing task based on unigrams we are using one item of context In the case of tagging we consider only the current token in isolation from any larger context Given such a model the best we can do is tag each word with its a priori most likely tag This means we would tag a word such as wind with the same tag regardless of whether it appears in the context the wind or to wind An ngram tagger is a generalization of a unigram tagger whose context is the current word together with the partofspeech tags of the n1 preceding tokens as shown in Figure 55 The tag to be chosen tn is circled and the context is shaded in grey In the example of an ngram tagger shown in Figure 55 we have n3 that is we consider the tags of the two preceding words in addition to the current word An ngram tagger picks the tag that is most likely in the given context A 1gram tagger is another term for a unigram tagger ie the context used to tag a token is just the text of the token itself 2gram taggers are also called bigram taggers and 3gram taggers are called trigram taggers The NgramTagger class uses a tagged training corpus to determine which partofspeech tag is most likely for each context Here we see a special case of an ngram tagger namely a bigram tagger First we train it then use it to tag untagged sentences  bigramtagger  nltkBigramTaggertrainsents  bigramtaggertagbrownsents2007 Various JJ of IN the AT apartments NNS are BER of IN the AT terrace NN type NN   being BEG on IN the AT ground NN floor NN so CS that CS entrance NN is BEZ direct JJ    unseensent  brownsents4203  bigramtaggertagunseensent The AT population NN of IN the AT Congo NP is BEZ 135 None million None  None divided None into None at None least None seven None major None  None culture None clusters None  None and None innumerable None tribes None speaking None 400 None separate None dialects None  None Notice that the bigram tagger manages to tag every word in a sentence it saw during training but does badly on an unseen sentence As soon as it encounters a new word ie 135 it is unable to assign a tag It cannot tag the following word ie million even if it was seen during training simply because it never saw it during training with a None tag on the previous word Consequently the tagger fails to tag the rest of the sentence Its overall accuracy score is very low  bigramtaggerevaluatetestsents 010276088906608193 Figure 55 Tagger context As n gets larger the specificity of the contexts increases as does the chance that the data we wish to tag contains contexts that were not present in the training data This is known as the sparse data problem and is quite pervasive in NLP As a consequence there is a tradeoff between the accuracy and the coverage of our results and this is related to the precisionrecall tradeoff in information retrieval Caution Ngram taggers should not consider context that crosses a sentence boundary Accordingly NLTK taggers are designed to work with lists of sentences where each sentence is a list of words At the start of a sentence tn1 and preceding tags are set to None Combining Taggers One way to address the tradeoff between accuracy and coverage is to use the more accurate algorithms when we can but to fall back on algorithms with wider coverage when necessary For example we could combine the results of a bigram tagger a unigram tagger and a default tagger as follows 1 Try tagging the token with the bigram tagger 2 If the bigram tagger is unable to find a tag for the token try the unigram tagger 3 If the unigram tagger is also unable to find a tag use a default tagger Most NLTK taggers permit a backoff tagger to be specified The backoff tagger may itself have a backoff tagger  t0  nltkDefaultTaggerNN  t1  nltkUnigramTaggertrainsents backofft0  t2  nltkBigramTaggertrainsents backofft1  t2evaluatetestsents 084491179108940495 Your Turn Extend the preceding example by defining a TrigramTag ger called t3 which backs off to t2 Note that we specify the backoff tagger when the tagger is initialized so that training can take advantage of the backoff tagger Thus if the bigram tagger would assign the same tag as its unigram backoff tagger in a certain context the bigram tagger discards the training instance This keeps the bigram tagger model as small as possible We can further specify that a tagger needs to see more than one instance of a context in order to retain it For example nltkBigramTaggersents cutoff2 backofft1 will dis card contexts that have only been seen once or twice Tagging Unknown Words Our approach to tagging unknown words still uses backoff to a regular expression tagger or a default tagger These are unable to make use of context Thus if our tagger encountered the word blog not seen during training it would assign it the same tag regardless of whether this word appeared in the context the blog or to blog How can we do better with these unknown words or outofvocabulary items A useful method to tag unknown words based on context is to limit the vocabulary of a tagger to the most frequent n words and to replace every other word with a special word UNK using the method shown in Section 53 During training a unigram tagger will probably learn that UNK is usually a noun However the ngram taggers will detect contexts in which it has some other tag For example if the preceding word is to tagged TO then UNK will probably be tagged as a verb Storing Taggers Training a tagger on a large corpus may take a significant time Instead of training a tagger every time we need one it is convenient to save a trained tagger in a file for later reuse Lets save our tagger t2 to a file t2pkl  from cPickle import dump  output  opent2pkl wb  dumpt2 output 1  outputclose Now in a separate Python process we can load our saved tagger  from cPickle import load  input  opent2pkl rb  tagger  loadinput  inputclose Now lets check that it can be used for tagging  text  The boards action shows what free enterprise      is up against in our complex maze of regulatory laws   tokens  textsplit  taggertagtokens The AT boards NN action NN shows NNS what WDT free JJ enterprise NN is BEZ up RP against IN in IN our PP complex JJ maze NN of IN regulatory NN laws NNS   Performance Limitations What is the upper limit to the performance of an ngram tagger Consider the case of a trigram tagger How many cases of partofspeech ambiguity does it encounter We can determine the answer to this question empirically  cfd  nltkConditionalFreqDist             x1 y1 z0 z1             for sent in browntaggedsents             for x y z in nltktrigramssent  ambiguouscontexts  c for c in cfdconditions if lencfdc  1  sumcfdcN for c in ambiguouscontexts  cfdN 0049297702068029296 Thus 1 out of 20 trigrams is ambiguous Given the current word and the previous two tags in 5 of cases there is more than one tag that could be legitimately assigned to the current word according to the training data Assuming we always pick the most likely tag in such ambiguous contexts we can derive a lower bound on the performance of a trigram tagger Another way to investigate the performance of a tagger is to study its mistakes Some tags may be harder than others to assign and it might be possible to treat them specially by pre or postprocessing the data A convenient way to look at tagging errors is the confusion matrix It charts expected tags the gold standard against actual tags gen erated by a tagger  testtags  tag for sent in brownsentscategorieseditorial                   for word tag in t2tagsent  goldtags  tag for word tag in browntaggedwordscategorieseditorial  print nltkConfusionMatrixgold test Based on such analysis we may decide to modify the tagset Perhaps a distinction be tween tags that is difficult to make can be dropped since it is not important in the context of some larger processing task Another way to analyze the performance bound on a tagger comes from the less than 100 agreement between human annotators In general observe that the tagging process collapses distinctions eg lexical identity is usually lost when all personal pronouns are tagged PRP At the same time the tagging process introduces new distinctions and removes ambiguities eg deal tagged as VB or NN This characteristic of collapsing certain distinctions and introducing new distinc tions is an important feature of tagging which facilitates classification and prediction When we introduce finer distinctions in a tagset an ngram tagger gets more detailed information about the leftcontext when it is deciding what tag to assign to a particular word However the tagger simultaneously has to do more work to classify the current token simply because there are more tags to choose from Conversely with fewer dis tinctions as with the simplified tagset the tagger has less information about context and it has a smaller range of choices in classifying the current token We have seen that ambiguity in the training data leads to an upper limit in tagger performance Sometimes more context will resolve the ambiguity In other cases how ever as noted by Abney 1996 the ambiguity can be resolved only with reference to syntax or to world knowledge Despite these imperfections partofspeech tagging has played a central role in the rise of statistical approaches to natural language processing In the early 1990s the surprising accuracy of statistical taggers was a striking demonstration that it was possible to solve one small part of the language understand ing problem namely partofspeech disambiguation without reference to deeper sour ces of linguistic knowledge Can this idea be pushed further In Chapter 7 we will see that it can Tagging Across Sentence Boundaries An ngram tagger uses recent tags to guide the choice of tag for the current word When tagging the first word of a sentence a trigram tagger will be using the partofspeech tag of the previous two tokens which will normally be the last word of the previous sentence and the sentenceending punctuation However the lexical category that closed the previous sentence has no bearing on the one that begins the next sentence To deal with this situation we can train run and evaluate taggers using lists of tagged sentences as shown in Example 55 Example 55 Ngram tagging at the sentence level browntaggedsents  browntaggedsentscategoriesnews brownsents  brownsentscategoriesnews size  intlenbrowntaggedsents  09 trainsents  browntaggedsentssize testsents  browntaggedsentssize t0  nltkDefaultTaggerNN t1  nltkUnigramTaggertrainsents backofft0 t2  nltkBigramTaggertrainsents backofft1  t2evaluatetestsents 084491179108940495",
                    "word_count": 2120
                },
                {
                    "title": "5.6 Transformation-Based Tagging",
                    "page": "500",
                    "subsections": [],
                    "content": " A potential issue with ngram taggers is the size of their ngram table or language model If tagging is to be employed in a variety of language technologies deployed on mobile computing devices it is important to strike a balance between model size and tagger performance An ngram tagger with backoff may store trigram and bigram ta bles which are large sparse arrays that may have hundreds of millions of entries A second issue concerns context The only information an ngram tagger considers from prior context is tags even though words themselves might be a useful source of information It is simply impractical for ngram models to be conditioned on the iden tities of words in the context In this section we examine Brill tagging an inductive tagging method which performs very well using models that are only a tiny fraction of the size of ngram taggers Brill tagging is a kind of transformationbased learning named after its inventor The general idea is very simple guess the tag of each word then go back and fix the mistakes In this way a Brill tagger successively transforms a bad tagging of a text into a better one As with ngram tagging this is a supervised learning method since we need an notated training data to figure out whether the taggers guess is a mistake or not How ever unlike ngram tagging it does not count observations but compiles a list of trans formational correction rules The process of Brill tagging is usually explained by analogy with painting Suppose we were painting a tree with all its details of boughs branches twigs and leaves against a uniform skyblue background Instead of painting the tree first and then trying to paint blue in the gaps it is simpler to paint the whole canvas blue then correct the tree section by overpainting the blue background In the same fashion we might paint the trunk a uniform brown before going back to overpaint further details with even finer brushes Brill tagging uses the same idea begin with broad brush strokes and then fix up the details with successively finer changes Lets look at an example in volving the following sentence 1 The President said he will ask Congress to increase grants to states for voca tional rehabilitation We will examine the operation of two rules a replace NN with VB when the previous word is TO b replace TO with IN when the next tag is NNS Table 56 illustrates this process first tagging with the unigram tagger then applying the rules to fix the errors Table 56 Steps in Brill tagging Phrase to increase grants to states for vocational rehabilitation Unigram TO NN NNS TO NNS IN JJ NN Rule 1  VB       Rule 2    IN     Output TO VB NNS IN NNS IN JJ NN Gold TO VB NNS IN NNS IN JJ NN In this table we see two rules All such rules are generated from a template of the following form replace T1 with T2 in the context C Typical contexts are the identity or the tag of the preceding or following word or the appearance of a specific tag within two to three words of the current word During its training phase the tagger guesses values for T1 T2 and C to create thousands of candidate rules Each rule is scored according to its net benefit the number of incorrect tags that it corrects less the number of correct tags it incorrectly modifies Brill taggers have another interesting property the rules are linguistically interpretable Compare this with the ngram taggers which employ a potentially massive table of n grams We cannot learn much from direct inspection of such a table in comparison to the rules learned by the Brill tagger Example 56 demonstrates NLTKs Brill tagger Example 56 Brill tagger demonstration The tagger has a collection of templates of the form X  Y if the preceding word is Z the variables in these templates are instantiated to particular words and tags to create rules the score for a rule is the number of broken examples it corrects minus the number of correct cases it breaks apart from training a tagger the demonstration displays residual errors  nltktagbrilldemo Training Brill tagger on 80 sentences Finding initial useful rules Found 6555 useful rules B       S   F   r   O          Score  Fixed  Broken c   i   o   t    R     Fixed  num tags changed incorrect  correct o   x   k   h    u     Broken  num tags changed correct  incorrect r   e   e   e    l     Other  num tags changed incorrect  incorrect e   d   n   r    e  12  13   1   4   NN  VB if the tag of the preceding word is TO 8   9   1  23   NN  VBD if the tag of the following word is DT 8   8   0   9   NN  VBD if the tag of the preceding word is NNS 6   9   3  16   NN  NNP if the tag of words i2i1 is NONE 5   8   3   6   NN  NNP if the tag of the following word is NNP 5   6   1   0   NN  NNP if the text of words i2i1 is like 5   5   0   3   NN  VBN if the text of the following word is 1   printopenerrorsoutread left context     wordtestgold      right context        ThenNNRB         inIN theDT guestsN  inIN theDT guestsNNS        VBDPOS        honorNN  theDT speed POS honorNN  theDT     speedwayJJNN      hauledVBD outRP fourCD NN  theDT speedwayNN      hauledNNVBD      outRP fourCD driversNN DT speedwayNN hauledVBD       outNNPRP        fourCD driversNNS  c dwayNN hauledVBD outRP       fourNNPCD       driversNNS  crewsNNS hauledVBD outRP fourCD     driversNNPNNS      crewsNNS andCC even P fourCD driversNNS       crewsNNNNS       andCC evenRB theDT off NNS andCC evenRB theDT     officialNNPJJ     IndianapolisNNP 500CD a      AfterVBDIN       theDT raceNN  Fortun sNNS drooledVBD likeIN   schoolboysNNPNNS    overIN theDT carsNNS a olboysNNS overIN theDT       carsNNNNS       andCC driversNNS ",
                    "word_count": 952
                },
                {
                    "title": "5.7 How to Determine the Category of a Word",
                    "page": "210",
                    "subsections": [],
                    "content": " Now that we have examined word classes in detail we turn to a more basic question how do we decide what category a word belongs to in the first place In general linguists use morphological syntactic and semantic clues to determine the category of a word Morphological Clues The internal structure of a word may give useful clues as to the words category For example ness is a suffix that combines with an adjective to produce a noun eg happy  happiness ill  illness So if we encounter a word that ends in ness this is very likely to be a noun Similarly ment is a suffix that combines with some verbs to produce a noun eg govern  government and establish  establishment English verbs can also be morphologically complex For instance the present par ticiple of a verb ends in ing and expresses the idea of ongoing incomplete action eg falling eating The ing suffix also appears on nouns derived from verbs eg the falling of the leaves this is known as the gerund Syntactic Clues Another source of information is the typical contexts in which a word can occur For example assume that we have already determined the category of nouns Then we might say that a syntactic criterion for an adjective in English is that it can occur im mediately before a noun or immediately following the words be or very According to these tests near should be categorized as an adjective 2 a the near window b The end is very near Semantic Clues Finally the meaning of a word is a useful clue as to its lexical category For example the bestknown definition of a noun is semantic the name of a person place or thing Within modern linguistics semantic criteria for word classes are treated with suspicion mainly because they are hard to formalize Nevertheless semantic criteria underpin many of our intuitions about word classes and enable us to make a good guess about the categorization of words in languages with which we are unfamiliar For example if all we know about the Dutch word verjaardag is that it means the same as the English word birthday then we can guess that verjaardag is a noun in Dutch However some care is needed although we might translate zij is vandaag jarig as its her birthday to day the word jarig is in fact an adjective in Dutch and has no exact equivalent in English New Words All languages acquire new lexical items A list of words recently added to the Oxford Dictionary of English includes cyberslacker fatoush blamestorm SARS cantopop bupkis noughties muggle and robata Notice that all these new words are nouns and this is reflected in calling nouns an open class By contrast prepositions are regarded as a closed class That is there is a limited set of words belonging to the class eg above along at below beside between during for from in near on outside over",
                    "word_count": 494
                },
                {
                    "title": "5.8 Summary",
                    "page": "212",
                    "subsections": [],
                    "content": "  Words can be grouped into classes such as nouns verbs adjectives and adverbs These classes are known as lexical categories or partsofspeech Partsofspeech are assigned short labels or tags such as NN and VB  The process of automatically assigning partsofspeech to words in text is called partofspeech tagging POS tagging or just tagging  Automatic tagging is an important step in the NLP pipeline and is useful in a variety of situations including predicting the behavior of previously unseen words ana lyzing word usage in corpora and texttospeech systems  Some linguistic corpora such as the Brown Corpus have been POS tagged  A variety of tagging methods are possible eg default tagger regular expression tagger unigram tagger and ngram taggers These can be combined using a tech nique known as backoff  Taggers can be trained and evaluated using tagged corpora  Backoff is a method for combining models when a more specialized model such as a bigram tagger cannot assign a tag in a given context we back off to a more general model such as a unigram tagger  Partofspeech tagging is an important early example of a sequence classification task in NLP a classification decision at any one point in the sequence makes use of words and tags in the local context  A dictionary is used to map between arbitrary types of information such as a string and a number freqcat  12 We create dictionaries using the brace notation pos   pos  furiously adv ideas n colorless adj  Ngram taggers can be defined for large values of n but once n is larger than 3 we usually encounter the sparse data problem even with a large quantity of training data we see only a tiny fraction of possible contexts",
                    "word_count": 286
                },
                {
                    "title": "5.9 Further Reading",
                    "page": "214",
                    "subsections": [],
                    "content": " Extra materials for this chapter are posted at httpwwwnltkorg including links to freely available resources on the Web For more examples of tagging with NLTK please see the Tagging HOWTO at httpwwwnltkorghowto Chapters 4 and 5 of Jurafsky  Martin 2008 contain more advanced material on ngrams and partofspeech tag ging Other approaches to tagging involve machine learning methods Chapter 6 In Chapter 7 we will see a generalization of tagging called chunking in which a contiguous sequence of words is assigned a single tag For tagset documentation see nltkhelpupenntagset and nltkhelpbrowntag set Lexical categories are introduced in linguistics textbooks including those listed in Chapter 1 of this book There are many other kinds of tagging Words can be tagged with directives to a speech synthesizer indicating which words should be emphasized Words can be tagged with sense numbers indicating which sense of the word was used Words can also be tagged with morphological features Examples of each of these kinds of tags are shown in the following list For space reasons we only show the tag for a single word Note also that the first two examples use XMLstyle tags where elements in angle brackets enclose the word that is tagged Speech Synthesis Markup Language W3C SSML That is a emphasisbigemphasis car SemCor Brown Corpus tagged with WordNet senses Space in any wf posNN lemmaform wnsn4formwf is completely meas ured by the three dimensions Wordnet formnn sense 4 shape form config uration contour conformation Morphological tagging from the Turin University Italian Treebank E italiano  come progetto e realizzazione  il primo PRIMO ADJ ORDIN M SING porto turistico dell Albania  Note that tagging is also performed at higher levels Here is an example of dialogue act tagging from the NPS Chat Corpus Forsyth  Martell 2007 included with NLTK Each turn of the dialogue is categorized as to its communicative function Statement  User117 Dude I wanted some of that ynQuestion User120 m I missing something Bye        User117 Im gonna go fix food Ill be back later System     User122 JOIN System     User2   slaps User122 around a bit with a large trout Statement  User121 18m pm me if u tryin to chat",
                    "word_count": 358
                },
                {
                    "title": "5.10 Exercises",
                    "page": "214",
                    "subsections": [],
                    "content": " 1  Search the Web for spoof newspaper headlines to find such gems as British Left Waffles on Falkland Islands and Juvenile Court to Try Shooting Defendant Manually tag these headlines to see whether knowledge of the partofspeech tags removes the ambiguity 2  Working with someone else take turns picking a word that can be either a noun or a verb eg contest the opponent has to predict which one is likely to be the most frequent in the Brown Corpus Check the opponents prediction and tally the score over several turns 3  Tokenize and tag the following sentence They wind back the clock while we chase after the wind What different pronunciations and partsofspeech are involved 4  Review the mappings in Table 54 Discuss any other examples of mappings you can think of What type of information do they map from and to 5  Using the Python interpreter in interactive mode experiment with the dictionary examples in this chapter Create a dictionary d and add some entries What hap pens whether you try to access a nonexistent entry eg dxyz 6  Try deleting an element from a dictionary d using the syntax del dabc Check that the item was deleted 7  Create two dictionaries d1 and d2 and add some entries to each Now issue the command d1updated2 What did this do What might it be useful for 8  Create a dictionary e to represent a single lexical entry for some word of your choice Define keys such as headword partofspeech sense and example and as sign them suitable values 9  Satisfy yourself that there are restrictions on the distribution of go and went in the sense that they cannot be freely interchanged in the kinds of contexts illustrated in 3 Section 57 10  Train a unigram tagger and run it on some new text Observe that some words are not assigned a tag Why not 11  Learn about the affix tagger type helpnltkAffixTagger Train an affix tagger and run it on some new text Experiment with different settings for the affix length and the minimum word length Discuss your findings 12  Train a bigram tagger with no backoff tagger and run it on some of the training data Next run it on some new data What happens to the performance of the tagger Why 13  We can use a dictionary to specify the values to be substituted into a formatting string Read Pythons library documentation for formatting strings httpdocspy",
                    "word_count": 410
                }
            ]
        },
        {
            "title": "Chapter 6: Learning to Classify Text",
            "page": "221",
            "sections": [
                {
                    "title": "6.1 Supervised Classification",
                    "page": "222",
                    "subsections": [],
                    "content": " Classification is the task of choosing the correct class label for a given input In basic classification tasks each input is considered in isolation from all other inputs and the set of labels is defined in advance Some examples of classification tasks are 221  Deciding whether an email is spam or not  Deciding what the topic of a news article is from a fixed list of topic areas such as sports technology and politics  Deciding whether a given occurrence of the word bank is used to refer to a river bank a financial institution the act of tilting to the side or the act of depositing something in a financial institution The basic classification task has a number of interesting variants For example in multi class classification each instance may be assigned multiple labels in openclass clas sification the set of labels is not defined in advance and in sequence classification a list of inputs are jointly classified A classifier is called supervised if it is built based on training corpora containing the correct label for each input The framework used by supervised classification is shown in Figure 61 Figure 61 Supervised classification a During training a feature extractor is used to convert each input value to a feature set These feature sets which capture the basic information about each input that should be used to classify it are discussed in the next section Pairs of feature sets and labels are fed into the machine learning algorithm to generate a model b During prediction the same feature extractor is used to convert unseen inputs to feature sets These feature sets are then fed into the model which generates predicted labels In the rest of this section we will look at how classifiers can be employed to solve a wide variety of tasks Our discussion is not intended to be comprehensive but to give a representative sample of tasks that can be performed with the help of text classifiers Gender Identification In Section 24 we saw that male and female names have some distinctive characteristics Names ending in a e and i are likely to be female while names ending in k o r s and t are likely to be male Lets build a classifier to model these differences more precisely The first step in creating a classifier is deciding what features of the input are relevant and how to encode those features For this example well start by just looking at the final letter of a given name The following feature extractor function builds a dic tionary containing relevant information about a given name  def genderfeaturesword      return lastletter word1  genderfeaturesShrek lastletter k The dictionary that is returned by this function is called a feature set and maps from features names to their values Feature names are casesensitive strings that typically provide a short humanreadable description of the feature Feature values are values with simple types such as Booleans numbers and strings Most classification methods require that features be encoded using sim ple value types such as Booleans numbers and strings But note that just because a feature has a simple type this does not necessarily mean that the features value is simple to express or compute indeed it is even possible to use very complex and informative values such as the output of a second supervised classifier as features Now that weve defined a feature extractor we need to prepare a list of examples and corresponding class labels  from nltkcorpus import names  import random  names  name male for name in nameswordsmaletxt            name female for name in nameswordsfemaletxt  randomshufflenames Next we use the feature extractor to process the names data and divide the resulting list of feature sets into a training set and a test set The training set is used to train a new naive Bayes classifier  featuresets  genderfeaturesn g for ng in names  trainset testset  featuresets500 featuresets500  classifier  nltkNaiveBayesClassifiertraintrainset We will learn more about the naive Bayes classifier later in the chapter For now lets just test it out on some names that did not appear in its training data  classifierclassifygenderfeaturesNeo male  classifierclassifygenderfeaturesTrinity female Observe that these character names from The Matrix are correctly classified Although this science fiction movie is set in 2199 it still conforms with our expectations about names and genders We can systematically evaluate the classifier on a much larger quantity of unseen data",
                    "word_count": 730
                },
                {
                    "title": "6.2 Further Examples of Supervised Classification",
                    "page": "232",
                    "subsections": [],
                    "content": " Sentence Segmentation Sentence segmentation can be viewed as a classification task for punctuation whenever we encounter a symbol that could possibly end a sentence such as a period or a question mark we have to decide whether it terminates the preceding sentence",
                    "word_count": 42
                },
                {
                    "title": "6.3 Evaluation",
                    "page": "236",
                    "subsections": [],
                    "content": " In order to decide whether a classification model is accurately capturing a pattern we must evaluate that model The result of this evaluation is important for deciding how trustworthy the model is and for what purposes we can use it Evaluation can also be an effective tool for guiding us in making future improvements to the model The Test Set Most evaluation techniques calculate a score for a model by comparing the labels that it generates for the inputs in a test set or evaluation set with the correct labels for",
                    "word_count": 91
                },
                {
                    "title": "6.4 Decision Trees",
                    "page": "242",
                    "subsections": [],
                    "content": " In the next three sections well take a closer look at three machine learning methods that can be used to automatically build classification models decision trees naive Bayes classifiers and Maximum Entropy classifiers As weve seen its possible to treat these learning methods as black boxes simply training models and using them for prediction without understanding how they work But theres a lot to be learned from taking a closer look at how these learning methods select models based on the data in a training set An understanding of these methods can help guide our selection of appropriate features and especially our decisions about how those features should be encoded And an understanding of the generated models can allow us to extract information about which features are most informative and how those features relate to one an other A decision tree is a simple flowchart that selects labels for input values This flowchart consists of decision nodes which check feature values and leaf nodes which assign labels To choose the label for an input value we begin at the flowcharts initial decision node known as its root node This node contains a condition that checks one of the input values features and selects a branch based on that features value Following the branch that describes our input value we arrive at a new decision node with a new condition on the input values features We continue following the branch selected by each nodes condition until we arrive at a leaf node which provides a label for the input value Figure 64 shows an example decision tree model for the name gender task Once we have a decision tree it is straightforward to use it to assign labels to new input values Whats less straightforward is how we can build a decision tree that models a given training set But before we look at the learning algorithm for building decision trees well consider a simpler task picking the best decision stump for a corpus A Figure 64 Decision Tree model for the name gender task Note that tree diagrams are conventionally drawn upside down with the root at the top and the leaves at the bottom decision stump is a decision tree with a single node that decides how to classify inputs based on a single feature It contains one leaf for each possible feature value specifying the class label that should be assigned to inputs whose features have that value In order to build a decision stump we must first decide which feature should be used The simplest method is to just build a decision stump for each possible feature and see which one achieves the highest accuracy on the training data although there are other alternatives that we will discuss later Once weve picked a feature we can build the decision stump by assigning a label to each leaf based on the most frequent label for the selected examples in the training set ie the examples where the selected feature has that value Given the algorithm for choosing decision stumps the algorithm for growing larger decision trees is straightforward We begin by selecting the overall best decision stump for the classification task We then check the accuracy of each of the leaves on the training set Leaves that do not achieve sufficient accuracy are then replaced by new decision stumps trained on the subset of the training corpus that is selected by the path to the leaf For example we could grow the decision tree in Figure 64 by replacing the leftmost leaf with a new decision stump trained on the subset of the training set names that do not start with a k or end with a vowel or an l Entropy and Information Gain As was mentioned before there are several methods for identifying the most informa tive feature for a decision stump One popular alternative called information gain measures how much more organized the input values become when we divide them up using a given feature To measure how disorganized the original set of input values are we calculate entropy of their labels which will be high if the input values have highly varied labels and low if many input values all have the same label In particular entropy is defined as the sum of the probability of each label times the log probability of that same label 1 H  l  labelsPl  log2Pl For example Figure 65 shows how the entropy of labels in the name gender prediction task depends on the ratio of male to female names Note that if most input values have the same label eg if Pmale is near 0 or near 1 then entropy is low In particular labels that have low frequency do not contribute much to the entropy since Pl is small and labels with high frequency also do not contribute much to the entropy since log2Pl is small On the other hand if the input values have a wide variety of labels then there are many labels with a medium frequency where neither Pl nor log2Pl is small so the entropy is high Example 68 demonstrates how to calculate the entropy of a list of labels",
                    "word_count": 868
                },
                {
                    "title": "6.5 Naive Bayes Classifiers",
                    "page": "244",
                    "subsections": [],
                    "content": " In naive Bayes classifiers every feature gets a say in determining which label should be assigned to a given input value To choose a label for an input value the naive Bayes",
                    "word_count": 32
                },
                {
                    "title": "6.6 Maximum Entropy Classifiers",
                    "page": "250",
                    "subsections": [],
                    "content": " The Maximum Entropy classifier uses a model that is very similar to the model em ployed by the naive Bayes classifier But rather than using probabilities to set the models parameters it uses search techniques to find a set of parameters that will max imize the performance of the classifier In particular it looks for the set of parameters that maximizes the total likelihood of the training corpus which is defined as 10 Pfeatures  x  corpus Plabelxfeaturesx Where Plabelfeatures the probability that an input whose features are features will have class label label is defined as 11 Plabelfeatures  Plabel featureslabel Plabel features Because of the potentially complex interactions between the effects of related features there is no way to directly calculate the model parameters that maximize the likelihood of the training set Therefore Maximum Entropy classifiers choose the model param eters using iterative optimization techniques which initialize the models parameters to random values and then repeatedly refine those parameters to bring them closer to the optimal solution These iterative optimization techniques guarantee that each re finement of the parameters will bring them closer to the optimal values but do not necessarily provide a means of determining when those optimal values have been reached Because the parameters for Maximum Entropy classifiers are selected using iterative optimization techniques they can take a long time to learn This is especially true when the size of the training set the number of features and the number of labels are all large Some iterative optimization techniques are much faster than others When training Maximum Entropy models avoid the use of Generalized Iterative Scaling GIS or Improved Iterative Scaling IIS which are both considerably slower than the Conjugate Gradient CG and the BFGS optimization methods The Maximum Entropy Model The Maximum Entropy classifier model is a generalization of the model used by the naive Bayes classifier Like the naive Bayes model the Maximum Entropy classifier calculates the likelihood of each label for a given input value by multiplying together the parameters that are applicable for the input value and label The naive Bayes clas sifier model defines a parameter for each label specifying its prior probability and a parameter for each feature label pair specifying the contribution of individual fea tures toward a labels likelihood In contrast the Maximum Entropy classifier model leaves it up to the user to decide what combinations of labels and features should receive their own parameters In par ticular it is possible to use a single parameter to associate a feature with more than one label or to associate more than one feature with a given label This will sometimes",
                    "word_count": 439
                },
                {
                    "title": "6.7 Modeling Linguistic Patterns",
                    "page": "254",
                    "subsections": [],
                    "content": " Classifiers can help us to understand the linguistic patterns that occur in natural lan guage by allowing us to create explicit models that capture those patterns Typically these models are using supervised classification techniques but it is also possible to build analytically motivated models Either way these explicit models serve two im portant purposes they help us to understand linguistic patterns and they can be used to make predictions about new language data The extent to which explicit models can give us insights into linguistic patterns depends largely on what kind of model is used Some models such as decision trees are relatively transparent and give us direct information about which factors are important in mak ing decisions and about which factors are related to one another Other models such as multilevel neural networks are much more opaque Although it can be possible to gain insight by studying them it typically takes a lot more work But all explicit models can make predictions about new unseen language data that was not included in the corpus used to build the model These predictions can be evaluated to assess the accuracy of the model Once a model is deemed sufficiently accurate it can then be used to automatically predict information about new language data These predictive models can be combined into systems that perform many useful language processing tasks such as document classification automatic translation and question answering What Do Models Tell Us Its important to understand what we can learn about language from an automatically constructed model One important consideration when dealing with models of lan guage is the distinction between descriptive models and explanatory models Descrip tive models capture patterns in the data but they dont provide any information about why the data contains those patterns For example as we saw in Table 31 the syno nyms absolutely and definitely are not interchangeable we say absolutely adore not definitely adore and definitely prefer not absolutely prefer In contrast explanatory models attempt to capture properties and relationships that cause the linguistic pat terns For example we might introduce the abstract concept of polar adjective as an adjective that has an extreme meaning and categorize some adjectives such as adore and detest as polar Our explanatory model would contain the constraint that abso lutely can combine only with polar adjectives and definitely can only combine with nonpolar adjectives In summary descriptive models provide information about cor relations in the data while explanatory models go further to postulate causal relationships Most models that are automatically constructed from a corpus are descriptive models in other words they can tell us what features are relevant to a given pattern or con struction but they cant necessarily tell us how those features and patterns relate to one another If our goal is to understand the linguistic patterns then we can use this information about which features are related as a starting point for further experiments designed to tease apart the relationships between features and patterns On the other hand if were just interested in using the model to make predictions eg as part of a language processing system then we can use the model to make predictions about new data without worrying about the details of underlying causal relationships",
                    "word_count": 543
                },
                {
                    "title": "6.8 Summary",
                    "page": "254",
                    "subsections": [],
                    "content": "  Modeling the linguistic data found in corpora can help us to understand linguistic patterns and can be used to make predictions about new language data  Supervised classifiers use labeled training corpora to build models that predict the label of an input based on specific features of that input  Supervised classifiers can perform a wide variety of NLP tasks including document classification partofspeech tagging sentence segmentation dialogue act type identification and determining entailment relations and many other tasks  When training a supervised classifier you should split your corpus into three da tasets a training set for building the classifier model a devtest set for helping select and tune the models features and a test set for evaluating the final models performance  When evaluating a supervised classifier it is important that you use fresh data that was not included in the training or devtest set Otherwise your evaluation results may be unrealistically optimistic  Decision trees are automatically constructed treestructured flowcharts that are used to assign labels to input values based on their features Although theyre easy to interpret they are not very good at handling cases where feature values interact in determining the proper label  In naive Bayes classifiers each feature independently contributes to the decision of which label should be used This allows feature values to interact but can be problematic when two or more features are highly correlated with one another  Maximum Entropy classifiers use a basic model that is similar to the model used by naive Bayes however they employ iterative optimization to find the set of fea ture weights that maximizes the probability of the training set  Most of the models that are automatically constructed from a corpus are descrip tive that is they let us know which features are relevant to a given pattern or construction but they dont give any information about causal relationships be tween those features and patterns",
                    "word_count": 314
                },
                {
                    "title": "6.9 Further Reading",
                    "page": "256",
                    "subsections": [],
                    "content": " Please consult httpwwwnltkorg for further materials on this chapter and on how to install external machine learning packages such as Weka Mallet TADM and MegaM For more examples of classification and machine learning with NLTK please see the classification HOWTOs at httpwwwnltkorghowto For a general introduction to machine learning we recommend Alpaydin 2004 For a more mathematically intense introduction to the theory of machine learning see Hastie Tibshirani  Friedman 2009 Excellent books on using machine learning techniques for NLP include Abney 2008 Daelemans  Bosch 2005 Feldman  Sanger 2007 Segaran 2007 and Weiss et al 2004 For more on smoothing tech niques for language problems see Manning  Schtze 1999 For more on sequence modeling and especially hidden Markov models see Manning  Schtze 1999 or Jurafsky  Martin 2008 Chapter 13 of Manning Raghavan  Schtze 2008 dis cusses the use of naive Bayes for classifying texts Many of the machine learning algorithms discussed in this chapter are numerically intensive and as a result they will run slowly when coded naively in Python For in formation on increasing the efficiency of numerically intensive algorithms in Python see Kiusalaas 2005 The classification techniques described in this chapter can be applied to a very wide variety of problems For example Agirre  Edmonds 2007 uses classifiers to perform wordsense disambiguation and Melamed 2001 uses classifiers to create parallel texts Recent textbooks that cover text classification include Manning Raghavan  Schtze 2008 and Croft Metzler  Strohman 2009 Much of the current research in the application of machine learning techniques to NLP problems is driven by governmentsponsored challenges where a set of research organizations are all provided with the same development corpus and asked to build a system and the resulting systems are compared based on a reserved test set Examples of these challenge competitions include CoNLL Shared Tasks the Recognizing Textual Entailment competitions the ACE competitions and the AQUAINT competitions Consult httpwwwnltkorg for a list of pointers to the web pages for these challenges",
                    "word_count": 325
                },
                {
                    "title": "6.10 Exercises",
                    "page": "256",
                    "subsections": [],
                    "content": " 1  Read up on one of the language technologies mentioned in this section such as word sense disambiguation semantic role labeling question answering machine translation or named entity recognition Find out what type and quantity of an notated data is required for developing such systems Why do you think a large amount of data is required 2  Using any of the three classifiers described in this chapter and any features you can think of build the best name gender classifier you can Begin by splitting the Names Corpus into three subsets 500 words for the test set 500 words for the devtest set and the remaining 6900 words for the training set Then starting with the example name gender classifier make incremental improvements Use the dev test set to check your progress Once you are satisfied with your classifier check its final performance on the test set How does the performance on the test set compare to the performance on the devtest set Is this what youd expect 3  The Senseval 2 Corpus contains data intended to train wordsense disambigua tion classifiers It contains data for four words hard interest line and serve Choose one of these four words and load the corresponding data",
                    "word_count": 204
                }
            ]
        },
        {
            "title": "Chapter 7: Extracting Information from Text",
            "page": "261",
            "sections": [
                {
                    "title": "7.1 Information Extraction",
                    "page": "262",
                    "subsections": [],
                    "content": " Information comes in many shapes and sizes One important form is structured data where there is a regular and predictable organization of entities and relationships For example we might be interested in the relation between companies and locations Given a particular company we would like to be able to identify the locations where it does business conversely given a location we would like to discover which com panies do business in that location If our data is in tabular form such as the example in Table 71 then answering these queries is straightforward 261 Table 71 Locations data OrgName LocationName Omnicom New York DDB Needham New York Kaplan Thaler Group New York BBDO South Atlanta GeorgiaPacific Atlanta If this location data was stored in Python as a list of tuples entity relation entity then the question Which organizations operate in Atlanta could be trans lated as follows  print org for e1 rel e2 if relIN and e2Atlanta BBDO South GeorgiaPacific Things are more tricky if we try to get similar information out of text For example consider the following snippet from nltkcorpusieer for fileid NYT199803150085 1 The fourth Wells account moving to another agency is the packaged paper products division of GeorgiaPacific Corp which arrived at Wells only last fall Like Hertz and the History Channel it is also leaving for an Omnicomowned agency the BBDO South unit of BBDO Worldwide BBDO South in Atlanta which handles corporate advertising for GeorgiaPacific will assume additional duties for brands like Angel Soft toilet tissue and Sparkle paper towels said Ken Haldin a spokesman for GeorgiaPacific in Atlanta If you read through 1 you will glean the information required to answer the example question But how do we get a machine to understand enough about 1 to return the list BBDO South GeorgiaPacific as an answer This is obviously a much harder task Unlike Table 71 1 contains no structure that links organization names with location names One approach to this problem involves building a very general representation of mean ing Chapter 10 In this chapter we take a different approach deciding in advance that we will only look for very specific kinds of information in text such as the relation between organizations and locations Rather than trying to use text like 1 to answer the question directly we first convert the unstructured data of natural language sen tences into the structured data of Table 71 Then we reap the benefits of powerful query tools such as SQL This method of getting meaning from text is called Infor mation Extraction Information Extraction has many applications including business intelligence resume harvesting media analysis sentiment detection patent search and email scanning A particularly important area of current research involves the attempt to extract structured data out of electronically available scientific literature especially in the do main of biology and medicine Information Extraction Architecture Figure 71 shows the architecture for a simple information extraction system It begins by processing a document using several of the procedures discussed in Chapters 3 and 5 first the raw text of the document is split into sentences using a sentence segmenter and each sentence is further subdivided into words using a tokenizer Next each sen tence is tagged with partofspeech tags which will prove very helpful in the next step named entity recognition In this step we search for mentions of potentially inter esting entities in each sentence Finally we use relation recognition to search for likely relations between different entities in the text Figure 71 Simple pipeline architecture for an information extraction system This system takes the raw text of a document as its input and generates a list of entity relation entity tuples as its output For example given a document that indicates that the company GeorgiaPacific is located in Atlanta it might generate the tuple ORG GeorgiaPacific in LOC Atlanta To perform the first three tasks we can define a function that simply connects together NLTKs default sentence segmenter  word tokenizer  and partofspeech tagger   def iepreprocessdocument     sentences  nltksenttokenizedocument     sentences  nltkwordtokenizesent for sent in sentences     sentences  nltkpostagsent for sent in sentences",
                    "word_count": 683
                },
                {
                    "title": "7.2 Chunking",
                    "page": "264",
                    "subsections": [],
                    "content": " The basic technique we will use for entity recognition is chunking which segments and labels multitoken sequences as illustrated in Figure 72 The smaller boxes show the wordlevel tokenization and partofspeech tagging while the large boxes show higherlevel chunking Each of these larger boxes is called a chunk Like tokenization which omits whitespace chunking usually selects a subset of the tokens Also like tokenization the pieces produced by a chunker do not overlap in the source text In this section we will explore chunking in some depth beginning with the definition and representation of chunks We will see regular expression and ngram approaches to chunking and will develop and evaluate chunkers using the CoNLL2000 Chunking Corpus We will then return in Sections 75 and 76 to the tasks of named entity rec ognition and relation extraction Noun Phrase Chunking We will begin by considering the task of noun phrase chunking or NPchunking where we search for chunks corresponding to individual noun phrases For example here is some Wall Street Journal text with NPchunks marked using brackets Figure 72 Segmentation and labeling at both the Token and Chunk levels 2  TheDT marketNN  forIN  systemmanagementNN softwareNN  for IN  DigitalNNP   sPOS hardwareNN  isVBZ fragmentedJJ enoughRB thatIN  aDT giantNN  suchJJ asIN  ComputerNNP AssociatesNNPS  shouldMD doVB wellRB thereRB  As we can see NPchunks are often smaller pieces than complete noun phrases For example the market for systemmanagement software for Digitals hardware is a single noun phrase containing two nested noun phrases but it is captured in NPchunks by the simpler chunk the market One of the motivations for this difference is that NP chunks are defined so as not to contain other NPchunks Consequently any preposi tional phrases or subordinate clauses that modify a nominal will not be included in the corresponding NPchunk since they almost certainly contain further noun phrases One of the most useful sources of information for NPchunking is partofspeech tags This is one of the motivations for performing partofspeech tagging in our information extraction system We demonstrate this approach using an example sentence that has been partofspeech tagged in Example 71 In order to create an NPchunker we will first define a chunk grammar consisting of rules that indicate how sentences should be chunked In this case we will define a simple grammar with a single regular expression rule  This rule says that an NP chunk should be formed whenever the chunker finds an optional determiner DT followed by any number of adjectives JJ and then a noun NN Using this grammar we create a chunk parser  and test it on our example sentence  The result is a tree which we can either print  or display graphically  Example 71 Example of a simple regular expressionbased NP chunker  sentence  the DT little JJ yellow JJ  dog NN barked VBD at IN  the DT cat NN  grammar  NP DTJJNN  cp  nltkRegexpParsergrammar  result  cpparsesentence  print result S NP theDT littleJJ yellowJJ dogNN barkedVBD atIN NP theDT catNN  resultdraw",
                    "word_count": 494
                },
                {
                    "title": "7.3 Developing and Evaluating Chunkers",
                    "page": "270",
                    "subsections": [],
                    "content": " Now you have a taste of what chunking does but we havent explained how to evaluate chunkers As usual this requires a suitably annotated corpus We begin by looking at the mechanics of converting IOB format into an NLTK tree then at how this is done on a larger scale using a chunked corpus We will see how to score the accuracy of a chunker relative to a corpus then look at some more datadriven ways to search for NP chunks Our focus throughout will be on expanding the coverage of a chunker Reading IOB Format and the CoNLL2000 Chunking Corpus Using the corpora module we can load Wall Street Journal text that has been tagged then chunked using the IOB notation The chunk categories provided in this corpus are NP VP and PP As we have seen each sentence is represented using multiple lines as shown here he PRP BNP accepted VBD BVP the DT BNP position NN INP  A conversion function chunkconllstr2tree builds a tree representation from one of these multiline strings Moreover it permits us to choose any subset of the three chunk types to use here just for NP chunks  text    he PRP BNP  accepted VBD BVP  the DT BNP  position NN INP  of IN BPP  vice NN BNP  chairman NN INP  of IN BPP  Carlyle NNP BNP  Group NNP INP    O  a DT BNP  merchant NN INP  banking NN INP  concern NN INP    O    nltkchunkconllstr2treetext chunktypesNPdraw We can use the NLTK corpus module to access a larger amount of chunked text The CoNLL2000 Chunking Corpus contains 270k words of Wall Street Journal text divi ded into train and test portions annotated with partofspeech tags and chunk tags in the IOB format We can access the data using nltkcorpusconll2000 Here is an example that reads the 100th sentence of the train portion of the corpus  from nltkcorpus import conll2000  print conll2000chunkedsentstraintxt99 S PP OverIN NP aDT cupNN PP ofIN NP coffeeNN  NP MrNNP StoneNNP VP toldVBD NP hisPRP storyNN  As you can see the CoNLL2000 Chunking Corpus contains three chunk types NP chunks which we have already seen VP chunks such as has already delivered and PP",
                    "word_count": 361
                },
                {
                    "title": "7.4 Recursion in Linguistic Structure",
                    "page": "276",
                    "subsections": [],
                    "content": " Building Nested Structure with Cascaded Chunkers So far our chunk structures have been relatively flat Trees consist of tagged tokens optionally grouped under a chunk node such as NP However it is possible to build chunk structures of arbitrary depth simply by creating a multistage chunk grammar",
                    "word_count": 47
                },
                {
                    "title": "7.5 Named Entity Recognition",
                    "page": "280",
                    "subsections": [],
                    "content": " At the start of this chapter we briefly introduced named entities NEs Named entities are definite noun phrases that refer to specific types of individuals such as organiza tions persons dates and so on Table 73 lists some of the more commonly used types of NEs These should be selfexplanatory except for FACILITY humanmade arti facts in the domains of architecture and civil engineering and GPE geopolitical entities such as city stateprovince and country Table 73 Commonly used types of named entity NE type Examples ORGANIZATION GeorgiaPacific Corp WHO PERSON Eddy Bonte President Obama LOCATION Murray River Mount Everest DATE June 20080629 TIME two fifty a m 130 pm MONEY 175 million Canadian Dollars GBP 1040 PERCENT twenty pct 1875  FACILITY Washington Monument Stonehenge GPE South East Asia Midlothian The goal of a named entity recognition NER system is to identify all textual men tions of the named entities This can be broken down into two subtasks identifying the boundaries of the NE and identifying its type While named entity recognition is frequently a prelude to identifying relations in Information Extraction it can also con tribute to other tasks For example in Question Answering QA we try to improve the precision of Information Retrieval by recovering not whole pages but just those parts which contain an answer to the users question Most QA systems take the",
                    "word_count": 226
                },
                {
                    "title": "7.6 Relation Extraction",
                    "page": "284",
                    "subsections": [],
                    "content": " Once named entities have been identified in a text we then want to extract the relations that exist between them As indicated earlier we will typically be looking for relations between specified types of named entity One way of approaching this task is to initially look for all triples of the form X  Y where X and Y are named entities of the required types and  is the string of words that intervenes between X and Y We can then use regular expressions to pull out just those instances of  that express the relation that we are looking for The following example searches for strings that contain the word in The special regular expression bingb is a negative lookahead assertion that allows us to disregard strings such as success in supervising the transition of where in is followed by a gerund  IN  recompilerbinbbing  for doc in nltkcorpusieerparseddocsNYT19980315      for rel in nltksemextractrelsORG LOC doc                                       corpusieer pattern  IN          print nltksemshowrawrtuplerel ORG WHYY in LOC Philadelphia ORG McGlashan AMP Sarrail firm in LOC San Mateo ORG Freedom Forum in LOC Arlington ORG Brookings Institution  the research group in LOC Washington ORG Idealab  a selfdescribed business incubator based in LOC Los Angeles ORG Open Text  based in LOC Waterloo ORG WGBH in LOC Boston ORG Bastille Opera in LOC Paris ORG Omnicom in LOC New York ORG DDB Needham in LOC New York ORG Kaplan Thaler Group in LOC New York ORG BBDO South in LOC Atlanta ORG GeorgiaPacific in LOC Atlanta Searching for the keyword in works reasonably well though it will also retrieve false positives such as ORG House Transportation Committee  secured the most money in the LOC New York there is unlikely to be a simple stringbased method of ex cluding filler strings such as this As shown earlier the Dutch section of the CoNLL 2002 Named Entity Corpus contains not just named entity annotation but also partofspeech tags This allows us to devise patterns that are sensitive to these tags as shown in the next example The method showclause prints out the relations in a clausal form where the binary relation sym bol is specified as the value of parameter relsym   from nltkcorpus import conll2002  vnv      isV     3rd sing present and  wasV    past forms of the verb zijn be  werdV   and also present  wordtV   past of worden become            followed by anything  vanPrep  followed by van of    VAN  recompilevnv reVERBOSE  for doc in conll2002chunkedsentsnedtrain      for r in nltksemextractrelsPER ORG doc                                     corpusconll2002 patternVAN          print  nltksemshowclauser relsymVAN VANcornetdelzius buitenlandsehandel VANjohanrottiers kardinaalvanroeyinstituut VANannielennox eurythmics Your Turn Replace the last line with print showrawrtuplerel lconTrue rconTrue This will show you the actual words that inter vene between the two NEs and also their left and right context within a default 10word window With the help of a Dutch dictionary you might be able to figure out why the result VANannielennox euryth mics is a false hit",
                    "word_count": 481
                },
                {
                    "title": "7.7 Summary",
                    "page": "284",
                    "subsections": [],
                    "content": "  Information extraction systems search large bodies of unrestricted text for specific types of entities and relations and use them to populate wellorganized databases These databases can then be used to find answers for specific questions  The typical architecture for an information extraction system begins by segment ing tokenizing and partofspeech tagging the text The resulting data is then searched for specific types of entity Finally the information extraction system looks at entities that are mentioned near one another in the text and tries to de termine whether specific relationships hold between those entities  Entity recognition is often performed using chunkers which segment multitoken sequences and label them with the appropriate entity type Common entity types include ORGANIZATION PERSON LOCATION DATE TIME MONEY and GPE geopolitical entity",
                    "word_count": 126
                },
                {
                    "title": "7.8 Further Reading",
                    "page": "284",
                    "subsections": [],
                    "content": " Extra materials for this chapter are posted at httpwwwnltkorg including links to freely available resources on the Web For more examples of chunking with NLTK please see the Chunking HOWTO at httpwwwnltkorghowto The popularity of chunking is due in great part to pioneering work by Abney eg Abney 1996a Abneys Cass chunker is described in httpwwwvinartusnetspa97a pdf The word chink initially meant a sequence of stopwords according to a 1975 paper by Ross and Tukey Abney 1996a The IOB format or sometimes BIO Format was developed for NP chunking by Ram shaw  Marcus 1995 and was used for the shared NP bracketing task run by the Conference on Natural Language Learning CoNLL in 1999 The same format was adopted by CoNLL 2000 for annotating a section of Wall Street Journal text as part of a shared task on NP chunking Section 135 of Jurafsky  Martin 2008 contains a discussion of chunking Chapter 22 covers information extraction including named entity recognition For information about text mining in biology and medicine see Ananiadou  McNaught 2006 For more information on the Getty and Alexandria gazetteers see httpenwikipedia orgwikiGettyThesaurusofGeographicNames and httpwwwalexandriaucsb edugazetteer",
                    "word_count": 188
                },
                {
                    "title": "7.9 Exercises",
                    "page": "286",
                    "subsections": [],
                    "content": " 1  The IOB format categorizes tagged tokens as I O and B Why are three tags necessary What problem would be caused if we used I and O tags exclusively 2  Write a tag pattern to match noun phrases containing plural head nouns eg manyJJ researchersNNS twoCD weeksNNS bothDT newJJ positionsNNS Try to do this by generalizing the tag pattern that handled singular noun phrases 3  Pick one of the three chunk types in the CoNLL2000 Chunking Corpus Inspect the data and try to observe any patterns in the POS tag sequences that make up this kind of chunk Develop a simple chunker using the regular expression chunker nltkRegexpParser Discuss any tag sequences that are difficult to chunk reliably 4  An early definition of chunk was the material that occurs between chinks De velop a chunker that starts by putting the whole sentence in a single chunk and then does the rest of its work solely by chinking Determine which tags or tag sequences are most likely to make up chinks with the help of your own utility program Compare the performance and simplicity of this approach relative to a chunker based entirely on chunk rules 5  Write a tag pattern to cover noun phrases that contain gerunds eg theDT receivingVBG endNN assistantNN managingVBG editorNN Add these patterns to the grammar one per line Test your work using some tagged sentences of your own devising 6  Write one or more tag patterns to handle coordinated noun phrases eg July NNP andCC AugustNNP allDT yourPRP managersNNS andCC supervisorsNNS companyNN courtsNNS andCC adjudicatorsNNS 7  Carry out the following evaluation tasks for any of the chunkers you have de veloped earlier Note that most chunking corpora contain some internal incon sistencies such that any reasonable rulebased approach will produce errors a Evaluate your chunker on 100 sentences from a chunked corpus and report the precision recall and Fmeasure b Use the chunkscoremissed and chunkscoreincorrect methods to identify the errors made by your chunker Discuss c Compare the performance of your chunker to the baseline chunker discussed in the evaluation section of this chapter 8  Develop a chunker for one of the chunk types in the CoNLL Chunking Corpus using a regular expressionbased chunk grammar RegexpChunk Use any combina tion of rules for chunking chinking merging or splitting 9  Sometimes a word is incorrectly tagged eg the head noun in 12CD orCC so RB casesVBZ Instead of requiring manual correction of tagger output good chunkers are able to work with the erroneous output of taggers Look for other examples of correctly chunked noun phrases with incorrect tags 10  The bigram chunker scores about 90 accuracy Study its errors and try to work out why it doesnt get 100 accuracy Experiment with trigram chunking Are you able to improve the performance any more",
                    "word_count": 468
                }
            ]
        },
        {
            "title": "Chapter 8: Analyzing Sentence Structure",
            "page": "291",
            "sections": [
                {
                    "title": "8.1 Some Grammatical Dilemmas",
                    "page": "292",
                    "subsections": [],
                    "content": " Linguistic Data and Unlimited Possibilities Previous chapters have shown you how to process and analyze text corpora and we have stressed the challenges for NLP in dealing with the vast amount of electronic language data that is growing daily Lets consider this data more closely and make the thought experiment that we have a gigantic corpus consisting of everything that has been either uttered or written in English over say the last 50 years Would we be justified in calling this corpus the language of modern English There are a number of reasons why we might answer no Recall that in Chapter 3 we asked you to search the Web for instances of the pattern the of Although it is easy to find examples on the Web containing this word sequence such as New man at the of IMG  see httpwww telegraphcouksport2387900NewmanattheofIMGhtml speakers of English will say that most such examples are errors and therefore not part of English after all Accordingly we can argue that modern English is not equivalent to the very big set of word sequences in our imaginary corpus Speakers of English can make judgments about these sequences and will reject some of them as being ungrammatical Equally it is easy to compose a new sentence and have speakers agree that it is perfectly good English For example sentences have an interesting property that they can be embedded inside larger sentences Consider the following sentences 1 a Usain Bolt broke the 100m record b The Jamaica Observer reported that Usain Bolt broke the 100m record c Andre said The Jamaica Observer reported that Usain Bolt broke the 100m record d I think Andre said the Jamaica Observer reported that Usain Bolt broke the 100m record If we replaced whole sentences with the symbol S we would see patterns like Andre said S and I think S These are templates for taking a sentence and constructing a bigger sentence There are other templates we can use such as S but S and S when S With a bit of ingenuity we can construct some really long sentences using these templates Heres an impressive example from a Winnie the Pooh story by AA Milne In Which Piglet Is Entirely Surrounded by Water You can imagine Piglets joy when at last the ship came in sight of him In afteryears he liked to think that he had been in Very Great Danger during the Terrible Flood but the only danger he had really been in was the last halfhour of his imprisonment when Owl who had just flown up sat on a branch of his tree to comfort him and told him a very long story about an aunt who had once laid a seagulls egg by mistake and the story went on and on rather like this sentence until Piglet who was listening out of his window without much hope went to sleep quietly and naturally slipping slowly out of the win dow towards the water until he was only hanging on by his toes at which moment luckily a sudden loud squawk from Owl which was really part of the story being what his aunt said woke the Piglet up and just gave him time to jerk himself back into safety and say How interesting and did she whenwell you can imagine his joy when at last he saw the good ship Brain of Pooh Captain C Robin 1st Mate P Bear coming over the sea to rescue him This long sentence actually has a simple structure that begins S but S when S We can see from this example that language provides us with constructions which seem to allow us to extend sentences indefinitely It is also striking that we can understand sentences of arbitrary length that weve never heard before its not hard to concoct an entirely novel sentence one that has probably never been used before in the history of the language yet all speakers of the language will understand it The purpose of a grammar is to give an explicit description of a language But the way in which we think of a grammar is closely intertwined with what we consider to be a language Is it a large but finite set of observed utterances and written texts Is it some thing more abstract like the implicit knowledge that competent speakers have about grammatical sentences Or is it some combination of the two We wont take a stand on this issue but instead will introduce the main approaches In this chapter we will adopt the formal framework of generative grammar in which a language is considered to be nothing more than an enormous collection of all grammatical sentences and a grammar is a formal notation that can be used for gen erating the members of this set Grammars use recursive productions of the form S  S and S as we will explore in Section 83 In Chapter 10 we will extend this to automatically build up the meaning of a sentence out of the meanings of its parts Ubiquitous Ambiguity A wellknown example of ambiguity is shown in 2 from the Groucho Marx movie Animal Crackers 1930 2 While hunting in Africa I shot an elephant in my pajamas How an elephant got into my pajamas Ill never know Lets take a closer look at the ambiguity in the phrase I shot an elephant in my paja mas First we need to define a simple grammar  grouchogrammar  nltkparsecfg  S  NP VP  PP  P NP  NP  Det N  Det N PP  I  VP  V NP  VP PP  Det  an  my  N  elephant  pajamas  V  shot  P  in  ",
                    "word_count": 943
                },
                {
                    "title": "8.2 What\u2019s the Use of Syntax?",
                    "page": "296",
                    "subsections": [],
                    "content": " Beyond ngrams We gave an example in Chapter 2 of how to use the frequency information in bigrams to generate text that seems perfectly acceptable for small sequences of words but rapidly degenerates into nonsense Heres another pair of examples that we created by com puting the bigrams over the text of a childrens story The Adventures of Buster Brown included in the Project Gutenberg Selection Corpus 4 a He roared with me the pail slip down his back b The worst part and clumsy looking for whoever heard light You intuitively know that these sequences are wordsalad but you probably find it hard to pin down whats wrong with them One benefit of studying grammar is that it provides a conceptual framework and vocabulary for spelling out these intuitions Lets take a closer look at the sequence the worst part and clumsy looking This looks like a coordinate structure where two phrases are joined by a coordinating conjunction such as and but or or Heres an informal and simplified statement of how coordi nation works syntactically Coordinate Structure if v1 and v2 are both phrases of grammatical category X then v1 and v2 is also a phrase of category X Here are a couple of examples In the first two NPs noun phrases have been conjoined to make an NP while in the second two APs adjective phrases have been conjoined to make an AP 5 a The books ending was NP the worst part and the best part for me b On land they are AP slow and clumsy looking What we cant do is conjoin an NP and an AP which is why the worst part and clumsy looking is ungrammatical Before we can formalize these ideas we need to understand the concept of constituent structure Constituent structure is based on the observation that words combine with other words to form units The evidence that a sequence of words forms such a unit is given by substitutabilitythat is a sequence of words in a wellformed sentence can be replaced by a shorter sequence without rendering the sentence illformed To clarify this idea consider the following sentence 6 The little bear saw the fine fat trout in the brook The fact that we can substitute He for The little bear indicates that the latter sequence is a unit By contrast we cannot replace little bear saw in the same way We use an asterisk at the start of a sentence to indicate that it is ungrammatical 7 a He saw the fine fat trout in the brook b The he the fine fat trout in the brook In Figure 81 we systematically substitute longer sequences by shorter ones in a way which preserves grammaticality Each sequence that forms a unit can in fact be replaced by a single word and we end up with just two elements Figure 81 Substitution of word sequences Working from the top row we can replace particular sequences of words eg the brook with individual words eg it repeating this process we arrive at a grammatical twoword sentence In Figure 82 we have added grammatical category labels to the words we saw in the earlier figure The labels NP VP and PP stand for noun phrase verb phrase and prepositional phrase respectively If we now strip out the words apart from the topmost row add an S node and flip the figure over we end up with a standard phrase structure tree shown in 8 Each node in this tree including the words is called a constituent The immediate constitu ents of S are NP and VP 8 As we saw in Section 81 sentences can have arbitrary length Conse quently phrase structure trees can have arbitrary depth The cascaded chunk parsers we saw in Section 74 can only produce structures of bounded depth so chunking methods arent applicable here Figure 82 Substitution of word sequences plus grammatical categories This diagram reproduces Figure 81 along with grammatical categories corresponding to noun phrases NP verb phrases VP prepositional phrases PP and nominals Nom As we will see in the next section a grammar specifies how the sentence can be subdi vided into its immediate constituents and how these can be further subdivided until we reach the level of individual words",
                    "word_count": 716
                },
                {
                    "title": "8.3 Context-Free Grammar",
                    "page": "300",
                    "subsections": [],
                    "content": " A Simple Grammar Lets start off by looking at a simple contextfree grammar CFG By convention the lefthand side of the first production is the startsymbol of the grammar typically S and all wellformed trees must have this symbol as their root label In NLTK context free grammars are defined in the nltkgrammar module In Example 81 we define a grammar and show how to parse a simple sentence admitted by the grammar Example 81 A simple contextfree grammar grammar1  nltkparsecfg S  NP VP VP  V NP  V NP PP PP  P NP V  saw  ate  walked NP  John  Mary  Bob  Det N  Det N PP Det  a  an  the  my N  man  dog  cat  telescope  park P  in  on  by  with   sent  Mary saw Bobsplit  rdparser  nltkRecursiveDescentParsergrammar1  for tree in rdparsernbestparsesent       print tree S NP Mary VP V saw NP Bob The grammar in Example 81 contains productions involving various syntactic cate gories as laid out in Table 81 The recursive descent parser used here can also be inspected via a graphical interface as illustrated in Figure 83 we discuss this parser in more detail in Section 84 Table 81 Syntactic categories Symbol Meaning Example S sentence the man walked NP noun phrase a dog VP verb phrase saw a park PP prepositional phrase with a telescope Det determiner the N noun dog Symbol Meaning Example V verb walked P preposition in A production like VP  V NP  V NP PP has a disjunction on the righthand side shown by the  and is an abbreviation for the two productions VP  V NP and VP  V NP PP If we parse the sentence The dog saw a man in the park using the grammar shown in Example 81 we end up with two trees similar to those we saw for 3 9 a b Since our grammar licenses two trees for this sentence the sentence is said to be struc turally ambiguous The ambiguity in question is called a prepositional phrase at tachment ambiguity as we saw earlier in this chapter As you may recall it is an ambiguity about attachment since the PP in the park needs to be attached to one of two places in the tree either as a child of VP or else as a child of NP When the PP is attached to VP the intended interpretation is that the seeing event happened in the park However if the PP is attached to NP then it was the man who was in the park and the agent of the seeing the dog might have been sitting on the balcony of an apartment overlooking the park Writing Your Own Grammars If you are interested in experimenting with writing CFGs you will find it helpful to create and edit your grammar in a text file say mygrammarcfg You can then load it into NLTK and parse with it as follows  grammar1  nltkdataloadfilemygrammarcfg  sent  Mary saw Bobsplit  rdparser  nltkRecursiveDescentParsergrammar1  for tree in rdparsernbestparsesent       print tree Make sure that you put a cfg suffix on the filename and that there are no spaces in the string filemygrammarcfg If the command print tree produces no output this is probably because your sentence sent is not admitted by your grammar In this case call the parser with tracing set to be on rdparser  nltkRecursiveDescent Figure 83 Recursive descent parser demo This tool allows you to watch the operation of a recursive descent parser as it grows the parse tree and matches it against the input words Parsergrammar1 trace2 You can also check what productions are currently in the grammar with the command for p in grammar1productions print p When you write CFGs for parsing in NLTK you cannot combine grammatical cate gories with lexical items on the righthand side of the same production Thus a pro duction such as PP  of NP is disallowed In addition you are not permitted to place multiword lexical items on the righthand side of a production So rather than writing NP  New York you have to resort to something like NP  NewYork instead Recursion in Syntactic Structure A grammar is said to be recursive if a category occurring on the lefthand side of a production also appears on the righthand side of a production as illustrated in Exam ple 82 The production Nom  Adj Nom where Nom is the category of nominals involves direct recursion on the category Nom whereas indirect recursion on S arises from the combination of two productions namely S  NP VP and VP  V S Example 82 A recursive contextfree grammar grammar2  nltkparsecfg S   NP VP NP  Det Nom  PropN Nom  Adj Nom  N VP  V Adj  V NP  V S  V NP PP PP  P NP PropN  Buster  Chatterer  Joe Det  the  a N  bear  squirrel  tree  fish  log Adj   angry  frightened   little  tall V   chased   saw  said  thought  was  put P  on  To see how recursion arises from this grammar consider the following trees 10a involves nested nominal phrases while 10b contains nested sentences 10 a b Weve only illustrated two levels of recursion here but theres no upper limit on the depth You can experiment with parsing sentences that involve more deeply nested structures Beware that the RecursiveDescentParser is unable to handle left recursive productions of the form X  X Y we will return to this in Section 84",
                    "word_count": 891
                },
                {
                    "title": "8.4 Parsing with Context-Free Grammar",
                    "page": "308",
                    "subsections": [],
                    "content": " A parser processes input sentences according to the productions of a grammar and builds one or more constituent structures that conform to the grammar A grammar is a declarative specification of wellformednessit is actually just a string not a pro gram A parser is a procedural interpretation of the grammar It searches through the space of trees licensed by a grammar to find one that has the required sentence along its fringe A parser permits a grammar to be evaluated against a collection of test sentences help ing linguists to discover mistakes in their grammatical analysis A parser can serve as a model of psycholinguistic processing helping to explain the difficulties that humans have with processing certain syntactic constructions Many natural language applica tions involve parsing at some point for example we would expect the natural language questions submitted to a questionanswering system to undergo parsing as an initial step In this section we see two simple parsing algorithms a topdown method called re cursive descent parsing and a bottomup method called shiftreduce parsing We also see some more sophisticated algorithms a topdown method with bottomup filtering called leftcorner parsing and a dynamic programming technique called chart parsing Recursive Descent Parsing The simplest kind of parser interprets a grammar as a specification of how to break a highlevel goal into several lowerlevel subgoals The toplevel goal is to find an S The S  NP VP production permits the parser to replace this goal with two subgoals find an NP then find a VP Each of these subgoals can be replaced in turn by subsubgoals using productions that have NP and VP on their lefthand side Eventually this expansion process leads to subgoals such as find the word telescope Such subgoals can be directly compared against the input sequence and succeed if the next word is matched If there is no match the parser must back up and try a different alternative The recursive descent parser builds a parse tree during this process With the initial goal find an S the S root node is created As the process recursively expands its goals using the productions of the grammar the parse tree is extended downwards hence the name recursive descent We can see this in action using the graphical demonstration nltkapprdparser Six stages of the execution of this parser are shown in Figure 84 During this process the parser is often forced to choose between several possible pro ductions For example in going from step 3 to step 4 it tries to find productions with N on the lefthand side The first of these is N  man When this does not work it backtracks and tries other N productions in order until it gets to N  dog which matches the next word in the input sentence Much later as shown in step 5 it finds a complete parse This is a tree that covers the entire sentence without any dangling edges Once a parse has been found we can get the parser to look for additional parses Again it will backtrack and explore other choices of production in case any of them result in a parse NLTK provides a recursive descent parser  rdparser  nltkRecursiveDescentParsergrammar1  sent  Mary saw a dogsplit  for t in rdparsernbestparsesent      print t S NP Mary VP V saw NP Det a N dog RecursiveDescentParser takes an optional parameter trace If trace is greater than zero then the parser will report the steps that it takes as it parses a text Recursive descent parsing has three key shortcomings First leftrecursive productions like NP  NP PP send it into an infinite loop Second the parser wastes a lot of time considering words and structures that do not correspond to the input sentence Third the backtracking process may discard parsed constituents that will need to be rebuilt again later For example backtracking over VP  V NP will discard the subtree created for the NP If the parser then proceeds with VP  V NP PP then the NP subtree must be created all over again Recursive descent parsing is a kind of topdown parsing Topdown parsers use a grammar to predict what the input will be before inspecting the input However since the input is available to the parser all along it would be more sensible to consider the input sentence from the very beginning This approach is called bottomup parsing and we will see an example in the next section ShiftReduce Parsing A simple kind of bottomup parser is the shiftreduce parser In common with all bottomup parsers a shiftreduce parser tries to find sequences of words and phrases that correspond to the righthand side of a grammar production and replace them with the lefthand side until the whole sentence is reduced to an S Figure 84 Six stages of a recursive descent parser The parser begins with a tree consisting of the node S at each stage it consults the grammar to find a production that can be used to enlarge the tree when a lexical production is encountered its word is compared against the input after a complete parse has been found the parser backtracks to look for more parses The shiftreduce parser repeatedly pushes the next input word onto a stack Sec tion 41 this is the shift operation If the top n items on the stack match the n items on the righthand side of some production then they are all popped off the stack and the item on the lefthand side of the production is pushed onto the stack This replace ment of the top n items with a single item is the reduce operation The operation may be applied only to the top of the stack reducing items lower in the stack must be done before later items are pushed onto the stack The parser finishes when all the input is consumed and there is only one item remaining on the stack a parse tree with an S node as its root The shiftreduce parser builds a parse tree during the above process Each time it pops n items off the stack it combines them into a partial parse tree and pushes this back onto the stack We can see the shiftreduce parsing algorithm in action using the graphical demonstration nltkappsrparser Six stages of the execution of this parser are shown in Figure 85 Figure 85 Six stages of a shiftreduce parser The parser begins by shifting the first input word onto its stack once the top items on the stack match the righthand side of a grammar production they can be replaced with the lefthand side of that production the parser succeeds once all input is consumed and one S item remains on the stack NLTK provides ShiftReduceParser a simple implementation of a shiftreduce parser This parser does not implement any backtracking so it is not guaranteed to find a parse for a text even if one exists Furthermore it will only find at most one parse even if more parses exist We can provide an optional trace parameter that controls how ver bosely the parser reports the steps that it takes as it parses a text  srparse  nltkShiftReduceParsergrammar1  sent  Mary saw a dogsplit  print srparseparsesent S NP Mary VP V saw NP Det a N dog Your Turn Run this parser in tracing mode to see the sequence of shift and reduce operations using srparse  nltkShiftReduceParsergram mar1 trace2 A shiftreduce parser can reach a dead end and fail to find any parse even if the input sentence is wellformed according to the grammar When this happens no input re mains and the stack contains items that cannot be reduced to an S The problem arises because there are choices made earlier that cannot be undone by the parser although users of the graphical demonstration can undo their choices There are two kinds of choices to be made by the parser a which reduction to do when more than one is possible and b whether to shift or reduce when either action is possible A shiftreduce parser may be extended to implement policies for resolving such con flicts For example it may address shiftreduce conflicts by shifting only when no re ductions are possible and it may address reducereduce conflicts by favoring the re duction operation that removes the most items from the stack A generalization of the shiftreduce parser a lookahead LR parser is commonly used in programming lan guage compilers The advantages of shiftreduce parsers over recursive descent parsers is that they only build structure that corresponds to the words in the input Furthermore they only build each substructure once eg NPDetthe Nman is only built and pushed onto the stack a single time regardless of whether it will later be used by the VP  V NP PP reduction or the NP  NP PP reduction The LeftCorner Parser One of the problems with the recursive descent parser is that it goes into an infinite loop when it encounters a leftrecursive production This is because it applies the grammar productions blindly without considering the actual input sentence A left corner parser is a hybrid between the bottomup and topdown approaches we have seen A leftcorner parser is a topdown parser with bottomup filtering Unlike an ordinary recursive descent parser it does not get trapped in leftrecursive productions Before starting its work a leftcorner parser preprocesses the contextfree grammar to build a table where each row contains two cells the first holding a nonterminal and the sec ond holding the collection of possible left corners of that nonterminal Table 82 il lustrates this for the grammar from grammar2 Table 82 Left corners in grammar2 Category Left corners preterminals S NP NP Det PropN VP V PP P Each time a production is considered by the parser it checks that the next input word is compatible with at least one of the preterminal categories in the leftcorner table WellFormed Substring Tables The simple parsers discussed in the previous sections suffer from limitations in both completeness and efficiency In order to remedy these we will apply the algorithm design technique of dynamic programming to the parsing problem As we saw in Section 47 dynamic programming stores intermediate results and reuses them when appropriate achieving significant efficiency gains This technique can be applied to syntactic parsing allowing us to store partial solutions to the parsing task and then look them up as necessary in order to efficiently arrive at a complete solution This approach to parsing is known as chart parsing We introduce the main idea in this section see the online materials available for this chapter for more implementation details Dynamic programming allows us to build the PP in my pajamas just once The first time we build it we save it in a table then we look it up when we need to use it as a sub constituent of either the object NP or the higher VP This table is known as a wellformed substring table or WFST for short The term substring refers to a contiguous se quence of words within a sentence We will show how to construct the WFST bottom up so as to systematically record what syntactic constituents have been found Lets set our input to be the sentence in 2 The numerically specified spans of the WFST are reminiscent of Pythons slice notation Section 32 Another way to think about the data structure is shown in Figure 86 a data structure known as a chart Figure 86 The chart data structure Words are the edge labels of a linear graph structure In a WFST we record the position of the words by filling in cells in a triangular matrix the vertical axis will denote the start position of a substring while the horizontal axis will denote the end position thus shot will appear in the cell with coordinates 1 2 To simplify this presentation we will assume each word has a unique lexical category and we will store this not the word in the matrix So cell 1 2 will contain the entry V More generally if our input string is a1a2  an and our grammar contains a pro duction of the form A  ai then we add A to the cell i1 i So for every word in text we can look up in our grammar what category it belongs to  text  I shot an elephant in my pajamas V  shot For our WFST we create an n1  n1 matrix as a list of lists in Python and initialize it with the lexical categories of each token in the initwfst function in Exam ple 83 We also define a utility function display to prettyprint the WFST for us As expected there is a V in cell 1 2 Example 83 Acceptor using wellformed substring table def initwfsttokens grammar numtokens  lentokens wfst  None for i in rangenumtokens1 for j in rangenumtokens1 for i in rangenumtokens productions  grammarproductionsrhstokensi wfstii1  productions0lhs return wfst def completewfstwfst tokens grammar traceFalse index  dictprhs plhs for p in grammarproductions numtokens  lentokens for span in range2 numtokens1 for start in rangenumtokens1span end  start  span for mid in rangestart1 end nt1 nt2  wfststartmid wfstmidend if nt1 and nt2 and nt1nt2 in index wfststartend  indexnt1nt2 if trace print s 3s s 3s s  s 3s s   start nt1 mid nt2 end start indexnt1nt2 end return wfst def displaywfst tokens print nWFST    join4d  i for i in range1 lenwfst for i in rangelenwfst1 print d     i for j in range1 lenwfst print 4s  wfstij or  print  tokens  I shot an elephant in my pajamassplit  wfst0  initwfsttokens grouchogrammar  displaywfst0 tokens WFST 1    2    3    4    5    6    7 0    NP                        1        V                     2            Det               3                N             4                    P         5                        Det   6                            N  wfst1  completewfstwfst0 tokens grouchogrammar  displaywfst1 tokens WFST 1    2    3    4    5    6    7 0    NP           S            S 1        V        VP           VP 2            Det  NP            3                N             4                    P        PP 5                        Det  NP 6                            N Returning to our tabular representation given that we have Det in cell 2 3 for the word an and N in cell 3 4 for the word elephant what should we put into cell 2 4 for an elephant We need to find a production of the form A  Det N Consulting the grammar we know that we can enter NP in cell 0 2 More generally we can enter A in i j if there is a production A  B C and we find nonterminal B in i k and C in k j The program in Example 83 uses this rule to complete the WFST By setting trace to True when calling the function completewfst we see tracing output that shows the WFST being constructed  wfst1  completewfstwfst0 tokens grouchogrammar traceTrue 2 Det 3   N 4  2  NP 4 5 Det 6   N 7  5  NP 7 1   V 2  NP 4  1  VP 4 4   P 5  NP 7  4  PP 7 0  NP 1  VP 4  0   S 4 1  VP 4  PP 7  1  VP 7 0  NP 1  VP 7  0   S 7 For example this says that since we found Det at wfst01 and N at wfst12 we can add NP to wfst02 To help us easily retrieve productions by their righthand sides we create an index for the grammar This is an example of a spacetime tradeoff we do a reverse lookup on the grammar instead of having to check through entire list of productions each time we want to look up via the righthand side We conclude that there is a parse for the whole input string once we have constructed an S node in cell 0 7 showing that we have found a sentence that covers the whole input The final state of the WFST is depicted in Figure 87 Notice that we have not used any builtin parsing functions here Weve implemented a complete primitive chart parser from the ground up WFSTs have several shortcomings First as you can see the WFST is not itself a parse tree so the technique is strictly speaking recognizing that a sentence is admitted by a grammar rather than parsing it Second it requires every nonlexical grammar pro duction to be binary Although it is possible to convert an arbitrary CFG into this form we would prefer to use an approach without such a requirement Third as a bottom up approach it is potentially wasteful being able to propose constituents in locations that would not be licensed by the grammar Finally the WFST did not represent the structural ambiguity in the sentence ie the two verb phrase readings The VP in cell 28 was actually entered twice once for a V NP reading and once for a VP PP reading These are different hypotheses and the second overwrote the first as it happens this didnt matter since the lefthand side was the same Chart parsers use a slightly richer data structure and some interesting algorithms to solve these problems see Section 88 Your Turn Try out the interactive chart parser application nltkappchartparser",
                    "word_count": 2845
                },
                {
                    "title": "8.5 Dependencies and Dependency Grammar",
                    "page": "310",
                    "subsections": [],
                    "content": " Phrase structure grammar is concerned with how words and sequences of words com bine to form constituents A distinct and complementary approach dependency grammar focuses instead on how words relate to other words Dependency is a binary asymmetric relation that holds between a head and its dependents The head of a sentence is usually taken to be the tensed verb and every other word is either dependent on the sentence head or connects to it through a path of dependencies A dependency representation is a labeled directed graph where the nodes are the lexical items and the labeled arcs represent dependency relations from heads to dependents Figure 88 illustrates a dependency graph where arrows point from heads to their dependents Figure 87 The chart data structure Nonterminals are represented as extra edges in the chart The arcs in Figure 88 are labeled with the grammatical function that holds between a dependent and its head For example I is the SBJ subject of shot which is the head of the whole sentence and in is an NMOD noun modifier of elephant In contrast to phrase structure grammar therefore dependency grammars can be used to directly express grammatical functions as a type of dependency Heres one way of encoding a dependency grammar in NLTKnote that it only cap tures bare dependency information without specifying the type of dependency  grouchodepgrammar  nltkparsedependencygrammar  shot  I  elephant  in  elephant  an  in  in  pajamas  pajamas  my    print grouchodepgrammar Dependency grammar with 7 productions shot  I shot  elephant shot  in elephant  an elephant  in in  pajamas pajamas  my A dependency graph is projective if when all the words are written in linear order the edges can be drawn above the words without crossing This is equivalent to saying that a word and all its descendants dependents and dependents of its dependents etc form a contiguous sequence of words within the sentence Figure 88 is projective and we can parse many sentences in English using a projective dependency parser The next example shows how grouchodepgrammar provides an alternative approach to captur ing the attachment ambiguity that we examined earlier with phrase structure grammar  pdp  nltkProjectiveDependencyParsergrouchodepgrammar  sent  I shot an elephant in my pajamassplit  trees  pdpparsesent  for tree in trees      print tree shot I elephant an in pajamas my shot I elephant an in pajamas my Figure 88 Dependency structure Arrows point from heads to their dependents labels indicate the grammatical function of the dependent as subject object or modifier",
                    "word_count": 410
                },
                {
                    "title": "8.6 Grammar Development",
                    "page": "314",
                    "subsections": [],
                    "content": " Parsing builds trees over sentences according to a phrase structure grammar Now all the examples we gave earlier only involved toy grammars containing a handful of pro ductions What happens if we try to scale up this approach to deal with realistic corpora of language In this section we will see how to access treebanks and look at the chal lenge of developing broadcoverage grammars Treebanks and Grammars The corpus module defines the treebank corpus reader which contains a 10 sample of the Penn Treebank Corpus  from nltkcorpus import treebank  t  treebankparsedsentswsj0001mrg0  print t S NPSBJ NP NNP Pierre NNP Vinken   ADJP NP CD 61 NNS years JJ old   VP MD will VP VB join NP DT the NN board PPCLR IN as NP DT a JJ nonexecutive NN director NPTMP NNP Nov CD 29   We can use this data to help develop a grammar For example the program in Exam ple 84 uses a simple filter to find verbs that take sentential complements Assuming we already have a production of the form VP  SV S this information enables us to identify particular verbs that would be included in the expansion of SV",
                    "word_count": 193
                },
                {
                    "title": "8.7 Summary",
                    "page": "320",
                    "subsections": [],
                    "content": "  Sentences have internal organization that can be represented using a tree Notable features of constituent structure are recursion heads complements and modifiers  A grammar is a compact characterization of a potentially infinite set of sentences we say that a tree is wellformed according to a grammar or that a grammar licenses a tree  A grammar is a formal model for describing whether a given phrase can be assigned a particular constituent or dependency structure  Given a set of syntactic categories a contextfree grammar uses a set of productions to say how a phrase of some category A can be analyzed into a sequence of smaller parts 1  n  A dependency grammar uses productions to specify what the dependents are of a given lexical head  Syntactic ambiguity arises when one sentence has more than one syntactic analysis eg prepositional phrase attachment ambiguity  A parser is a procedure for finding one or more trees corresponding to a grammat ically wellformed sentence  A simple topdown parser is the recursive descent parser which recursively ex pands the start symbol usually S with the help of the grammar productions and tries to match the input sentence This parser cannot handle leftrecursive pro ductions eg productions such as NP  NP PP It is inefficient in the way it blindly expands categories without checking whether they are compatible with the input string and in repeatedly expanding the same nonterminals and discarding the results  A simple bottomup parser is the shiftreduce parser which shifts input onto a stack and tries to match the items at the top of the stack with the righthand side of grammar productions This parser is not guaranteed to find a valid parse for the input even if one exists and builds substructures without checking whether it is globally consistent with the grammar",
                    "word_count": 299
                },
                {
                    "title": "8.8 Further Reading",
                    "page": "320",
                    "subsections": [],
                    "content": " Extra materials for this chapter are posted at httpwwwnltkorg including links to freely available resources on the Web For more examples of parsing with NLTK please see the Parsing HOWTO at httpwwwnltkorghowto There are many introductory books on syntax OGrady et al 2004 is a general in troduction to linguistics while Radford 1988 provides a gentle introduction to trans formational grammar and can be recommended for its coverage of transformational approaches to unbounded dependency constructions The most widely used term in linguistics for formal grammar is generative grammar though it has nothing to do with generation Chomsky 1965 BurtonRoberts 1997 is a practically oriented textbook on how to analyze constitu ency in English with extensive exemplification and exercises Huddleston  Pullum 2002 provides an uptodate and comprehensive analysis of syntactic phenomena in English Chapter 12 of Jurafsky  Martin 2008 covers formal grammars of English Sections 1313 cover simple parsing algorithms and techniques for dealing with ambiguity Chapter 14 covers statistical parsing and Chapter 16 covers the Chomsky hierarchy and the formal complexity of natural language Levin 1993 has categorized English verbs into finegrained classes according to their syntactic properties There are several ongoing efforts to build largescale rulebased grammars eg the LFG Pargram project httpwww2parccomistlgroupsnlttpargram the HPSG Lin GO Matrix framework httpwwwdelphinnetmatrix and the XTAG Project http wwwcisupenneduxtag",
                    "word_count": 217
                },
                {
                    "title": "8.9 Exercises",
                    "page": "322",
                    "subsections": [],
                    "content": " 1  Can you come up with grammatical sentences that probably have never been uttered before Take turns with a partner What does this tell you about human language 2  Recall Strunk and Whites prohibition against using a sentenceinitial however to mean although Do a web search for however used at the start of the sentence How widely used is this construction 3  Consider the sentence Kim arrived or Dana left and everyone cheered Write down the parenthesized forms to show the relative scope of and and or Generate tree structures corresponding to both of these interpretations 4  The Tree class implements a variety of other useful methods See the Tree help documentation for more details ie import the Tree class and then type helpTree 5  In this exercise you will manually construct some parse trees a Write code to produce two trees one for each reading of the phrase old men and women b Encode any of the trees presented in this chapter as a labeled bracketing and use nltkTree to check that it is wellformed Now use draw to display the tree c As in a draw a tree for The woman saw a man last Thursday 6  Write a recursive function to traverse a tree and return the depth of the tree such that a tree with a single node would have depth zero Hint the depth of a subtree is the maximum depth of its children plus one 7  Analyze the AA Milne sentence about Piglet by underlining all of the sentences it contains then replacing these with S eg the first sentence becomes S when S Draw a tree structure for this compressed sentence What are the main syntactic constructions used for building such a long sentence 8  In the recursive descent parser demo experiment with changing the sentence to be parsed by selecting Edit Text in the Edit menu 9  Can the grammar in grammar1 Example 81 be used to describe sentences that are more than 20 words in length 10  Use the graphical chartparser interface to experiment with different rule invo cation strategies Come up with your own strategy that you can execute manually using the graphical interface Describe the steps and report any efficiency im provements it has eg in terms of the size of the resulting chart Do these im provements depend on the structure of the grammar What do you think of the prospects for significant performance boosts from cleverer rule invocation strategies 11  With pen and paper manually trace the execution of a recursive descent parser and a shiftreduce parser for a CFG you have already seen or one of your own devising 12  We have seen that a chart parser adds but never removes edges from a chart Why 13  Consider the sequence of words Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo This is a grammatically correct sentence as explained at httpen wikipediaorgwikiBuffalobuffaloBuffalobuffalobuffalobuffaloBuffalobuf falo Consider the tree diagram presented on this Wikipedia page and write down a suitable grammar Normalize case to lowercase to simulate the problem that a listener has when hearing this sentence Can you find other parses for this sentence How does the number of parse trees grow as the sentence gets longer More ex amples of these sentences can be found at httpenwikipediaorgwikiListofho mophonousphrases 14  You can modify the grammar in the recursive descent parser demo by selecting Edit Grammar in the Edit menu Change the first expansion production namely",
                    "word_count": 576
                }
            ]
        },
        {
            "title": "Chapter 9: Building Feature-Based Grammars",
            "page": "327",
            "sections": [
                {
                    "title": "9.1 Grammatical Features",
                    "page": "328",
                    "subsections": [],
                    "content": " In Chapter 6 we described how to build classifiers that rely on detecting features of text Such features may be quite simple such as extracting the last letter of a word or more complex such as a partofspeech tag that has itself been predicted by the clas sifier In this chapter we will investigate the role of features in building rulebased grammars In contrast to feature extractors which record features that have been au tomatically detected we are now going to declare the features of words and phrases We start off with a very simple example using dictionaries to store features and their values  kim  CAT NP ORTH Kim REF k  chase  CAT V ORTH chased REL chase 327 The objects kim and chase both have a couple of shared features CAT grammatical category and ORTH orthography ie spelling In addition each has a more semanti cally oriented feature kimREF is intended to give the referent of kim while chaseREL gives the relation expressed by chase In the context of rulebased gram mars such pairings of features and values are known as feature structures and we will shortly see alternative notations for them Feature structures contain various kinds of information about grammatical entities The information need not be exhaustive and we might want to add further properties For example in the case of a verb it is often useful to know what semantic role is played by the arguments of the verb In the case of chase the subject plays the role of agent whereas the object has the role of patient Lets add this information using sbj subject and obj object as placeholders which will get filled once the verb combines with its grammatical arguments  chaseAGT  sbj  chasePAT  obj If we now process a sentence Kim chased Lee we want to bind the verbs agent role to the subject and the patient role to the object We do this by linking to the REF feature of the relevant NP In the following example we make the simpleminded assumption that the NPs immediately to the left and right of the verb are the subject and object respectively We also add a feature structure for Lee to complete the example  sent  Kim chased Lee  tokens  sentsplit  lee  CAT NP ORTH Lee REF l  def lex2fsword      for fs in kim lee chase          if fsORTH  word              return fs  subj verb obj  lex2fstokens0 lex2fstokens1 lex2fstokens2  verbAGT  subjREF  agent of chase is Kim  verbPAT  objREF   patient of chase is Lee  for k in ORTH REL AGT PAT  check featstruct of chase      print 5s  s  k verbk ORTH   chased REL    chase AGT    k PAT    l The same approach could be adopted for a different verbsay surprisethough in this case the subject would play the role of source SRC and the object plays the role of experiencer EXP  surprise  CAT V ORTH surprised REL surprise              SRC sbj EXP obj Feature structures are pretty powerful but the way in which we have manipulated them is extremely ad hoc Our next task in this chapter is to show how the framework of contextfree grammar and parsing can be expanded to accommodate feature structures so that we can build analyses like this in a more generic and principled way We will start off by looking at the phenomenon of syntactic agreement we will show how agreement constraints can be expressed elegantly using features and illustrate their use in a simple grammar Since feature structures are a general data structure for representing information of any kind we will briefly look at them from a more formal point of view and illustrate the support for feature structures offered by NLTK In the final part of the chapter we demonstrate that the additional expressiveness of features opens up a wide spectrum of possibilities for describing sophisticated aspects of linguistic structure Syntactic Agreement The following examples show pairs of word sequences the first of which is grammatical and the second not We use an asterisk at the start of a word sequence to signal that it is ungrammatical 1 a this dog b these dog 2 a these dogs b this dogs In English nouns are usually marked as being singular or plural The form of the de monstrative also varies this singular and these plural Examples 1 and 2 show that there are constraints on the use of demonstratives and nouns within a noun phrase either both are singular or both are plural A similar constraint holds between subjects and predicates 3 a the dog runs b the dog run 4 a the dogs run b the dogs runs Here we can see that morphological properties of the verb covary with syntactic prop erties of the subject noun phrase This covariance is called agreement If we look further at verb agreement in English we will see that present tense verbs typically have two inflected forms one for third person singular and another for every other combi nation of person and number as shown in Table 91",
                    "word_count": 833
                },
                {
                    "title": "9.2 Processing Feature Structures",
                    "page": "336",
                    "subsections": [],
                    "content": " In this section we will show how feature structures can be constructed and manipulated in NLTK We will also discuss the fundamental operation of unification which allows us to combine the information contained in two different feature structures Feature structures in NLTK are declared with the FeatStruct constructor Atomic feature values can be strings or integers  fs1  nltkFeatStructTENSEpast NUMsg  print fs1  NUM    sg     TENSE  past  A feature structure is actually just a kind of dictionary and so we access its values by indexing in the usual way We can use our familiar syntax to assign values to features  fs1  nltkFeatStructPER3 NUMpl GNDfem  print fs1GND fem  fs1CASE  acc We can also define feature structures that have complex values as discussed earlier  fs2  nltkFeatStructPOSN AGRfs1  print fs2         CASE  acc    AGR   GND   fem           NUM   pl            PER   3                                 POS  N              ",
                    "word_count": 136
                },
                {
                    "title": "9.3 Extending a Feature-Based Grammar",
                    "page": "354",
                    "subsections": [],
                    "content": " In this section we return to featurebased grammar and explore a variety of linguistic issues and demonstrate the benefits of incorporating features into the grammar Subcategorization In Chapter 8 we augmented our category labels to represent different kinds of verbs and used the labels IV and TV for intransitive and transitive verbs respectively This allowed us to write productions like the following 29 VP  IV VP  TV NP Although we know that IV and TV are two kinds of V they are just atomic nonterminal symbols in a CFG and are as distinct from each other as any other pair of symbols This notation doesnt let us say anything about verbs in general eg we cannot say All lexical items of category V can be marked for tense since walk say is an item of category IV not V So can we replace category labels such as TV and IV by V along with a feature that tells us whether the verb combines with a following NP object or whether it can occur without any complement A simple approach originally developed for a grammar framework called Generalized Phrase Structure Grammar GPSG tries to solve this problem by allowing lexical cat egories to bear a SUBCAT feature which tells us what subcategorization class the item belongs to In contrast to the integer values for SUBCAT used by GPSG the example here adopts more mnemonic values namely intrans trans and clause 30 VPTENSEt NUMn  VSUBCATintrans TENSEt NUMn VPTENSEt NUMn  VSUBCATtrans TENSEt NUMn NP VPTENSEt NUMn  VSUBCATclause TENSEt NUMn SBar VSUBCATintrans TENSEpres NUMsg  disappears  walks VSUBCATtrans TENSEpres NUMsg  sees  likes VSUBCATclause TENSEpres NUMsg  says  claims VSUBCATintrans TENSEpres NUMpl  disappear  walk VSUBCATtrans TENSEpres NUMpl  see  like VSUBCATclause TENSEpres NUMpl  say  claim VSUBCATintrans TENSEpast  disappeared  walked VSUBCATtrans TENSEpast  saw  liked VSUBCATclause TENSEpast  said  claimed When we see a lexical category like VSUBCATtrans we can interpret the SUBCAT spec ification as a pointer to a production in which VSUBCATtrans is introduced as the head child in a VP production By convention there is a correspondence between the values of SUBCAT and the productions that introduce lexical heads On this approach SUBCAT can appear only on lexical categories it makes no sense for example to specify a SUBCAT value on VP As required walk and like both belong to the category V Never theless walk will occur only in VPs expanded by a production with the feature SUBCATintrans on the righthand side as opposed to like which requires a SUBCATtrans In our third class of verbs in 30 we have specified a category SBar This is a label for subordinate clauses such as the complement of claim in the example You claim that you like children We require two further productions to analyze such sentences 31 SBar  Comp S Comp  that The resulting structure is the following 32 An alternative treatment of subcategorization due originally to a framework known as categorial grammar is represented in featurebased frameworks such as PATR and Headdriven Phrase Structure Grammar Rather than using SUBCAT values as a way of indexing productions the SUBCAT value directly encodes the valency of a head the list of arguments that it can combine with For example a verb like put that takes NP and PP complements put the book on the table might be represented as 33 33 VSUBCATNP NP PP This says that the verb can combine with three arguments The leftmost element in the list is the subject NP while everything elsean NP followed by a PP in this casecom prises the subcategorizedfor complements When a verb like put is combined with appropriate complements the requirements which are specified in the SUBCAT are dis charged and only a subject NP is needed This category which corresponds to what is traditionally thought of as VP might be represented as follows 34 VSUBCATNP Finally a sentence is a kind of verbal category that has no requirements for further arguments and hence has a SUBCAT whose value is the empty list The tree 35 shows how these category assignments combine in a parse of Kim put the book on the table 35 Heads Revisited We noted in the previous section that by factoring subcategorization information out of the main category label we could express more generalizations about properties of verbs Another property of this kind is the following expressions of category V are heads of phrases of category VP Similarly Ns are heads of NPs As ie adjectives are heads of APs and Ps ie prepositions are heads of PPs Not all phrases have headsfor exam ple it is standard to say that coordinate phrases eg the book and the bell lack heads Nevertheless we would like our grammar formalism to express the parentheadchild relation where it holds At present V and VP are just atomic symbols and we need to find a way to relate them using features as we did earlier to relate IV and TV Xbar syntax addresses this issue by abstracting out the notion of phrasal level It is usual to recognize three such levels If N represents the lexical level then N represents the next level up corresponding to the more traditional category Nom and N represents the phrasal level corresponding to the category NP 36a illustrates a representative structure while 36b is the more conventional counterpart 36 a b The head of the structure 36a is N and N and N are called phrasal projections of N N is the maximal projection and N is sometimes called the zero projection One of the central claims of Xbar syntax is that all constituents share a structural similarity Using X as a variable over N V A and P we say that directly subcategorized comple ments of a lexical head X are always placed as siblings of the head whereas adjuncts are placed as siblings of the intermediate category X Thus the configuration of the two P adjuncts in 37 contrasts with that of the complement P in 36a 37 The productions in 38 illustrate how bar levels can be encoded using feature struc tures The nested structure in 37 is achieved by two applications of the recursive rule expanding NBAR1 38 S  NBAR2 VBAR2 NBAR2  Det NBAR1 NBAR1  NBAR1 PBAR2 NBAR1  NBAR0 PBAR2 Auxiliary Verbs and Inversion Inverted clauseswhere the order of subject and verb is switchedoccur in English interrogatives and also after negative adverbs 39 a Do you like children b Can Jody walk 40 a Rarely do you see Kim b Never have I seen this dog However we cannot place just any verb in presubject position 41 a Like you children b Walks Jody 42 a Rarely see you Kim b Never saw I this dog Verbs that can be positioned initially in inverted clauses belong to the class known as auxiliaries and as well as do can and have include be will and shall One way of capturing such structures is with the following production 43 SINV  VAUX NP VP That is a clause marked as inv consists of an auxiliary verb followed by a VP In a more detailed grammar we would need to place some constraints on the form of the VP depending on the choice of auxiliary 44 illustrates the structure of an inverted clause 44 Unbounded Dependency Constructions Consider the following contrasts 45 a You like Jody b You like 46 a You put the card into the slot b You put into the slot c You put the card d You put The verb like requires an NP complement while put requires both a following NP and PP 45 and 46 show that these complements are obligatory omitting them leads to ungrammaticality Yet there are contexts in which obligatory complements can be omitted as 47 and 48 illustrate 47 a Kim knows who you like b This music you really like 48 a Which card do you put into the slot b Which slot do you put the card into That is an obligatory complement can be omitted if there is an appropriate filler in the sentence such as the question word who in 47a the preposed topic this music in 47b or the wh phrases which cardslot in 48 It is common to say that sentences like those in 47 and 48 contain gaps where the obligatory complements have been omitted and these gaps are sometimes made explicit using an underscore 49 a Which card do you put  into the slot b Which slot do you put the card into  So a gap can occur if it is licensed by a filler Conversely fillers can occur only if there is an appropriate gap elsewhere in the sentence as shown by the following examples 50 a Kim knows who you like Jody b This music you really like hiphop 51 a Which card do you put this into the slot b Which slot do you put the card into this one The mutual cooccurrence between filler and gap is sometimes termed a dependency One issue of considerable importance in theoretical linguistics has been the nature of the material that can intervene between a filler and the gap that it licenses in particular can we simply list a finite set of sequences that separate the two The answer is no there is no upper bound on the distance between filler and gap This fact can be easily illustrated with constructions involving sentential complements as shown in 52 52 a Who do you like  b Who do you claim that you like  c Who do you claim that Jody says that you like  Since we can have indefinitely deep recursion of sentential complements the gap can be embedded indefinitely far inside the whole sentence This constellation of properties leads to the notion of an unbounded dependency construction that is a fillergap dependency where there is no upper bound on the distance between filler and gap A variety of mechanisms have been suggested for handling unbounded dependencies in formal grammars here we illustrate the approach due to Generalized Phrase Struc ture Grammar that involves slash categories A slash category has the form YXP we interpret this as a phrase of category Y that is missing a subconstituent of category XP For example SNP is an S that is missing an NP The use of slash categories is illustrated in 53 53 The top part of the tree introduces the filler who treated as an expression of category NPwh together with a corresponding gapcontaining constituent SNP The gap information is then percolated down the tree via the VPNP category until it reaches the category NPNP At this point the dependency is discharged by realizing the gap information as the empty string immediately dominated by NPNP Do we need to think of slash categories as a completely new kind of object Fortunately we can accommodate them within our existing featurebased framework by treating slash as a feature and the category to its right as a value that is SNP is reducible to SSLASHNP In practice this is also how the parser interprets slash categories The grammar shown in Example 93 illustrates the main principles of slash categories and also includes productions for inverted clauses To simplify presentation we have omitted any specification of tense on the verbs Example 93 Grammar with productions for inverted clauses and longdistance dependencies making use of slash categories  nltkdatashowcfggrammarsbookgrammarsfeat1fcfg  start S    Grammar Productions   SINV  NP VP SINVx  NP VPx SINV  NP SNP SINV  AdvNEG SINV SINV  VAUX NP VP SINVx  VAUX NP VPx SBar  Comp SINV SBarx  Comp SINVx VP  VSUBCATintrans AUX VP  VSUBCATtrans AUX NP VPx  VSUBCATtrans AUX NPx VP  VSUBCATclause AUX SBar VPx  VSUBCATclause AUX SBarx VP  VAUX VP VPx  VAUX VPx    Lexical Productions   VSUBCATintrans AUX  walk  sing VSUBCATtrans AUX  see  like VSUBCATclause AUX  say  claim VAUX  do  can NPWH  you  cats NPWH  who AdvNEG  rarely  never NPNP  Comp  that The grammar in Example 93 contains one gapintroduction production namely S INV  NP SNP In order to percolate the slash feature correctly we need to add slashes with variable values to both sides of the arrow in productions that expand S VP and NP For example VPx  V SBarx is the slashed version of VP  V SBar and says that a slash value can be specified on the VP parent of a constituent if the same value is also specified on the SBar child Finally NPNP  allows the slash information on NP to be discharged as the empty string Using the grammar in Example 93 we can parse the sequence who do you claim that you like  tokens  who do you claim that you likesplit  from nltk import loadparser  cp  loadparsergrammarsbookgrammarsfeat1fcfg  for tree in cpnbestparsetokens      print tree SINV NPWH who SINVNP VAUX do NPWH you VPNP VAUX SUBCATclause claim SBarNP Comp that SINVNP NPWH you VPNP VAUX SUBCATtrans like NPNP  A more readable version of this tree is shown in 54 54 The grammar in Example 93 will also allow us to parse sentences without gaps  tokens  you claim that you like catssplit  for tree in cpnbestparsetokens      print tree SINV NPWH you VP VAUX SUBCATclause claim SBar Comp that SINV NPWH you VP VAUX SUBCATtrans like NPWH cats In addition it admits inverted sentences that do not involve wh constructions  tokens  rarely do you singsplit  for tree in cpnbestparsetokens      print tree SINV AdvNEG rarely SINV VAUX do NPWH you VP VAUX SUBCATintrans sing Case and Gender in German Compared with English German has a relatively rich morphology for agreement For example the definite article in German varies with case gender and number as shown in Table 92 Table 92 Morphological paradigm for the German definite article Case Masculine Feminine Neutral Plural Nominative der die das die Genitive des der des der Dative dem der dem den Accusative den die das die Subjects in German take the nominative case and most verbs govern their objects in the accusative case However there are exceptions such as helfen that govern the dative case 55 a Die Katze sieht den Hund theNOMFEMSG cat3FEMSG see3SG theACCMASCSG dog3MASCSG the cat sees the dog b Die Katze sieht dem Hund theNOMFEMSG cat3FEMSG see3SG theDATMASCSG dog3MASCSG c Die Katze hilft dem Hund theNOMFEMSG cat3FEMSG help3SG theDATMASCSG dog3MASCSG the cat helps the dog d Die Katze hilft den Hund theNOMFEMSG cat3FEMSG help3SG theACCMASCSG dog3MASCSG The grammar in Example 94 illustrates the interaction of agreement comprising per son number and gender with case Example 94 Example featurebased grammar  nltkdatashowcfggrammarsbookgrammarsgermanfcfg  start S  Grammar Productions S  NPCASEnom AGRa VPAGRa NPCASEc AGRa  PROCASEc AGRa NPCASEc AGRa  DetCASEc AGRa NCASEc AGRa VPAGRa  IVAGRa VPAGRa  TVOBJCASEc AGRa NPCASEc  Lexical Productions  Singular determiners  masc DetCASEnom AGRGNDmascPER3NUMsg  der DetCASEdat AGRGNDmascPER3NUMsg  dem DetCASEacc AGRGNDmascPER3NUMsg  den  fem DetCASEnom AGRGNDfemPER3NUMsg  die DetCASEdat AGRGNDfemPER3NUMsg  der DetCASEacc AGRGNDfemPER3NUMsg  die  Plural determiners DetCASEnom AGRPER3NUMpl  die DetCASEdat AGRPER3NUMpl  den DetCASEacc AGRPER3NUMpl  die  Nouns NAGRGNDmascPER3NUMsg  Hund NCASEnom AGRGNDmascPER3NUMpl  Hunde NCASEdat AGRGNDmascPER3NUMpl  Hunden NCASEacc AGRGNDmascPER3NUMpl  Hunde NAGRGNDfemPER3NUMsg  Katze NAGRGNDfemPER3NUMpl  Katzen  Pronouns PROCASEnom AGRPER1NUMsg  ich PROCASEacc AGRPER1NUMsg  mich PROCASEdat AGRPER1NUMsg  mir PROCASEnom AGRPER2NUMsg  du PROCASEnom AGRPER3NUMsg  er  sie  es PROCASEnom AGRPER1NUMpl  wir PROCASEacc AGRPER1NUMpl  uns PROCASEdat AGRPER1NUMpl  uns PROCASEnom AGRPER2NUMpl  ihr PROCASEnom AGRPER3NUMpl  sie  Verbs IVAGRNUMsgPER1  komme IVAGRNUMsgPER2  kommst IVAGRNUMsgPER3  kommt IVAGRNUMpl PER1  kommen IVAGRNUMpl PER2  kommt IVAGRNUMpl PER3  kommen TVOBJCASEacc AGRNUMsgPER1  sehe  mag TVOBJCASEacc AGRNUMsgPER2  siehst  magst TVOBJCASEacc AGRNUMsgPER3  sieht  mag TVOBJCASEdat AGRNUMsgPER1  folge  helfe TVOBJCASEdat AGRNUMsgPER2  folgst  hilfst TVOBJCASEdat AGRNUMsgPER3  folgt  hilft TVOBJCASEacc AGRNUMplPER1  sehen  moegen TVOBJCASEacc AGRNUMplPER2  sieht  moegt TVOBJCASEacc AGRNUMplPER3  sehen  moegen TVOBJCASEdat AGRNUMplPER1  folgen  helfen TVOBJCASEdat AGRNUMplPER2  folgt  helft TVOBJCASEdat AGRNUMplPER3  folgen  helfen As you can see the feature objcase is used to specify the case that a verb governs on its object The next example illustrates the parse tree for a sentence containing a verb that governs the dative case  tokens  ich folge den Katzensplit  cp  loadparsergrammarsbookgrammarsgermanfcfg  for tree in cpnbestparsetokens      print tree S NPAGRNUMsg PER1 CASEnom PROAGRNUMsg PER1 CASEnom ich VPAGRNUMsg PER1 TVAGRNUMsg PER1 OBJCASEdat folge NPAGRGNDfem NUMpl PER3 CASEdat DetAGRNUMpl PER3 CASEdat den NAGRGNDfem NUMpl PER3 Katzen In developing grammars excluding ungrammatical word sequences is often as chal lenging as parsing grammatical ones In order to get an idea where and why a sequence fails to parse setting the trace parameter of the loadparser method can be crucial Consider the following parse failure  tokens  ich folge den Katzesplit  cp  loadparsergrammarsbookgrammarsgermanfcfg trace2  for tree in cpnbestparsetokens      print tree ichfoldenKat           PROAGRNUMsg PER1 CASEnom  ich            NPAGRNUMsg PER1 CASEnom  PROAGRNUMsg PER1 CASEnom            S  NPAGRa CASEnom  VPAGRa a NUMsg PER1           TVAGRNUMsg PER1 OBJCASEdat  folge            VPAGRa  TVAGRa OBJCASEc  NPCASEc a NUMsg PER1 c dat           DetAGRGNDmasc NUMsg PER3 CASEacc  den            DetAGRNUMpl PER3 CASEdat  den            NPAGRa CASEc  DetAGRa CASEc  NAGRa CASEc a NUMpl PER3 c dat           NPAGRa CASEc  DetAGRa CASEc  NAGRa CASEc a GNDmasc NUMsg PER3 c acc           NAGRGNDfem NUMsg PER3  Katze  The last two Scanner lines in the trace show that den is recognized as admitting two possible categories DetAGRGNDmasc NUMsg PER3 CASEacc and DetAGRNUMpl PER3 CASEdat We know from the grammar in Exam ple 94 that Katze has category NAGRGNDfem NUMsg PER3 Thus there is no binding for the variable a in production NPCASEc AGRa  DetCASEc AGR a NCASEc AGRa that will satisfy these constraints since the AGR value of Katze will not unify with either of the AGR values of den that is with either GNDmasc NUMsg PER3 or NUMpl PER3",
                    "word_count": 2876
                },
                {
                    "title": "9.4 Summary",
                    "page": "356",
                    "subsections": [],
                    "content": "  The traditional categories of contextfree grammar are atomic symbols An impor tant motivation for feature structures is to capture finegrained distinctions that would otherwise require a massive multiplication of atomic categories  By using variables over feature values we can express constraints in grammar pro ductions that allow the realization of different feature specifications to be inter dependent  Typically we specify fixed values of features at the lexical level and constrain the values of features in phrases to unify with the corresponding values in their children  Feature values are either atomic or complex A particular subcase of atomic value is the Boolean value represented by convention as  feat  Two features can share a value either atomic or complex Structures with shared values are said to be reentrant Shared values are represented by numerical indexes or tags in AVMs  A path in a feature structure is a tuple of features corresponding to the labels on a sequence of arcs from the root of the graph representation  Two paths are equivalent if they share a value  Feature structures are partially ordered by subsumption FS0 subsumes FS1 when FS0 is more general less informative than FS1  The unification of two structures FS0 and FS1 if successful is the feature structure FS2 that contains the combined information of both FS0 and FS1  If unification specializes a path  in FS then it also specializes every path  equiv alent to   We can use feature structures to build succinct analyses of a wide variety of lin guistic phenomena including verb subcategorization inversion constructions unbounded dependency constructions and case government",
                    "word_count": 261
                },
                {
                    "title": "9.5 Further Reading",
                    "page": "356",
                    "subsections": [],
                    "content": " Please consult httpwwwnltkorg for further materials on this chapter including HOWTOs feature structures feature grammars Earley parsing and grammar test suites For an excellent introduction to the phenomenon of agreement see Corbett 2006 The earliest use of features in theoretical linguistics was designed to capture phono logical properties of phonemes For example a sound like b might be decomposed into the structure labial voice An important motivation was to capture gener alizations across classes of segments for example that n gets realized as m preceding any labial consonant Within Chomskyan grammar it was standard to use atomic features for phenomena such as agreement and also to capture generalizations across syntactic categories by analogy with phonology A radical expansion of the use of features in theoretical syntax was advocated by Generalized Phrase Structure Grammar GPSG Gazdar et al 1985 particularly in the use of features with complex values Coming more from the perspective of computational linguistics Kay 1985 proposed that functional aspects of language could be captured by unification of attributevalue structures and a similar approach was elaborated by Grosz  Stickel 1983 within the PATRII formalism Early work in LexicalFunctional grammar LFG Kaplan  Bresnan 1982 introduced the notion of an fstructure that was primarily intended to represent the grammatical relations and predicateargument structure associated with a constituent structure parse Shieber 1986 provides an excellent introduction to this phase of research into featurebased grammars One conceptual difficulty with algebraic approaches to feature structures arose when researchers attempted to model negation An alternative perspective pioneered by Kasper  Rounds 1986 and Johnson 1988 argues that grammars involve descrip tions of feature structures rather than the structures themselves These descriptions are combined using logical operations such as conjunction and negation is just the usual logical operation over feature descriptions This descriptionoriented perspective was integral to LFG from the outset Kaplan 1989 and was also adopted by later versions of HeadDriven Phrase Structure Grammar HPSG Sag  Wasow 1999 A com prehensive bibliography of HPSG literature can be found at httpwwwclunibremen deHPSGBib Feature structures as presented in this chapter are unable to capture important con straints on linguistic information For example there is no way of saying that the only permissible values for NUM are sg and pl while a specification such as NUMmasc is anomalous Similarly we cannot say that the complex value of AGR must contain spec ifications for the features PER NUM and GND but cannot contain a specification such as SUBCATtrans Typed feature structures were developed to remedy this deficiency A good early review of work on typed feature structures is Emele  Zajac 1990 A more comprehensive examination of the formal foundations can be found in",
                    "word_count": 443
                },
                {
                    "title": "9.6 Exercises",
                    "page": "358",
                    "subsections": [],
                    "content": " 1  What constraints are required to correctly parse word sequences like I am hap py and she is happy but not you is happy or they am happy Implement two sol utions for the present tense paradigm of the verb be in English first taking Gram mar 8 as your starting point and then taking Grammar 20 as the starting point 2  Develop a variant of grammar in Example 91 that uses a feature COUNT to make the distinctions shown here 56 a The boy sings b Boy sings 57 a The boys sing b Boys sing 58 a The water is precious b Water is precious 3  Write a function subsumes that holds of two feature structures fs1 and fs2 just in case fs1 subsumes fs2 4  Modify the grammar illustrated in 30 to incorporate a BAR feature for dealing with phrasal projections 5  Modify the German grammar in Example 94 to incorporate the treatment of subcategorization presented in Section 93 6  Develop a featurebased grammar that will correctly describe the following Spanish noun phrases 59 un cuadro hermoso INDEFSGMASC picture beautifulSGMASC a beautiful picture 60 unos cuadros hermosos INDEFPLMASC picturePL beautifulPLMASC beautiful pictures 61 una cortina hermosa INDEFSGFEM curtain beautifulSGFEM a beautiful curtain 62 unas cortinas hermosas INDEFPLFEM curtain beautifulPLFEM beautiful curtains 7  Develop a wrapper for the earleyparser so that a trace is only printed if the input sequence fails to parse 8  Consider the feature structures shown in Example 95 Example 95 Exploring feature structures fs1  nltkFeatStructA  x B C  x fs2  nltkFeatStructB  D  d fs3  nltkFeatStructB  C  d fs4  nltkFeatStructA  1B  b C1 fs5  nltkFeatStructA  1D  x C  E  1 F  x  fs6  nltkFeatStructA  D  d fs7  nltkFeatStructA  D  d C  F  D  d fs8  nltkFeatStructA  1D  x G  x C  B  x E  1  fs9  nltkFeatStructA  B  b C  E  G  e fs10  nltkFeatStructA  1B  b C  1 Work out on paper what the result is of the following unifications Hint you might find it useful to draw the graph structures a fs1 and fs2 b fs1 and fs3 c fs4 and fs5 d fs5 and fs6 e fs5 and fs7 f fs8 and fs9 g fs8 and fs10 Check your answers using NLTK 9  List two feature structures that subsume Ax Bx 10  Ignoring structure sharing give an informal algorithm for unifying two feature structures 11  Extend the German grammar in Example 94 so that it can handle socalled verb second structures like the following 63 Heute sieht der Hund die Katze 12  Seemingly synonymous verbs have slightly different syntactic properties Levin 1993 Consider the following patterns of grammaticality for the verbs loaded filled and dumped Can you write grammar productions to handle such data",
                    "word_count": 452
                }
            ]
        },
        {
            "title": "Chapter 10: Analyzing the Meaning of Sentences",
            "page": "361",
            "sections": [
                {
                    "title": "10.1 Natural Language Understanding",
                    "page": "362",
                    "subsections": [],
                    "content": " Querying a Database Suppose we have a program that lets us type in a natural language question and gives us back the right answer 1 a Which country is Athens in b Greece How hard is it to write such a program And can we just use the same techniques that weve encountered so far in this book or does it involve something new In this section we will show that solving the task in a restricted domain is pretty straightforward But we will also see that to address the problem in a more general way we have to open up a whole new box of ideas and techniques involving the representation of meaning 361 So lets start off by assuming that we have data about cities and countries in a structured form To be concrete we will use a database table whose first few rows are shown in Table 101 The data illustrated in Table 101 is drawn from the Chat80 system Warren  Pereira 1982 Population figures are given in thousands but note that the data used in these examples dates back at least to the 1980s and was already somewhat out of date at the point when Warren  Pereira 1982 was published Table 101 citytable A table of cities countries and populations City Country Population athens greece 1368 bangkok thailand 1178 barcelona spain 1280 berlin eastgermany 3481 birmingham unitedkingdom 1112 The obvious way to retrieve answers from this tabular data involves writing queries in a database query language such as SQL SQL Structured Query Language is a language designed for retrieving and managing data in relational databases If you want to find out more about SQL httpwwww3schoolscomsql is a convenient online reference For example executing the query 2 will pull out the value greece 2 SELECT Country FROM citytable WHERE City  athens This specifies a result set consisting of all values for the column Country in data rows where the value of the City column is athens How can we get the same effect using English as our input to the query system The featurebased grammar formalism described in Chapter 9 makes it easy to translate from English to SQL The grammar sql0fcfg illustrates how to assemble a meaning representation for a sentence in tandem with parsing the sentence Each phrase struc ture rule is supplemented with a recipe for constructing a value for the feature SEM You can see that these recipes are extremely simple in each case we use the string concat enation operation  to splice the values for the child constituents to make a value for the parent constituent  nltkdatashowcfggrammarsbookgrammarssql0fcfg  start S SSEMnp  WHERE  vp  NPSEMnp VPSEMvp VPSEMv  pp  IVSEMv PPSEMpp VPSEMv  ap  IVSEMv APSEMap NPSEMdet  n  DetSEMdet NSEMn PPSEMp  np  PSEMp NPSEMnp APSEMpp  ASEMa PPSEMpp NPSEMCountrygreece  Greece NPSEMCountrychina  China DetSEMSELECT  Which  What NSEMCity FROM citytable  cities IVSEM  are ASEM  located PSEM  in This allows us to parse a query into SQL  from nltk import loadparser  cp  loadparsergrammarsbookgrammarssql0fcfg  query  What cities are located in China  trees  cpnbestparsequerysplit  answer  trees0nodesem  q   joinanswer  print q SELECT City FROM citytable WHERE Countrychina Your Turn Run the parser with maximum tracing on ie cp  loadparsergrammarsbookgrammarssql0fcfg trace3 and ex amine how the values of SEM are built up as complete edges are added to the chart Finally we execute the query over the database citydb and retrieve some results  from nltksem import chat80  rows  chat80sqlquerycorporacitydatabasecitydb q  for r in rows print r0 canton chungking dairen harbin kowloon mukden peking shanghai sian tientsin Since each row r is a oneelement tuple we print out the member of the tuple rather than the tuple itself  To summarize we have defined a task where the computer returns useful data in re sponse to a natural language query and we implemented this by translating a small subset of English into SQL We can say that our NLTK code already understands SQL given that Python is able to execute SQL queries against a database and by ex tension it also understands queries such as What cities are located in China This parallels being able to translate from Dutch into English as an example of natural lan guage understanding Suppose that you are a native speaker of English and have started to learn Dutch Your teacher asks if you understand what 3 means 3 Margrietje houdt van Brunoke",
                    "word_count": 722
                },
                {
                    "title": "10.2 Propositional Logic",
                    "page": "368",
                    "subsections": [],
                    "content": " A logical language is designed to make reasoning formally explicit As a result it can capture aspects of natural language which determine whether a set of sentences is con sistent As part of this approach we need to develop logical representations of a sen tence  that formally capture the truthconditions of  Well start off with a simple example 8 Klaus chased Evi and Evi ran away Lets replace the two subsentences in 8 by  and  respectively and put  for the logical operator corresponding to the English word and    This structure is the logical form of 8 Propositional logic allows us to represent just those parts of linguistic structure that correspond to certain sentential connectives We have just looked at and Other such connectives are not or and if then In the formalization of propositional logic the counterparts of such connectives are sometimes called Boolean operators The basic expressions of propositional logic are propositional symbols often written as P Q R etc There are varying conventions for representing Boolean operators Since we will be focusing on ways of exploring logic within NLTK we will stick to the following ASCII versions of the operators  nltkbooleanops negation             conjunction          disjunction          implication          equivalence          From the propositional symbols and the Boolean operators we can build an infinite set of wellformed formulas or just formulas for short of propositional logic First every propositional letter is a formula Then if  is a formula so is  And if  and  are formulas then so are          and   Table 102 specifies the truthconditions for formulas containing these operators As before we use  and  as variables over sentences and abbreviate if and only if as iff Table 102 Truth conditions for the Boolean operators in propositional logic Boolean operator Truth conditions negation it is not the case that   is true in s iff  is false in s conjunction and    is true in s iff  is true in s and  is true in s Boolean operator Truth conditions disjunction or    is true in s iff  is true in s or  is true in s implication if  then     is true in s iff  is false in s or  is true in s equivalence if and only if    is true in s iff  and  are both true in s or both false in s These rules are generally straightforward though the truth conditions for implication depart in many cases from our usual intuitions about the conditional in English A formula of the form P  Q is false only when P is true and Q is false If P is false say P corresponds to The moon is made of green cheese and Q is true say Q corresponds to Two plus two equals four then P  Q will come out true NLTKs LogicParser parses logical expressions into various subclasses of Expression  lp  nltkLogicParser  lpparseP  Q NegatedExpression P  Q  lpparseP  Q AndExpression P  Q  lpparseP  R  Q OrExpression P  R  Q  lpparseP   P IffExpression P  P From a computational perspective logics give us an important tool for performing inference Suppose you state that Freedonia is not to the north of Sylvania and you give as your reasons that Sylvania is to the north of Freedonia In this case you have produced an argument The sentence Sylvania is to the north of Freedonia is the assumption of the argument while Freedonia is not to the north of Sylvania is the conclusion The step of moving from one or more assumptions to a conclusion is called inference Informally it is common to write arguments in a format where the conclu sion is preceded by therefore 9 Sylvania is to the north of Freedonia Therefore Freedonia is not to the north of Sylvania An argument is valid if there is no possible situation in which its premises are all true and its conclusion is not true Now the validity of 9 crucially depends on the meaning of the phrase to the north of in particular the fact that it is an asymmetric relation 10 if x is to the north of y then y is not to the north of x Unfortunately we cant express such rules in propositional logic the smallest elements we have to play with are atomic propositions and we cannot look inside these to talk about relations between individuals x and y The best we can do in this case is capture a particular case of the asymmetry Lets use the propositional symbol SnF to",
                    "word_count": 739
                },
                {
                    "title": "10.3 First-Order Logic",
                    "page": "384",
                    "subsections": [],
                    "content": " In the remainder of this chapter we will represent the meaning of natural language expressions by translating them into firstorder logic Not all of natural language se mantics can be expressed in firstorder logic But it is a good choice for computational semantics because it is expressive enough to represent many aspects of semantics and on the other hand there are excellent systems available off the shelf for carrying out automated inference in firstorder logic Our next step will be to describe how formulas of firstorder logic are constructed and then how such formulas can be evaluated in a model Syntax Firstorder logic keeps all the Boolean operators of propositional logic but it adds some important new mechanisms To start with propositions are analyzed into predicates and arguments which takes us a step closer to the structure of natural languages The standard construction rules for firstorder logic recognize terms such as individual variables and individual constants and predicates that take differing numbers of ar guments For example Angus walks might be formalized as walkangus and Angus sees Bertie as seeangus bertie We will call walk a unary predicate and see a binary predicate The symbols used as predicates do not have intrinsic meaning although it is hard to remember this Returning to one of our earlier examples there is no logical difference between 13a and 13b 13 a lovemargrietje brunoke b houdenvanmargrietje brunoke By itself firstorder logic has nothing substantive to say about lexical semanticsthe meaning of individual wordsalthough some theories of lexical semantics can be en coded in firstorder logic Whether an atomic predication like seeangus bertie is true or false in a situation is not a matter of logic but depends on the particular valuation that we have chosen for the constants see angus and bertie For this reason such expressions are called nonlogical constants By contrast logical constants such as the Boolean operators always receive the same interpretation in every model for firstorder logic We should mention here that one binary predicate has special status namely equality as in formulas such as angus  aj Equality is regarded as a logical constant since for individual terms t1 and t2 the formula t1  t2 is true if and only if t1 and t2 refer to one and the same entity It is often helpful to inspect the syntactic structure of expressions of firstorder logic and the usual way of doing this is to assign types to expressions Following the tradition of Montague grammar we will use two basic types e is the type of entities while t is the type of formulas ie expressions that have truth values Given these two basic types we can form complex types for function expressions That is given any types  and    is a complex type corresponding to functions from  things to  things For example e t is the type of expressions from entities to truth values namely unary predicates The LogicParser can be invoked so that it carries out type checking  tlp  nltkLogicParsertypecheckTrue  parsed  tlpparsewalkangus  parsedargument ConstantExpression angus  parsedargumenttype e  parsedfunction ConstantExpression walk  parsedfunctiontype e Why do we see e at the end of this example Although the typechecker will try to infer as many types as possible in this case it has not managed to fully specify the type of walk since its result type is unknown Although we are intending walk to receive type e t as far as the typechecker knows in this context it could be of some other type such as e e or e e t To help the typechecker we need to specify a signa ture implemented as a dictionary that explicitly associates types with nonlogical con stants  sig  walk e t  parsed  tlpparsewalkangus sig  parsedfunctiontype et A binary predicate has type e e t Although this is the type of something which combines first with an argument of type e to make a unary predicate we represent binary predicates as combining directly with their two arguments For example the predicate see in the translation of Angus sees Cyril will combine with its arguments to give the result seeangus cyril In firstorder logic arguments of predicates can also be individual variables such as x y and z In NLTK we adopt the convention that variables of type e are all lowercase Individual variables are similar to personal pronouns like he she and it in that we need to know about the context of use in order to figure out their denotation One way of interpreting the pronoun in 14 is by pointing to a relevant individual in the local context 14 He disappeared Another way is to supply a textual antecedent for the pronoun he for example by uttering 15a prior to 14 Here we say that he is coreferential with the noun phrase Cyril In such a context 14 is semantically equivalent to 15b 15 a Cyril is Anguss dog b Cyril disappeared Consider by contrast the occurrence of he in 16a In this case it is bound by the indefinite NP a dog and this is a different relationship than coreference If we replace the pronoun he by a dog the result 16b is not semantically equivalent to 16a 16 a Angus had a dog but he disappeared b Angus had a dog but a dog disappeared Corresponding to 17a we can construct an open formula 17b with two occurrences of the variable x We ignore tense to simplify exposition 17 a He is a dog and he disappeared b dogx  disappearx By placing an existential quantifier x for some x in front of 17b we can bind these variables as in 18a which means 18b or more idiomatically 18c 18 a xdogx  disappearx b At least one entity is a dog and disappeared c A dog disappeared Here is the NLTK counterpart of 18a 19 exists xdogx  disappearx In addition to the existential quantifier firstorder logic offers us the universal quan tifier x for all x illustrated in 20 20 a xdogx  disappearx b Everything has the property that if it is a dog it disappears c Every dog disappeared Here is the NLTK counterpart of 20a 21 all xdogx  disappearx Although 20a is the standard firstorder logic translation of 20c the truth conditions arent necessarily what you expect The formula says that if some x is a dog then x disappearsbut it doesnt say that there are any dogs So in a situation where there are no dogs 20a will still come out true Remember that P  Q is true when P is false Now you might argue that every dog disappeared does presuppose the existence of dogs and that the logic formalization is simply wrong But it is possible to find other examples that lack such a presupposition For instance we might explain that the value of the Python expression astringreplaceate 8 is the result of replacing every occur rence of ate in astring by 8 even though there may in fact be no such occurrences Table 32 We have seen a number of examples where variables are bound by quantifiers What happens in formulas such as the following exists x dogx  barkx The scope of the exists x quantifier is dogx so the occurrence of x in barkx is unbound Consequently it can become bound by some other quantifier for example all x in the next formula all xexists x dogx  barkx In general an occurrence of a variable x in a formula  is free in  if that occurrence doesnt fall within the scope of all x or some x in  Conversely if x is free in formula  then it is bound in all x and exists x If all variable occurrences in a formula are bound the formula is said to be closed We mentioned before that the parse method of NLTKs LogicParser returns objects of class Expression Each instance expr of this class comes with a method free which returns the set of variables that are free in expr  lp  nltkLogicParser  lpparsedogcyrilfree set  lpparsedogxfree setVariablex  lpparseownangus cyrilfree set  lpparseexists xdogxfree set  lpparsesome x walkx  singxfree setVariablex  lpparseexists xowny xfree setVariabley FirstOrder Theorem Proving Recall the constraint on to the north of which we proposed earlier as 10 22 if x is to the north of y then y is not to the north of x We observed that propositional logic is not expressive enough to represent generali zations about binary predicates and as a result we did not properly capture the argu ment Sylvania is to the north of Freedonia Therefore Freedonia is not to the north of Sylvania You have no doubt realized that firstorder logic by contrast is ideal for formalizing such rules all x all ynorthofx y  northofy x Even better we can perform automated inference to show the validity of the argument The general case in theorem proving is to determine whether a formula that we want to prove a proof goal can be derived by a finite sequence of inference steps from a list of assumed formulas We write this as A  g where A is a possibly empty list of assumptions and g is a proof goal We will illustrate this with NLTKs interface to the theorem prover Prover9 First we parse the required proof goal and the two as sumptions   Then we create a Prover9 instance  and call its prove method on the goal given the list of assumptions   NotFnS  lpparsenorthoff s  SnF  lpparsenorthofs f  R  lpparseall x all y northofx y  northofy x  prover  nltkProver9  proverproveNotFnS SnF R True Happily the theorem prover agrees with us that the argument is valid By contrast it concludes that it is not possible to infer northoff s from our assumptions  FnS  lpparsenorthoff s  proverproveFnS SnF R False Summarizing the Language of FirstOrder Logic Well take this opportunity to restate our earlier syntactic rules for propositional logic and add the formation rules for quantifiers together these give us the syntax of first order logic In addition we make explicit the types of the expressions involved Well adopt the convention that en t is the type of a predicate that combines with n argu ments of type e to yield an expression of type t In this case we say that n is the arity of the predicate 1 If P is a predicate of type en t and 1  n are terms of type e then P1  n is of type t 2 If  and  are both of type e then    and    are of type t 3 If  is of type t then so is  4 If  and  are of type t then so are          and    5 If  is of type t and x is a variable of type e then exists x and all x are of type t Table 103 summarizes the new logical constants of the logic module and two of the methods of Expressions Table 103 Summary of new logical relations and operators required for firstorder logic Example Description  Equality  Inequality exists Existential quantifier all Universal quantifier Truth in Model We have looked at the syntax of firstorder logic and in Section 104 we will examine the task of translating English into firstorder logic Yet as we argued in Section 101 this gets us further forward only if we can give a meaning to sentences of firstorder logic In other words we need to give a truthconditional semantics to firstorder logic From the point of view of computational semantics there are obvious limits to how far one can push this approach Although we want to talk about sentences being true or false in situations we only have the means of representing situations in the computer in a symbolic manner Despite this limitation it is still possible to gain a clearer picture of truthconditional semantics by encoding models in NLTK Given a firstorder logic language L a model M for L is a pair D Val where D is an nonempty set called the domain of the model and Val is a function called the valu ation function which assigns values from D to expressions of L as follows 1 For every individual constant c in L Valc is an element of D 2 For every predicate symbol P of arity n  0 ValP is a function from Dn to True False If the arity of P is 0 then ValP is simply a truth value and P is regarded as a propositional symbol According to 2 if P is of arity 2 then ValP will be a function f from pairs of elements of D to True False In the models we shall build in NLTK well adopt a more con venient alternative in which ValP is a set S of pairs defined as follows 23 S  s  fs  True Such an f is called the characteristic function of S as discussed in the further readings Relations are represented semantically in NLTK in the standard settheoretic way as sets of tuples For example lets suppose we have a domain of discourse consisting of the individuals Bertie Olive and Cyril where Bertie is a boy Olive is a girl and Cyril is a dog For mnemonic reasons we use b o and c as the corresponding labels in the model We can declare the domain as follows  dom  setb o c We will use the utility function parsevaluation to convert a sequence of strings of the form symbol  value into a Valuation object  v    bertie  b  olive  o  cyril  c  boy  b  girl  o  dog  c  walk  o c  see  b o c b o c    val  nltkparsevaluationv  print val bertie b boy setb cyril c dog setc girl seto olive o see seto c c b b o walk setc o So according to this valuation the value of see is a set of tuples such that Bertie sees Olive Cyril sees Bertie and Olive sees Cyril Your Turn Draw a picture of the domain dom and the sets correspond ing to each of the unary predicates by analogy with the diagram shown in Figure 102 You may have noticed that our unary predicates ie boy girl dog also come out as sets of singleton tuples rather than just sets of individuals This is a convenience which allows us to have a uniform treatment of relations of any arity A predication of the form P1  n where P is of arity n comes out true just in case the tuple of values corresponding to 1  n belongs to the set of tuples in the value of P  o c in valsee True  b in valboy True Individual Variables and Assignments In our models the counterpart of a context of use is a variable assignment This is a mapping from individual variables to entities in the domain Assignments are created using the Assignment constructor which also takes the models domain of discourse as a parameter We are not required to actually enter any bindings but if we do they are in a variable value format similar to what we saw earlier for valuations  g  nltkAssignmentdom x o y c  g y c x o In addition there is a print format for assignments which uses a notation closer to that often found in logic textbooks  print g gcyox Lets now look at how we can evaluate an atomic formula of firstorder logic First we create a model and then we call the evaluate method to compute the truth value  m  nltkModeldom val  mevaluateseeolive y g True Whats happening here We are evaluating a formula which is similar to our earlier example seeolive cyril However when the interpretation function encounters the variable y rather than checking for a value in val it asks the variable assignment g to come up with a value  gy c Since we already know that individuals o and c stand in the see relation the value True is what we expected In this case we can say that assignment g satisfies the for mula seeolive y By contrast the following formula evaluates to False relative to g check that you see why this is  mevaluateseey x g False In our approach though not in standard firstorder logic variable assignments are partial For example g says nothing about any variables apart from x and y The method purge clears all bindings from an assignment  gpurge  g  If we now try to evaluate a formula such as seeolive y relative to g it is like trying to interpret a sentence containing a him when we dont know what him refers to In this case the evaluation function fails to deliver a truth value  mevaluateseeolive y g Undefined Since our models already contain rules for interpreting Boolean operators arbitrarily complex formulas can be composed and evaluated  mevaluateseebertie olive  boybertie  walkbertie g True The general process of determining truth or falsity of a formula in a model is called model checking Quantification One of the crucial insights of modern logic is that the notion of variable satisfaction can be used to provide an interpretation for quantified formulas Lets use 24 as an example 24 exists xgirlx  walkx When is it true Lets think about all the individuals in our domain ie in dom We want to check whether any of these individuals has the property of being a girl and walking In other words we want to know if there is some u in dom such that gux satisfies the open formula 25 25 girlx  walkx Consider the following  mevaluateexists xgirlx  walkx g True evaluate returns True here because there is some u in dom such that 25 is satisfied by an assignment which binds x to u In fact o is such a u  mevaluategirlx  walkx gaddx o True One useful tool offered by NLTK is the satisfiers method This returns a set of all the individuals that satisfy an open formula The method parameters are a parsed for mula a variable and an assignment Here are a few examples  fmla1  lpparsegirlx  boyx  msatisfiersfmla1 x g setb o  fmla2  lpparsegirlx  walkx  msatisfiersfmla2 x g setc b o  fmla3  lpparsewalkx  girlx  msatisfiersfmla3 x g setb o Its useful to think about why fmla2 and fmla3 receive the values they do The truth conditions for  mean that fmla2 is equivalent to girlx  walkx which is satisfied by something that either isnt a girl or walks Since neither b Bertie nor c Cyril are girls according to model m they both satisfy the whole formula And of course o satisfies the formula because o satisfies both disjuncts Now since every member of the domain of discourse satisfies fmla2 the corresponding universally quantified formula is also true  mevaluateall xgirlx  walkx g True In other words a universally quantified formula x is true with respect to g just in case for every u  is true with respect to gux Your Turn Try to figure out first with pencil and paper and then using mevaluate what the truth values are for all xgirlx  walkx and exists xboyx  walkx Make sure you understand why they receive these values Quantifier Scope Ambiguity What happens when we want to give a formal representation of a sentence with two quantifiers such as the following 26 Everybody admires someone There are at least two ways of expressing 26 in firstorder logic 27 a all xpersonx  exists ypersony  admirexy b exists ypersony  all xpersonx  admirexy Can we use both of these The answer is yes but they have different meanings 27b is logically stronger than 27a it claims that there is a unique person say Bruce who is admired by everyone 27a on the other hand just requires that for every person u we can find some person u whom u admires but this could be a different person u in each case We distinguish between 27a and 27b in terms of the scope of the quantifiers In the first  has wider scope than  whereas in 27b the scope ordering is reversed So now we have two ways of representing the meaning of 26 and they are both quite legitimate In other words we are claiming that 26 is ambiguous with respect to quantifier scope and the formulas in 27 give us a way to make the two readings explicit However we are not just interested in associating two distinct rep resentations with 26 we also want to show in detail how the two representations lead to different conditions for truth in a model In order to examine the ambiguity more closely lets fix our valuation as follows  v2    bruce  b  cyril  c  elspeth  e  julia  j  matthew  m  person  b e j m  admire  j b b b m e e m c a    val2  nltkparsevaluationv2 The admire relation can be visualized using the mapping diagram shown in 28 28 In 28 an arrow between two individuals x and y indicates that x admires y So j and b both admire b Bruce is very vain while e admires m and m admires e In this model formula 27a is true but 27b is false One way of exploring these results is by using the satisfiers method of Model objects  dom2  val2domain  m2  nltkModeldom2 val2  g2  nltkAssignmentdom2  fmla4  lpparsepersonx  exists ypersony  admirex y  m2satisfiersfmla4 x g2 seta c b e j m This shows that fmla4 holds of every individual in the domain By contrast consider the formula fmla5 this has no satisfiers for the variable y  fmla5  lpparsepersony  all xpersonx  admirex y  m2satisfiersfmla5 y g2 set That is there is no person that is admired by everybody Taking a different open for mula fmla6 we can verify that there is a person namely Bruce who is admired by both Julia and Bruce  fmla6  lpparsepersony  all xx  bruce  x  julia  admirex y  m2satisfiersfmla6 y g2 setb Your Turn Devise a new model based on m2 such that 27a comes out false in your model similarly devise a new model such that 27b comes out true Model Building We have been assuming that we already had a model and wanted to check the truth of a sentence in the model By contrast model building tries to create a new model given some set of sentences If it succeeds then we know that the set is consistent since we have an existence proof of the model We invoke the Mace4 model builder by creating an instance of Mace and calling its buildmodel method in an analogous way to calling the Prover9 theorem prover One option is to treat our candidate set of sentences as assumptions while leaving the goal unspecified The following interaction shows how both a c1 and a c2 are con sistent lists since Mace succeeds in building a model for each of them whereas c1 c2 is inconsistent  a3  lpparseexists xmanx  walksx  c1  lpparsemortalsocrates  c2  lpparsemortalsocrates  mb  nltkMace5  print mbbuildmodelNone a3 c1 True  print mbbuildmodelNone a3 c2 True  print mbbuildmodelNone c1 c2 False We can also use the model builder as an adjunct to the theorem prover Lets suppose we are trying to prove A  g ie that g is logically derivable from assumptions A  a1 a2  an We can feed this same input to Mace4 and the model builder will try to find a counterexample that is to show that g does not follow from A So given this input Mace4 will try to find a model for the assumptions A together with the negation of g namely the list A  a1 a2  an g If g fails to follow from S then Mace4 may well return with a counterexample faster than Prover9 concludes that it cannot find the required proof Conversely if g is provable from S Mace4 may take a long time unsuccessfully trying to find a countermodel and will eventually give up Lets consider a concrete scenario Our assumptions are the list There is a woman that every man loves Adam is a man Eve is a woman Our conclusion is Adam loves Eve Can Mace4 find a model in which the premises are true but the conclusion is false In the following code we use MaceCommand which will let us inspect the model that has been built  a4  lpparseexists y womany  all x manx  lovexy  a5  lpparsemanadam  a6  lpparsewomaneve  g  lpparseloveadameve  mc  nltkMaceCommandg assumptionsa4 a5 a6  mcbuildmodel True So the answer is yes Mace4 found a countermodel in which there is some woman other than Eve that Adam loves But lets have a closer look at Mace4s model converted to the format we use for valuations  print mcvaluation C1 b adam a eve a love seta b man seta woman seta b The general form of this valuation should be familiar to you it contains some individual constants and predicates each with an appropriate kind of value What might be puz zling is the C1 This is a Skolem constant that the model builder introduces as a representative of the existential quantifier That is when the model builder encoun tered the exists y part of a4 it knew that there is some individual b in the domain which satisfies the open formula in the body of a4 However it doesnt know whether b is also the denotation of an individual constant anywhere else in its input so it makes up a new name for b on the fly namely C1 Now since our premises said nothing about the individual constants adam and eve the model builder has decided there is no reason to treat them as denoting different entities and they both get mapped to a Moreover we didnt specify that man and woman denote disjoint sets so the model builder lets their denotations overlap This illustrates quite dramatically the implicit knowledge that we bring to bear in interpreting our scenario but which the model builder knows nothing about So lets add a new assumption which makes the sets of men and women disjoint The model builder still produces a countermodel but this time it is more in accord with our intuitions about the situation  a7  lpparseall x manx  womanx  g  lpparseloveadameve  mc  nltkMaceCommandg assumptionsa4 a5 a6 a7  mcbuildmodel True  print mcvaluation C1 c adam a eve b love seta c man seta woman setb c On reflection we can see that there is nothing in our premises which says that Eve is the only woman in the domain of discourse so the countermodel in fact is acceptable If we wanted to rule it out we would have to add a further assumption such as exists y all x womanx  x  y to ensure that there is only one woman in the model",
                    "word_count": 4438
                },
                {
                    "title": "10.4 The Semantics of English Sentences",
                    "page": "384",
                    "subsections": [],
                    "content": " Compositional Semantics in FeatureBased Grammar At the beginning of the chapter we briefly illustrated a method of building semantic representations on the basis of a syntactic parse using the grammar framework devel oped in Chapter 9 This time rather than constructing an SQL query we will build a logical form One of our guiding ideas for designing such grammars is the Principle of Compositionality Also known as Freges Principle see Partee 1995 for the for mulation given Principle of Compositionality the meaning of a whole is a function of the meanings of the parts and of the way they are syntactically combined We will assume that the semantically relevant parts of a complex expression are given by a theory of syntactic analysis Within this chapter we will take it for granted that expressions are parsed against a contextfree grammar However this is not entailed by the Principle of Compositionality Our goal now is to integrate the construction of a semantic representation in a manner that can be smoothly with the process of parsing 29 illustrates a first approximation to the kind of analyses we would like to build 29 In 29 the SEM value at the root node shows a semantic representation for the whole sentence while the SEM values at lower nodes show semantic representations for con stituents of the sentence Since the values of SEM have to be treated in a special manner they are distinguished from other feature values by being enclosed in angle brackets So far so good but how do we write grammar rules that will give us this kind of result Our approach will be similar to that adopted for the grammar sql0fcfg at the start of this chapter in that we will assign semantic representations to lexical nodes and then compose the semantic representations for each phrase from those of its child nodes However in the present case we will use function application rather than string con catenation as the mode of composition To be more specific suppose we have NP and VP constituents with appropriate values for their SEM nodes Then the SEM value of an S is handled by a rule like 30 Observe that in the case where the value of SEM is a variable we omit the angle brackets",
                    "word_count": 380
                },
                {
                    "title": "10.5 Discourse Semantics",
                    "page": "396",
                    "subsections": [],
                    "content": " A discourse is a sequence of sentences Very often the interpretation of a sentence in a discourse depends on what preceded it A clear example of this comes from anaphoric pronouns such as he she and it Given a discourse such as Angus used to have a dog But he recently disappeared you will probably interpret he as referring to Anguss dog However in Angus used to have a dog He took him for walks in New Town you are more likely to interpret he as referring to Angus himself Discourse Representation Theory The standard approach to quantification in firstorder logic is limited to single senten ces Yet there seem to be examples where the scope of a quantifier can extend over two or more sentences We saw one earlier and heres a second example together with a translation 54 a Angus owns a dog It bit Irene b xdogx  ownAngus x  bitex Irene That is the NP a dog acts like a quantifier which binds the it in the second sentence Discourse Representation Theory DRT was developed with the specific goal of pro viding a means for handling this and other semantic phenomena which seem to be characteristic of discourse A discourse representation structure DRS presents the meaning of discourse in terms of a list of discourse referents and a list of conditions The discourse referents are the things under discussion in the discourse and they correspond to the individual variables of firstorder logic The DRS conditions apply to those discourse referents and correspond to atomic open formulas of firstorder logic Figure 104 illustrates how a DRS for the first sentence in 54a is augmented to become a DRS for both sentences When the second sentence of 54a is processed it is interpreted in the context of what is already present in the lefthand side of Figure 104 The pronoun it triggers the addi tion of a new discourse referent say u and we need to find an anaphoric antecedent for itthat is we want to work out what it refers to In DRT the task of finding the antecedent for an anaphoric pronoun involves linking it to a discourse ref erent already within the current DRS and y is the obvious choice We will say more about anaphora resolution shortly This processing step gives rise to a new condition u  y The remaining content contributed by the second sentence is also merged with the content of the first and this is shown on the righthand side of Figure 104 Figure 104 illustrates how a DRS can represent more than just a single sentence In this case it is a twosentence discourse but in principle a single DRS could correspond to the interpretation of a whole text We can inquire into the truth conditions of the righthand DRS in Figure 104 Informally it is true in some situation s if there are entities a c and i in s corresponding to the discourse referents in the DRS such that",
                    "word_count": 500
                },
                {
                    "title": "10.6 Summary",
                    "page": "402",
                    "subsections": [],
                    "content": "  Firstorder logic is a suitable language for representing natural language meaning in a computational setting since it is flexible enough to represent many useful as pects of natural meaning and there are efficient theorem provers for reasoning with firstorder logic Equally there are a variety of phenomena in natural language semantics which are believed to require more powerful logical mechanisms  As well as translating natural language sentences into firstorder logic we can state the truth conditions of these sentences by examining models of firstorder formu las  In order to build meaning representations compositionally we supplement first order logic with the calculus  reduction in the calculus corresponds semantically to application of a function to an argument Syntactically it involves replacing a variable bound by  in the function expression with the expression that provides the argument in the function application  A key part of constructing a model lies in building a valuation which assigns in terpretations to nonlogical constants These are interpreted as either nary predi cates or as individual constants  An open expression is an expression containing one or more free variables Open expressions receive an interpretation only when their free variables receive values from a variable assignment  Quantifiers are interpreted by constructing for a formula x open in variable x the set of individuals which make x true when an assignment g assigns them as the value of x The quantifier then places constraints on that set  A closed expression is one that has no free variables that is the variables are all bound A closed sentence is true or false with respect to all variable assignments  If two formulas differ only in the label of the variable bound by binding operator ie  or a quantifier  they are said to be equivalents The result of relabeling a bound variable in a formula is called conversion  Given a formula with two nested quantifiers Q1 and Q2 the outermost quantifier Q1 is said to have wide scope or scope over Q2 English sentences are frequently ambiguous with respect to the scope of the quantifiers they contain  English sentences can be associated with a semantic representation by treating SEM as a feature in a featurebased grammar The SEM value of a complex expressions typically involves functional application of the SEM values of the component expressions",
                    "word_count": 382
                },
                {
                    "title": "10.7 Further Reading",
                    "page": "402",
                    "subsections": [],
                    "content": " Consult httpwwwnltkorg for further materials on this chapter and on how to install the Prover9 theorem prover and Mace4 model builder General information about these two inference tools is given by McCune 2008 For more examples of semantic analysis with NLTK please see the semantics and logic HOWTOs at httpwwwnltkorghowto Note that there are implementations of two other approaches to scope ambiguity namely Hole semantics as described in Black burn  Bos 2005 and Glue semantics as described in Dalrymple et al 1999 There are many phenomena in natural language semantics that have not been touched on in this chapter most notably 1 Events tense and aspect 2 Semantic roles 3 Generalized quantifiers such as most 4 Intensional constructions involving for example verbs such as may and believe While 1 and 2 can be dealt with using firstorder logic 3 and 4 require different logics These issues are covered by many of the references in the following readings A comprehensive overview of results and techniques in building natural language front ends to databases can be found in Androutsopoulos Ritchie  Thanisch 1995 Any introductory book to modern logic will present propositional and firstorder logic Hodges 1977 is highly recommended as an entertaining and insightful text with many illustrations from natural language For a wideranging twovolume textbook on logic that also presents contemporary material on the formal semantics of natural language including Montague Grammar and intensional logic see Gamut 1991a 1991b Kamp  Reyle 1993 provides the",
                    "word_count": 243
                },
                {
                    "title": "10.8 Exercises",
                    "page": "404",
                    "subsections": [],
                    "content": " 1  Translate the following sentences into propositional logic and verify that they parse with LogicParser Provide a key that shows how the propositional variables in your translation correspond to expressions of English a If Angus sings it is not the case that Bertie sulks b Cyril runs and barks c It will snow if it doesnt rain d Its not the case that Irene will be happy if Olive or Tofu comes e Pat didnt cough or sneeze f If you dont come if I call I wont come if you call 2  Translate the following sentences into predicateargument formulas of firstorder logic a Angus likes Cyril and Irene hates Cyril b Tofu is taller than Bertie c Bruce loves himself and Pat does too d Cyril saw Bertie but Angus didnt e Cyril is a fourlegged friend f Tofu and Olive are near each other 3  Translate the following sentences into quantified formulas of firstorder logic a Angus likes someone and someone likes Julia b Angus loves a dog who loves him c Nobody smiles at Pat d Somebody coughs and sneezes e Nobody coughed or sneezed f Bruce loves somebody other than Bruce g Nobody other than Matthew loves Pat h Cyril likes everyone except for Irene i Exactly one person is asleep 4  Translate the following verb phrases using abstracts and quantified formulas of firstorder logic a feed Cyril and give a capuccino to Angus b be given War and Peace by Pat c be loved by everyone d be loved or detested by everyone e be loved by everyone and detested by noone 5  Consider the following statements  lp  nltkLogicParser  e2  lpparsepat  e3  nltkApplicationExpressione1 e2  print e3simplify exists ylovepat y Clearly something is missing here namely a declaration of the value of e1 In order for ApplicationExpressione1 e2 to be convertible to exists ylovepat y e1 must be a abstract which can take pat as an argument Your task is to construct such an abstract bind it to e1 and satisfy yourself that these statements are all satisfied up to alphabetic variance In addition provide an informal English translation of e3simplify Now carry on doing this same task for the further cases of e3simplify shown here  print e3simplify exists ylovepaty  loveypat  print e3simplify exists ylovepaty  loveypat  print e3simplify walkfido 6  As in the preceding exercise find a abstract e1 that yields results equivalent to those shown here  e2  lpparsechase  e3  nltkApplicationExpressione1 e2",
                    "word_count": 406
                }
            ]
        },
        {
            "title": "Chapter 11: Managing Linguistic Data",
            "page": "407",
            "sections": [
                {
                    "title": "11.1 Corpus Structure: A Case Study",
                    "page": "408",
                    "subsections": [],
                    "content": " The TIMIT Corpus was the first annotated speech database to be widely distributed and it has an especially clear organization TIMIT was developed by a consortium in cluding Texas Instruments and MIT from which it derives its name It was designed to provide data for the acquisition of acousticphonetic knowledge and to support the development and evaluation of automatic speech recognition systems The Structure of TIMIT Like the Brown Corpus which displays a balanced selection of text genres and sources TIMIT includes a balanced selection of dialects speakers and materials For each of eight dialect regions 50 male and female speakers having a range of ages and educa tional backgrounds each read 10 carefully chosen sentences Two sentences read by all speakers were designed to bring out dialect variation 407 1 a she had your dark suit in greasy wash water all year b dont ask me to carry an oily rag like that The remaining sentences were chosen to be phonetically rich involving all phones sounds and a comprehensive range of diphones phone bigrams Additionally the design strikes a balance between multiple speakers saying the same sentence in order to permit comparison across speakers and having a large range of sentences covered by the corpus to get maximal coverage of diphones Five of the sentences read by each speaker are also read by six other speakers for comparability The remaining three sentences read by each speaker were unique to that speaker for coverage NLTK includes a sample from the TIMIT Corpus You can access its documentation in the usual way using helpnltkcorpustimit Print nltkcorpustimitfileids to see a list of the 160 recorded utterances in the corpus sample Each filename has internal structure as shown in Figure 111 Figure 111 Structure of a TIMIT identifier Each recording is labeled using a string made up of the speakers dialect region gender speaker identifier sentence type and sentence identifier Each item has a phonetic transcription which can be accessed using the phones meth od We can access the corresponding word tokens in the customary way Both access methods permit an optional argument offsetTrue which includes the start and end offsets of the corresponding span in the audio file  phonetic  nltkcorpustimitphonesdr1fvmh0sa1  phonetic h sh iy hv ae dcl y ix dcl d aa kcl s ux tcl en gcl g r iy s iy w aa sh epi w aa dx ax q ao l y ih ax h  nltkcorpustimitwordtimesdr1fvmh0sa1 she 7812 10610 had 10610 14496 your 14496 15791 dark 15791 20720 suit 20720 25647 in 25647 26906 greasy 26906 32668 wash 32668 37890 water 38531 42417 all 43091 46052 year 46052 50522 In addition to this text data TIMIT includes a lexicon that provides the canonical pronunciation of every word which can be compared with a particular utterance  timitdict  nltkcorpustimittranscriptiondict  timitdictgreasy  timitdictwash  timitdictwater g r iy1 s iy w ao1 sh w ao1 t axr  phonetic1730 g r iy s iy w aa sh epi w aa dx ax This gives us a sense of what a speech processing system would have to do in producing or recognizing speech in this particular dialect New England Finally TIMIT includes demographic data about the speakers permitting finegrained study of vocal social and gender characteristics  nltkcorpustimitspkrinfodr1fvmh0 SpeakerInfoidVMH0 sexF dr1 useTRN recdate031186 birthdate010860 ht505 raceWHT eduBS commentsBEST NEW ENGLAND ACCENT SO FAR Notable Design Features TIMIT illustrates several key features of corpus design First the corpus contains two layers of annotation at the phonetic and orthographic levels In general a text or speech corpus may be annotated at many different linguistic levels including morphological syntactic and discourse levels Moreover even at a given level there may be different labeling schemes or even disagreement among annotators such that we want to rep resent multiple versions A second property of TIMIT is its balance across multiple dimensions of variation for coverage of dialect regions and diphones The inclusion of speaker demographics brings in many more independent variables that may help to account for variation in the data and which facilitate later uses of the corpus for pur poses that were not envisaged when the corpus was created such as sociolinguistics A third property is that there is a sharp division between the original linguistic event captured as an audio recording and the annotations of that event The same holds true of text corpora in the sense that the original text usually has an external source and is considered to be an immutable artifact Any transformations of that artifact which involve human judgmenteven something as simple as tokenizationare subject to later revision thus it is important to retain the source material in a form that is as close to the original as possible A fourth feature of TIMIT is the hierarchical structure of the corpus With 4 files per sentence and 10 sentences for each of 500 speakers there are 20000 files These are organized into a tree structure shown schematically in Figure 112 At the top level",
                    "word_count": 829
                },
                {
                    "title": "11.2 The Life Cycle of a Corpus",
                    "page": "412",
                    "subsections": [],
                    "content": " Corpora are not born fully formed but involve careful preparation and input from many people over an extended period Raw data needs to be collected cleaned up documented and stored in a systematic structure Various layers of annotation might be applied some requiring specialized knowledge of the morphology or syntax of the language Success at this stage depends on creating an efficient workflow involving appropriate tools and format converters Quality control procedures can be put in place to find inconsistencies in the annotations and to ensure the highest possible level of interannotator agreement Because of the scale and complexity of the task large cor pora may take years to prepare and involve tens or hundreds of personyears of effort In this section we briefly review the various stages in the life cycle of a corpus Three Corpus Creation Scenarios In one type of corpus the design unfolds over in the course of the creators explorations This is the pattern typical of traditional field linguistics in which material from elic itation sessions is analyzed as it is gathered with tomorrows elicitation often based on questions that arise in analyzing todays The resulting corpus is then used during sub sequent years of research and may serve as an archival resource indefinitely Comput erization is an obvious boon to work of this type as exemplified by the popular program Shoebox now over two decades old and rereleased as Toolbox see Section 24 Other software tools even simple word processors and spreadsheets are routinely used to acquire the data In the next section we will look at how to extract data from these sources Another corpus creation scenario is typical of experimental research where a body of carefully designed material is collected from a range of human subjects then analyzed to evaluate a hypothesis or develop a technology It has become common for such databases to be shared and reused within a laboratory or company and often to be published more widely Corpora of this type are the basis of the common task method of research management which over the past two decades has become the norm in governmentfunded research programs in language technology We have already en countered many such corpora in the earlier chapters we will see how to write Python programs to implement the kinds of curation tasks that are necessary before such cor pora are published Finally there are efforts to gather a reference corpus for a particular language such as the American National Corpus ANC and the British National Corpus BNC Here the goal has been to produce a comprehensive record of the many forms styles and uses of a language Apart from the sheer challenge of scale there is a heavy reliance on automatic annotation tools together with postediting to fix any errors However we can write programs to locate and repair the errors and also to analyze the corpus for balance Quality Control Good tools for automatic and manual preparation of data are essential However the creation of a highquality corpus depends just as much on such mundane things as documentation training and workflow Annotation guidelines define the task and document the markup conventions They may be regularly updated to cover difficult cases along with new rules that are devised to achieve more consistent annotations Annotators need to be trained in the procedures including methods for resolving cases not covered in the guidelines A workflow needs to be established possibly with sup porting software to keep track of which files have been initialized annotated validated manually checked and so on There may be multiple layers of annotation provided by different specialists Cases of uncertainty or disagreement may require adjudication Large annotation tasks require multiple annotators which raises the problem of achieving consistency How consistently can a group of annotators perform We can easily measure consistency by having a portion of the source material independently annotated by two people This may reveal shortcomings in the guidelines or differing abilities with the annotation task In cases where quality is paramount the entire corpus can be annotated twice and any inconsistencies adjudicated by an expert It is considered best practice to report the interannotator agreement that was achieved for a corpus eg by doubleannotating 10 of the corpus This score serves as a helpful upper bound on the expected performance of any automatic system that is trained on this corpus Caution Care should be exercised when interpreting an interannotator agree ment score since annotation tasks vary greatly in their difficulty For example 90 agreement would be a terrible score for partofspeech tagging but an exceptional score for semantic role labeling The Kappa coefficient  measures agreement between two people making category judgments correcting for expected chance agreement For example suppose an item is to be annotated and four coding options are equally likely In this case two people coding randomly would be expected to agree 25 of the time Thus an agreement of",
                    "word_count": 822
                },
                {
                    "title": "11.3 Acquiring Data",
                    "page": "416",
                    "subsections": [],
                    "content": " Obtaining Data from the Web The Web is a rich source of data for language analysis purposes We have already discussed methods for accessing individual files RSS feeds and search engine results see Section 31 However in some cases we want to obtain large quantities of web text The simplest approach is to obtain a published corpus of web text The ACL Special Interest Group on Web as Corpus SIGWAC maintains a list of resources at http wwwsigwacorguk The advantage of using a welldefined web corpus is that they are documented stable and permit reproducible experimentation If the desired content is localized to a particular website there are many utilities for capturing all the accessible contents of a site such as GNU Wget httpwwwgnuorg softwarewget For maximal flexibility and control a web crawler can be used such as Heritrix httpcrawlerarchiveorg Crawlers permit finegrained control over where to look which links to follow and how to organize the results For example if we want to compile a bilingual text collection having corresponding pairs of documents in each language the crawler needs to detect the structure of the site in order to extract the correspondence between the documents and it needs to organize the downloaded pages in such a way that the correspondence is captured It might be tempting to write your own web crawler but there are dozens of pitfalls having to do with detecting MIME types converting relative to absolute URLs avoiding getting trapped in cyclic link structures dealing with network latencies avoiding overloading the site or being banned from accessing the site and so on Obtaining Data from Word Processor Files Word processing software is often used in the manual preparation of texts and lexicons in projects that have limited computational infrastructure Such projects often provide templates for data entry though the word processing software does not ensure that the data is correctly structured For example each text may be required to have a title and date Similarly each lexical entry may have certain obligatory fields As the data grows in size and complexity a larger proportion of time may be spent maintaining its con sistency How can we extract the content of such files so that we can manipulate it in external programs Moreover how can we validate the content of these files to help authors create wellstructured data so that the quality of the data can be maximized in the context of the original authoring process Consider a dictionary in which each entry has a partofspeech field drawn from a set of 20 possibilities displayed after the pronunciation field and rendered in 11point bold type No conventional word processor has search or macro functions capable of verifying that all partofspeech fields have been correctly entered and displayed This task requires exhaustive manual checking If the word processor permits the document to be saved in a nonproprietary format such as text HTML or XML we can some times write programs to do this checking automatically Consider the following fragment of a lexical entry sleep slip vi condition of body and mind We can key in such text using MSWord then Save as Web Page then inspect the resulting HTML file p classMsoNormalsleep span stylemsospacerunyes span span classSpellEslipspan span stylemsospacerunyes span bspan stylefontsize110ptvispanb span stylemsospacerunyes span ia condition of body and mind opopi p Observe that the entry is represented as an HTML paragraph using the p element and that the part of speech appears inside a span stylefontsize110pt element The following program defines the set of legal partsofspeech legalpos Then it ex tracts all 11point content from the dicthtm file and stores it in the set usedpos Observe that the search pattern contains a parenthesized subexpression only the material that matches this subexpression is returned by refindall Finally the program constructs the set of illegal partsofspeech as the set difference between usedpos and legalpos  legalpos  setn vt vi adj det  pattern  recompilerfontsize110ptaz  document  opendicthtmread  usedpos  setrefindallpattern document  illegalpos  usedposdifferencelegalpos  print listillegalpos vi intrans This simple program represents the tip of the iceberg We can develop sophisticated tools to check the consistency of word processor files and report errors so that the maintainer of the dictionary can correct the original file using the original word processor",
                    "word_count": 707
                },
                {
                    "title": "11.4 Working with XML",
                    "page": "424",
                    "subsections": [],
                    "content": " The Extensible Markup Language XML provides a framework for designing domain specific markup languages It is sometimes used for representing annotated text and for lexical resources Unlike HTML with its predefined tags XML permits us to make up our own tags Unlike a database XML permits us to create data without first spec ifying its structure and it permits us to have optional and repeatable elements In this section we briefly review some features of XML that are relevant for representing lin guistic data and show how to access data stored in XML files using Python programs Using XML for Linguistic Structures Thanks to its flexibility and extensibility XML is a natural choice for representing linguistic structures Heres an example of a simple lexical entry 2 entry headwordwhaleheadword posnounpos glossany of the larger cetacean mammals having a streamlined body and breathing through a blowhole on the headgloss entry It consists of a series of XML tags enclosed in angle brackets Each opening tag such as gloss is matched with a closing tag gloss together they constitute an XML element The preceding example has been laid out nicely using whitespace but it could equally have been put on a single long line Our approach to processing XML will usually not be sensitive to whitespace In order for XML to be well formed all opening tags must have corresponding closing tags at the same level of nesting ie the XML document must be a wellformed tree XML permits us to repeat elements eg to add another gloss field as we see next We will use different whitespace to underscore the point that layout does not matter 3 entryheadwordwhaleheadwordposnounposglossany of the larger cetacean mammals having a streamlined body and breathing through a blowhole on the headglossglossa very large person impressive in size or qualitiesglossentry A further step might be to link our lexicon to some external resource such as WordNet using external identifiers In 4 we group the gloss and a synset identifier inside a new element which we have called sense 4 entry headwordwhaleheadword posnounpos sense glossany of the larger cetacean mammals having a streamlined body and breathing through a blowhole on the headgloss synsetwhalen02synset",
                    "word_count": 363
                },
                {
                    "title": "11.5 Working with Toolbox Data",
                    "page": "430",
                    "subsections": [],
                    "content": " Given the popularity of Toolbox among linguists we will discuss some further methods for working with Toolbox data Many of the methods discussed in previous chapters such as counting building frequency distributions and tabulating cooccurrences can be applied to the content of Toolbox entries For example we can trivially compute the average number of fields for each entry  from nltkcorpus import toolbox  lexicon  toolboxxmlrotokasdic  sumlenentry for entry in lexicon  lenlexicon 13635955056179775 In this section we will discuss two tasks that arise in the context of documentary lin guistics neither of which is supported by the Toolbox software Adding a Field to Each Entry It is often convenient to add new fields that are derived automatically from existing ones Such fields often facilitate search and analysis For instance in Example 112 we define a function cv which maps a string of consonants and vowels to the corre sponding CV sequence eg kakapua would map to CVCVCVV This mapping has four steps First the string is converted to lowercase then we replace any nonalphabetic characters az with an underscore Next we replace all vowels with V Finally any thing that is not a V or an underscore must be a consonant so we replace it with a C Now we can scan the lexicon and add a new cv field after every lx field Exam ple 112 shows what this does to a particular entry note the last line of output which shows the new cv field",
                    "word_count": 245
                },
                {
                    "title": "11.6 Describing Language Resources Using OLAC Metadata",
                    "page": "434",
                    "subsections": [],
                    "content": " Members of the NLP community have a common need for discovering language re sources with high precision and recall The solution which has been developed by the Digital Libraries community involves metadata aggregation What Is Metadata The simplest definition of metadata is structured data about data Metadata is de scriptive information about an object or resource whether it be physical or electronic Although the term metadata itself is relatively new the underlying concepts behind metadata have been in use for as long as collections of information have been organized Library catalogs represent a wellestablished type of metadata they have served as col lection management and resource discovery tools for decades Metadata can be gen erated either by hand or automatically using software The Dublin Core Metadata Initiative began in 1995 to develop conventions for finding sharing and managing information The Dublin Core metadata elements represent a broad interdisciplinary consensus about the core set of elements that are likely to be widely useful to support resource discovery The Dublin Core consists of 15 metadata elements where each element is optional and repeatable Title Creator Subject De scription Publisher Contributor Date Type Format Identifier Source Language Relation Coverage and Rights This metadata set can be used to describe resources that exist in digital or traditional formats The Open Archives Initiative OAI provides a common framework across digital re positories of scholarly materials regardless of their type including documents data software recordings physical artifacts digital surrogates and so forth Each repository consists of a networkaccessible server offering public access to archived items Each item has a unique identifier and is associated with a Dublin Core metadata record and possibly additional records in other formats The OAI defines a protocol for metadata search services to harvest the contents of repositories OLAC Open Language Archives Community The Open Language Archives Community or OLAC is an international partnership of institutions and individuals who are creating a worldwide virtual library of language resources by i developing consensus on best current practices for the digital archiving of language resources and ii developing a network of interoperating repositories and services for housing and accessing such resources OLACs home on the Web is at http wwwlanguagearchivesorg OLAC Metadata is a standard for describing language resources Uniform description across repositories is ensured by limiting the values of certain metadata elements to the use of terms from controlled vocabularies OLAC metadata can be used to describe data and tools in both physical and digital formats OLAC metadata extends the",
                    "word_count": 417
                },
                {
                    "title": "11.7 Summary",
                    "page": "436",
                    "subsections": [],
                    "content": "  Fundamental data types present in most corpora are annotated texts and lexicons Texts have a temporal structure whereas lexicons have a record structure  The life cycle of a corpus includes data collection annotation quality control and publication The life cycle continues after publication as the corpus is modified and enriched during the course of research  Corpus development involves a balance between capturing a representative sample of language usage and capturing enough material from any one source or genre to be useful multiplying out the dimensions of variability is usually not feasible be cause of resource limitations  XML provides a useful format for the storage and interchange of linguistic data but provides no shortcuts for solving pervasive data modeling problems  Toolbox format is widely used in language documentation projects we can write programs to support the curation of Toolbox files and to convert them to XML  The Open Language Archives Community OLAC provides an infrastructure for documenting and discovering language resources",
                    "word_count": 160
                },
                {
                    "title": "11.8 Further Reading",
                    "page": "436",
                    "subsections": [],
                    "content": " Extra materials for this chapter are posted at httpwwwnltkorg including links to freely available resources on the Web The primary sources of linguistic corpora are the Linguistic Data Consortium and the European Language Resources Agency both with extensive online catalogs More de tails concerning the major corpora mentioned in the chapter are available American National Corpus Reppen Ide  Suderman 2005 British National Corpus BNC 1999 Thesaurus Linguae Graecae TLG 1999 Child Language Data Exchange Sys tem CHILDES MacWhinney 1995 and TIMIT Garofolo et al 1986 Two special interest groups of the Association for Computational Linguistics that or ganize regular workshops with published proceedings are SIGWAC which promotes the use of the Web as a corpus and has sponsored the CLEANEVAL task for removing HTML markup and SIGANN which is encouraging efforts toward interoperability of",
                    "word_count": 134
                },
                {
                    "title": "11.9 Exercises",
                    "page": "438",
                    "subsections": [],
                    "content": " 1  In Example 112 the new field appeared at the bottom of the entry Modify this program so that it inserts the new subelement right after the lx field Hint create the new cv field using Elementcv assign a text value to it then use the insert method of the parent element 2  Write a function that deletes a specified field from a lexical entry We could use this to sanitize our lexical data before giving it to others eg by removing fields containing irrelevant or uncertain content 3  Write a program that scans an HTML dictionary file to find entries having an illegal partofspeech field and then reports the headword for each entry 4  Write a program to find any partsofspeech ps field that occurred less than 10 times Perhaps these are typing mistakes 5  We saw a method for adding a cv field Section 115 There is an interesting issue with keeping this uptodate when someone modifies the content of the lx field on which it is based Write a version of this program to add a cv field replacing any existing cv field 6  Write a function to add a new field syl which gives a count of the number of syllables in the word 7  Write a function which displays the complete entry for a lexeme When the lexeme is incorrectly spelled it should display the entry for the most similarly spelled lexeme 8  Write a function that takes a lexicon and finds which pairs of consecutive fields are most frequent eg ps is often followed by pt This might help us to discover some of the structure of a lexical entry 9  Create a spreadsheet using office software containing one lexical entry per row consisting of a headword a part of speech and a gloss Save the spreadsheet in CSV format Write Python code to read the CSV file and print it in Toolbox format using lx for the headword ps for the part of speech and gl for the gloss 10  Index the words of Shakespeares plays with the help of nltkIndex The result ing data structure should permit lookup on individual words such as music re turning a list of references to acts scenes and speeches of the form 3 2 9 5 1 23  where 3 2 9 indicates Act 3 Scene 2 Speech 9 11  Construct a conditional frequency distribution which records the word length for each speech in The Merchant of Venice conditioned on the name of the char acter eg cfdPORTIA12 would give us the number of speeches by Portia consisting of 12 words 12  Write a recursive function to convert an arbitrary NLTK tree into an XML coun terpart with nonterminals represented as XML elements and leaves represented as text content eg S NP typeSBJ NP NNPPierreNNP NNPVinkenNNP NP COMMACOMMA 13  Obtain a comparative wordlist in CSV format and write a program that prints those cognates having an editdistance of at least three from each other 14  Build an index of those lexemes which appear in example sentences Suppose the lexeme for a given entry is w Then add a single crossreference field xrf to this entry referencing the headwords of other entries having example sentences con taining w Do this for all entries and save the result as a Toolboxformat file",
                    "word_count": 554
                }
            ]
        }
    ],
    "total_word_count": 153281
}